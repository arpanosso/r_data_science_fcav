[["index.html", "Análise de Dados de Ciência do Solo no R Bem-vindos!!", " Análise de Dados de Ciência do Solo no R Alan R. Panosso alan.panosso@unesp.br &amp; Gener T. Pereira gener.t.pereira@unesp.br 14 a 25 de Fevereiro de 2022 Bem-vindos!! As análises estatísticas e modelagem matemática são amplamente utilizadas na área da Agronomia na Ciência do Solo. Nesse contexto, a correta utilização dos recursos computacionais para decisão e execução das principais técnicas estatísticas e matemáticas é fundamental para a formação do estudante ao nível de pós-graduação, visando uma boa condução das análises de suas pesquisas. Neste contexto, a Ciência de Dados tem destaque mundial, uma vez que todos os campos de estudo e áreas de negócios foram afetados à medida que as pessoas percebem cada vez mais o valor das incríveis quantidades de dados sendo gerados. Mas para extrair valor desses dados, é necessário ser treinado nas habilidades adequadas de ciência de dados. A linguagem de programação R (software livre, de domínio público) se tornou a linguagem de programação de fato para a ciência de dados. Sua flexibilidade, potência, sofisticação e expressividade a tornaram uma ferramenta inestimável para cientistas de dados em todo o mundo. Neste curso pretende-se dar uma introdução às técnicas de programação e à riqueza do ambiente R, destinada aos pós-graduandos iniciantes e intermediários da estatística e da experimentação agronômica e, consequentemente, da ciência de dados. Inicialmente serão abordados tópicos de programação R, para a familiarização dos alunos às estruturas básicas e recursos de programação e visualização de dados. Em seguida serão abordados os temas de estatística descritiva, teste de hipóteses, análise de regressão, análise de variância, e rotinas de diagnósticos, todos desenvolvidos no ambiente R. Você começará com o básico da linguagem, aprenderá como manipular conjuntos de dados, como escrever funções, como visualizar seus dados e como analizá-los. Com os fundamentos fornecidos neste curso, esperamos que você tenha uma base sólida sobre a qual construir sua caixa de ferramentas de ciência de dados na ciência do solo, ou em em sua área de atuação. "],["ambientação.html", "1 Ambientação 1.1 Instalando o R 1.2 Instalando o RStudio", " 1 Ambientação 1.1 Instalando o R Faça o download do R do site oficial The R Project for Statistical Computing https://www.r-project.org/ Acesso rápido: Download Salve o arquivo de instalação em um diretório de seu computador e em seguida execute-o. O processo é simples e intuitivo. 1.2 Instalando o RStudio É um ambiente de desenvolvimento integrado (IDE) para o R, disponível em https://rstudio.com/products/rstudio/download/ Escolha o instalador de acordo com o seu sistema operacional. Windows no nosso caso. Novamente, salve o arquivo de instalação em um diretório de seu computador e em seguida execute-o. Após a instalação procure o ícone do R criado pelo instalador e clique nele. Um breve tutorial para instalação do R e do RStudio pode ser encontrado no vídeo abaixo. "],["pacotes-no-r.html", "2 Pacotes no R 2.1 Pacotes básicos 2.2 Instalando pacotes", " 2 Pacotes no R Um pacote é uma coleção de funções, exemplos e documentação. A funcionalidade de um pacote é frequentemente focada em uma metodologia estatística especial\" (Everitt &amp; Hothorn). Figure 2.1: Pacotes no R são coleções de funções, exemoplos e documentações, os quais devem ser previamente instalados e alocados no ambiente por meio da função library ou require. 2.1 Pacotes básicos Liste os pacotes carregados no ambiente com: (.packages()) ## [1] &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; ## [7] &quot;base&quot; O retorno da função é uma lista de nomes, caracteres (ou strings), na forma de um objeto denominado vetor. Observe que cada pacote (elemento) é referenciado dentro do vetor por um índice, um número inteiro \\([\\;i\\;]\\) apresentado entre colchetes [i], onde \\(i\\) varia de \\(1\\) a \\(7\\), em nosso exemplo, pois sete é o número total de elementos do vetor. Carregue um pacote chamando a função library. library(MASS) ## Warning: package &#39;MASS&#39; was built under R version 4.1.1 Ou utilize a função require. require(car) Agora, liste novamente os pacotes e observe a diferença no retorno da função. (.packages()) ## [1] &quot;car&quot; &quot;carData&quot; &quot;MASS&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [7] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; 2.2 Instalando pacotes Para a realização de vários procedimentos estatístico e manipulação de arquivos durante o curso, serão necessários vários pacotes que não fazem parte do base do R, que deverão ser instalados. Utilizando a opção Install/Packages Instale alguns pacotes: tidyverse agricolae readxl stringr lubridate Os pacotes também podem ser instalados a partir das linhas de comandos (códigos): install.packages(&quot;tidyverse&quot;) install.packages(&quot;agricolae&quot;) install.packages(&quot;readxl&quot;) install.packages(&quot;stringr&quot;) install.packages(&quot;lubridate&quot;) "],["lógica-de-programação.html", "3 Lógica de Programação 3.1 Introdução 3.2 Conceitos básicos", " 3 Lógica de Programação 3.1 Introdução A Lógica pode ser definida como a análise das formas e leis do pensamento, mas não se preocupa com a produção do pensamento, não se preocupa com o conteúdo do pensamento, mas sim com a maneira pela qual os pensamentos são organizados e apresentados, possibilitando que cheguemos a uma conclusão por meio do encadeamento dos argumentos. A lógica é a ciência que estuda as leis do raciocínio. Correção/validação do pensamento. Encadeamento/ordem de ideias. Arte de bem pensar. 3.2 Conceitos básicos Em Lógica um conceito importante é o de Proposição. Proposição: é um enunciado verbal, ao qual deve ser atribuído, sem ambiguidade, um valor lógico verdadeiro (\\(V\\) ou \\(TRUE\\)) ou falso (\\(F\\) ou \\(FALSE\\)). Abstração: Operação mental que observa a realidade e captura apenas os aspectos relevantes para um contexto. Figure 3.1: Passe alguns segundos olhando para a figura abaixo e diga o que você consegue abstrair dela. Perceba que a realidade é a mesma, isto é, uma figura em preto e branco, mas, dependendo da observação da realidade, você pode ter abstrações diferentes. Por isso, a abstração depende mais do observador do que da realidade observada. A tarefa de programar sistemas computacionais envolve o exercício constante da abstração da realidade e sua codificação em uma linguagem de programação. Programação: A tarefa de programar sistemas computacionais e envolve o exercício constante da abstração da realidade e sua codificação em uma linguagem de programação. Figure 3.2: Exemplo de programação de um sistema para exibir a média de dois números #Entrada a&lt;-6 b&lt;-8 #Processamento resultado &lt;- (a+b)/2 #Saída resultado ## [1] 7 Linguagem de Programação: conjunto de palavras e regras que permitem comunicar ao computador o que este deve executar. Em computação, uma linguagem de programação é a ferramenta de comunicação entre o programador que visa resolver um problema e o computador que irá ajudá-lo a resolver. Tipos de linguagens de programação: 1 - Totalmente codificadas em binário (0´s e 1´s). 2 - Usa instruções simbólicas para representar os 0´s e 1´s. 3 - Voltadas para facilitar o raciocínio humano. Figure 3.3: Resumo dos tipos de linguagem de programação. Se o computador só entende linguagem de máquina, o que deve ser feito para que ele entenda programas em linguagem assembly ou de alto nível? Tradutores no contexto de linguagens de programação são programas que recebem como entrada um programa em linguagem assembly ou de alto nível (dita linguagem fonte) e produzem como saída as instruções deste programa traduzidas para linguagem de máquina. Existem basicamente três tipos de tradutores: + Compilador + Interpretador + Assembler Figure 3.4: O Compilador traduz de uma vez só todo o programa escrito em linguagem de alto nível (código-fonte) para um programa equivalente escrito em linguagem de máquina (código-objeto). Por sua vez, o Interpretador, como o R, traduz (sem gerar código-objeto) e em seguida executa, uma-a-uma, as instruções de um programa em linguagem de alto nível (código-fonte). "],["introdução-à-ciência-dos-dados.html", "4 Introdução à Ciência dos Dados 4.1 Importação 4.2 Organização 4.3 Transformação dos dados 4.4 Visualização 4.5 Modelação 4.6 Comunicação 4.7 Programação", " 4 Introdução à Ciência dos Dados A Ciência dos Dados, ou Data Science, é uma área interdisciplinar voltada para o estudo e a análise de dados, estruturados e não-estruturados, que visa a extração de conhecimento, detecção de padrões e/ou obtenção de variáveis para possíveis tomadas de decisão. O modelo base das ferramentas necessárias em um projeto típico de ciência dos dados é parecido com isso: Figure 4.1: Modelo base de um projeto de Ciência dos Dados. 4.1 Importação Primeiro devemos importar os dados no R. Ou seja, pegar os dados armazenados em um arquivo, base de dados ou na Web e carregá-los em uma estrutuda de dados no R. Sem eles no R, não conseguiremos fazer Data Science. Para essa prática, vamos utilizar um banco de dados oriundo de um estudo geomorfológico. Importação via web Acesse o banco de dados na web: Clique no link para o arquivo geomorfologia.txt. # definir o caminho URL &lt;- &quot;https://raw.githubusercontent.com/arpanosso/r_data_science_fcav/master/dados/geomorfologia.txt&quot; # Importação dados&lt;-read.table(URL, header = TRUE) # Argumento para cabeçalho na primeira linha # Inspeção do banco de dados head(dados) # mostra o 6 primeiros registros ## SUP Solo Amostra X AMG AG AM AF AMF SILTE ARGILA S_A AF_AG P pH ## 1 I LV 1 0 0.2 3.72 20.4 22.9 30.0 1.2 21.5 0.05 6.16 42 4.2 ## 2 I LV 2 25 0.1 4.27 22.6 23.6 28.4 1.2 20.4 0.05 5.53 22 3.8 ## 3 I LV 3 50 0.7 5.00 22.7 22.2 26.9 1.2 21.4 0.05 4.44 41 4.8 ## 4 I LV 4 75 0.4 3.80 23.7 24.4 26.7 0.6 20.5 0.02 6.42 27 4.0 ## 5 I LV 5 100 0.4 3.10 22.3 24.6 26.9 2.1 20.7 0.10 7.94 11 4.4 ## 6 I LV 6 125 0.4 3.80 23.8 19.1 27.1 2.2 23.5 0.09 5.03 12 4.0 ## K Ca Mg H_Al SB T V ## 1 0.27 1.4 0.3 5.2 1.97 7.17 27 ## 2 0.11 0.4 0.1 5.8 0.61 6.41 10 ## 3 0.34 2.4 0.4 4.2 3.14 7.34 43 ## 4 0.13 0.7 0.1 5.2 0.93 6.13 15 ## 5 0.11 1.4 0.3 4.2 1.81 6.01 30 ## 6 0.14 0.6 0.1 5.2 0.84 6.04 14 Importação via Excel Acesse o banco de dados para essa prática, denominado geomorfologia.xlsx, salve o arquivo em uma pasta de seu computador. Figure 4.2: Na aba Environment selecione a opção Import Dataser e escolha From Excel Figure 4.3: 1) Clique em Browse, acesse a pasta na qual você salvou o arquivo; 2) selecione o arquivo geomorfologia.xlsx; 3) clique em Open. Figure 4.4: Pré-visualização dos dados, observe que o código de importação é apresentado abaixo dessa janela. Copie esse código, e clique em CANCEL. Cole as linhas de código no script do R e as execute, para ter a importação dos dados. library(readxl) ## Warning: package &#39;readxl&#39; was built under R version 4.1.2 geomorfologia &lt;- read_excel(&quot;C:/GitHub/r_data_science_fcav/dados/geomorfologia.xlsx&quot;) Figure 4.5: Após executar o código, os dados devem ser apresentados dessa forma. 4.2 Organização Uma vez que os dados estão no R, a próxima etapa é organizá-los, ou seja, armazená-los de uma forma consistente que combine a semântica da base de dados com a maneira com a qual eles são armazenados. Cada coluna é uma variável e cada linha é uma observação. Figure 4.6: Exemplo de dados organizados em planilha eletrônica, onde cada coluna é uma variável e cada linha é uma observação, ou registro. Observação: O R, como a maioria dos softwares estatísticos, utiliza o ponto como separador decimal, ou seja, como o símbolo usado para separar a parte inteira da parte complementar não inteira da representação decimal do numeral de um real (ponto flutuante). Portanto, aconselhamos você a padronizar o separador decimal do seu computador. Para isso sigua os passos abaixo: Figure 4.7: Acesse o PAINEL DE CONTROLE, na opção REGIÃO clique em CONFIGURAÇÕES ADICIONAIS e modifique o SÍBOLO DECIMAL para PONTO e o SÍMBOLO DE AGRUPAMENTO DE DÍGITO para VÍRGULA. Vamos conhecer a estrutura do nosso banco de dados, a partir da função str. str(dados) ## &#39;data.frame&#39;: 106 obs. of 22 variables: ## $ SUP : chr &quot;I&quot; &quot;I&quot; &quot;I&quot; &quot;I&quot; ... ## $ Solo : chr &quot;LV&quot; &quot;LV&quot; &quot;LV&quot; &quot;LV&quot; ... ## $ Amostra: int 1 2 3 4 5 6 7 8 9 10 ... ## $ X : int 0 25 50 75 100 125 150 175 200 225 ... ## $ AMG : num 0.2 0.1 0.7 0.4 0.4 0.4 1.2 0.8 1.1 1.2 ... ## $ AG : num 3.72 4.27 5 3.8 3.1 3.8 3.6 4.7 4.5 5.9 ... ## $ AM : num 20.4 22.6 22.7 23.7 22.3 23.8 23.1 25.8 25.5 32.8 ... ## $ AF : num 22.9 23.6 22.2 24.4 24.6 19.1 21.7 21.1 18.9 19.8 ... ## $ AMF : num 30 28.4 26.9 26.7 26.9 27.1 26.5 24.7 25.4 21.7 ... ## $ SILTE : num 1.2 1.2 1.2 0.6 2.1 2.2 0.7 0.2 2.5 0.2 ... ## $ ARGILA : num 21.5 20.4 21.4 20.5 20.7 23.5 23.1 22.7 22 18.5 ... ## $ S_A : num 0.05 0.05 0.05 0.02 0.1 0.09 0.03 0.01 0.11 0.01 ... ## $ AF_AG : num 6.16 5.53 4.44 6.42 7.94 5.03 6.03 4.49 4.2 3.36 ... ## $ P : num 42 22 41 27 11 12 11 16 38 25 ... ## $ pH : num 4.2 3.8 4.8 4 4.4 4 4.8 5.4 4.4 5.2 ... ## $ K : num 0.27 0.11 0.34 0.13 0.11 0.14 0.23 0.28 0.19 0.14 ... ## $ Ca : num 1.4 0.4 2.4 0.7 1.4 0.6 1.6 3.3 1.6 2.9 ... ## $ Mg : num 0.3 0.1 0.4 0.1 0.3 0.1 0.7 1.3 0.5 1.7 ... ## $ H_Al : num 5.2 5.8 4.2 5.2 4.2 5.2 3.4 2.5 5.2 3.1 ... ## $ SB : num 1.97 0.61 3.14 0.93 1.81 0.84 2.53 4.88 2.29 4.74 ... ## $ T : num 7.17 6.41 7.34 6.13 6.01 6.04 5.93 7.38 7.49 7.84 ... ## $ V : num 27 10 43 15 30 14 43 66 31 60 ... Podemos identificar que os dados são compostos por 106 linhas e 22 colunas. As duas primeiras colunas são do tipo texto (chr - character) e as demais colunas são numéricas (num - numeric). 4.3 Transformação dos dados O termo transformação significa literalmente recortar o banco de dados, assim podemos focar nas observações de interesse. Criar novas variáveis em função das existentes e calcular, por exemplo, um resumo estatístico desse conjunto de interesse. Por exemplo, vamos selecionar somente os solos do tipo LV. # Criar um filtro f &lt;- dados$Solo == &quot;LV&quot; # Criar um banco de dados auxiliar da da &lt;- dados[f,] # Vamos ver a estrutura desse banco auxiliar str(da) ## &#39;data.frame&#39;: 16 obs. of 22 variables: ## $ SUP : chr &quot;I&quot; &quot;I&quot; &quot;I&quot; &quot;I&quot; ... ## $ Solo : chr &quot;LV&quot; &quot;LV&quot; &quot;LV&quot; &quot;LV&quot; ... ## $ Amostra: int 1 2 3 4 5 6 7 8 9 10 ... ## $ X : int 0 25 50 75 100 125 150 175 200 225 ... ## $ AMG : num 0.2 0.1 0.7 0.4 0.4 0.4 1.2 0.8 1.1 1.2 ... ## $ AG : num 3.72 4.27 5 3.8 3.1 3.8 3.6 4.7 4.5 5.9 ... ## $ AM : num 20.4 22.6 22.7 23.7 22.3 23.8 23.1 25.8 25.5 32.8 ... ## $ AF : num 22.9 23.6 22.2 24.4 24.6 19.1 21.7 21.1 18.9 19.8 ... ## $ AMF : num 30 28.4 26.9 26.7 26.9 27.1 26.5 24.7 25.4 21.7 ... ## $ SILTE : num 1.2 1.2 1.2 0.6 2.1 2.2 0.7 0.2 2.5 0.2 ... ## $ ARGILA : num 21.5 20.4 21.4 20.5 20.7 23.5 23.1 22.7 22 18.5 ... ## $ S_A : num 0.05 0.05 0.05 0.02 0.1 0.09 0.03 0.01 0.11 0.01 ... ## $ AF_AG : num 6.16 5.53 4.44 6.42 7.94 5.03 6.03 4.49 4.2 3.36 ... ## $ P : num 42 22 41 27 11 12 11 16 38 25 ... ## $ pH : num 4.2 3.8 4.8 4 4.4 4 4.8 5.4 4.4 5.2 ... ## $ K : num 0.27 0.11 0.34 0.13 0.11 0.14 0.23 0.28 0.19 0.14 ... ## $ Ca : num 1.4 0.4 2.4 0.7 1.4 0.6 1.6 3.3 1.6 2.9 ... ## $ Mg : num 0.3 0.1 0.4 0.1 0.3 0.1 0.7 1.3 0.5 1.7 ... ## $ H_Al : num 5.2 5.8 4.2 5.2 4.2 5.2 3.4 2.5 5.2 3.1 ... ## $ SB : num 1.97 0.61 3.14 0.93 1.81 0.84 2.53 4.88 2.29 4.74 ... ## $ T : num 7.17 6.41 7.34 6.13 6.01 6.04 5.93 7.38 7.49 7.84 ... ## $ V : num 27 10 43 15 30 14 43 66 31 60 ... Vamos continuar o processo de filtragem e selecionar somente as colunas SUP, ARGILA, SILTE e T. da&lt;-da[,c(1,11,10,21)] str(da) ## &#39;data.frame&#39;: 16 obs. of 4 variables: ## $ SUP : chr &quot;I&quot; &quot;I&quot; &quot;I&quot; &quot;I&quot; ... ## $ ARGILA: num 21.5 20.4 21.4 20.5 20.7 23.5 23.1 22.7 22 18.5 ... ## $ SILTE : num 1.2 1.2 1.2 0.6 2.1 2.2 0.7 0.2 2.5 0.2 ... ## $ T : num 7.17 6.41 7.34 6.13 6.01 6.04 5.93 7.38 7.49 7.84 ... Vamos criar uma nova variável, ARG_SILT a partir da soma dos valores de ARGILA + SILTE, para isso vamos utilizar o símbolo de acesso de colunas, o cifrão ($). Em seguida, aplicaremos a transformação logarítmica aos dados de T. da$ARG_SILT &lt;- da$ARGILA + da$SILTE da$Log_T &lt;- log10(da$T) str(da) ## &#39;data.frame&#39;: 16 obs. of 6 variables: ## $ SUP : chr &quot;I&quot; &quot;I&quot; &quot;I&quot; &quot;I&quot; ... ## $ ARGILA : num 21.5 20.4 21.4 20.5 20.7 23.5 23.1 22.7 22 18.5 ... ## $ SILTE : num 1.2 1.2 1.2 0.6 2.1 2.2 0.7 0.2 2.5 0.2 ... ## $ T : num 7.17 6.41 7.34 6.13 6.01 6.04 5.93 7.38 7.49 7.84 ... ## $ ARG_SILT: num 22.7 21.6 22.6 21.1 22.8 25.7 23.8 22.9 24.5 18.7 ... ## $ Log_T : num 0.856 0.807 0.866 0.787 0.779 ... Agora vamos gerar um resumo estatístico para esses dados. A primeira coluna é do tipo texto, então, deve ser retirada do banco de dados auxiliar da antes de realizarmos os cálculos. Para isso, utilizamos o índice \\(-1\\) na dimensão das colunas do objeto da, ou seja, estamos retirando a coluna 1 SUP de da. # Número de observações apply(da[,-1],2,length) ## ARGILA SILTE T ARG_SILT Log_T ## 16 16 16 16 16 # Média apply(da[,-1],2,mean) ## ARGILA SILTE T ARG_SILT Log_T ## 21.1937500 1.2125000 6.2618750 22.4062500 0.7920605 # Mediana apply(da[,-1],2,median) ## ARGILA SILTE T ARG_SILT Log_T ## 20.8500000 1.1000000 6.0850000 22.6500000 0.7842487 # Variância apply(da[,-1],2,var) ## ARGILA SILTE T ARG_SILT Log_T ## 2.101958333 0.782500000 0.890589583 3.015291667 0.004318016 # Desvio Padrão apply(da[,-1],2,sd) ## ARGILA SILTE T ARG_SILT Log_T ## 1.44981321 0.88459030 0.94371054 1.73645952 0.06571161 # Podemos utilizar a função summary. summary(da[,-1]) ## ARGILA SILTE T ARG_SILT ## Min. :18.50 Min. :0.200 Min. :4.840 Min. :18.70 ## 1st Qu.:20.48 1st Qu.:0.475 1st Qu.:5.760 1st Qu.:21.32 ## Median :20.85 Median :1.100 Median :6.085 Median :22.65 ## Mean :21.19 Mean :1.212 Mean :6.262 Mean :22.41 ## 3rd Qu.:22.18 3rd Qu.:2.125 3rd Qu.:7.213 3rd Qu.:23.20 ## Max. :23.50 Max. :2.600 Max. :7.840 Max. :25.70 ## Log_T ## Min. :0.6848 ## 1st Qu.:0.7598 ## Median :0.7842 ## Mean :0.7921 ## 3rd Qu.:0.8581 ## Max. :0.8943 Agora podemos gerar o conhecimento, por meio da Visualização e Modelagem. Essas tem suas vantegens e desvantagens as quais são complementares, portanto, quaisquer análises reais farão muitas vezes iteraçoes entre elas. Figure 4.8: Visualização e modelagem são dois processos iterativos, onde a matemática é uma ferramenta essencial para a extração de padrões, declaração e testes de hipóteses. 4.4 Visualização Atividade fundamentalmente humana, uma boa visualização lhe mostrará coisas que não esperava, ou levantará novas questões sobre os dados. Além disso, pode mostrar também que você esta fazendo a pergunta errada, ou que precisa coletar dados diferentes. Visualizações podem surpreender o analista de dados, mas não escalam particularmente bem, por que requerem um humano para interpretá-las. plot(da$ARGILA,da$T) 4.5 Modelação Modelo são ferramentas complementares da visualização. Uma vez que você tenha feito perguntas suficientemente precisas, poderá usar um modelo para respondê-las. Modelos são fundamentalmente matemáticos ou computacionais, então, geralmente escalam muito bem. Porém, cada modelo faz suposições e, por sua própria natureza, não podem questionar suas própria hipóteses, ou seja, um modelo não pode nos surpreender. plot(da$ARGILA,da$T) mod&lt;-lm(da$T~da$ARGILA) summary.lm(mod) ## ## Call: ## lm(formula = da$T ~ da$ARGILA) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4256 -0.4641 -0.1615 0.9560 1.5268 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.66574 3.69347 1.805 0.0927 . ## da$ARGILA -0.01906 0.17389 -0.110 0.9143 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9764 on 14 degrees of freedom ## Multiple R-squared: 0.000857, Adjusted R-squared: -0.07051 ## F-statistic: 0.01201 on 1 and 14 DF, p-value: 0.9143 abline(mod) 4.5.1 Exemplo de Análise multivariada (PCA) Figure 4.9: Classificação das técnicas multivariadas (Dependências e Interdependência). Carregando alguns pacotes library(vegan) ## Carregando pacotes exigidos: permute ## Carregando pacotes exigidos: lattice ## This is vegan 2.5-7 library(corrplot) ## corrplot 0.90 loaded Matriz de Correlação geomorfo_num &lt;- geomorfologia[-(1:5)] matriz_correlacao &lt;- cor(geomorfo_num) corrplot(matriz_correlacao) corrplot(matriz_correlacao, method = &quot;ellipse&quot;) corrplot.mixed(matriz_correlacao, lower.col = &quot;black&quot;, upper = &quot;ellipse&quot;) Notoriamente existem muitas variáveis, vamos filtar e transformas str(geomorfo_num) ## tibble [106 x 17] (S3: tbl_df/tbl/data.frame) ## $ AG : num [1:106] 3.72 4.27 5 3.8 3.1 3.8 3.6 4.7 4.5 5.9 ... ## $ AM : num [1:106] 20.4 22.6 22.7 23.7 22.3 23.8 23.1 25.8 25.5 32.8 ... ## $ AF : num [1:106] 22.9 23.6 22.2 24.4 24.6 19.1 21.7 21.1 18.9 19.8 ... ## $ AMF : num [1:106] 30 28.4 26.9 26.7 26.9 27.1 26.5 24.7 25.4 21.7 ... ## $ SILTE : num [1:106] 1.2 1.2 1.2 0.6 2.1 2.2 0.7 0.2 2.5 0.2 ... ## $ ARGILA: num [1:106] 21.5 20.4 21.4 20.5 20.7 23.5 23.1 22.7 22 18.5 ... ## $ S_A : num [1:106] 0.05 0.05 0.05 0.02 0.1 0.09 0.03 0.01 0.11 0.01 ... ## $ AF_AG : num [1:106] 6.16 5.53 4.44 6.42 7.94 5.03 6.03 4.49 4.2 3.36 ... ## $ P : num [1:106] 42 22 41 27 11 12 11 16 38 25 ... ## $ pH : num [1:106] 4.2 3.8 4.8 4 4.4 4 4.8 5.4 4.4 5.2 ... ## $ K : num [1:106] 0.27 0.11 0.34 0.13 0.11 0.14 0.23 0.28 0.19 0.14 ... ## $ Ca : num [1:106] 1.4 0.4 2.4 0.7 1.4 0.6 1.6 3.3 1.6 2.9 ... ## $ Mg : num [1:106] 0.3 0.1 0.4 0.1 0.3 0.1 0.7 1.3 0.5 1.7 ... ## $ H_Al : num [1:106] 5.2 5.8 4.2 5.2 4.2 5.2 3.4 2.5 5.2 3.1 ... ## $ SB : num [1:106] 1.97 0.61 3.14 0.93 1.81 0.84 2.53 4.88 2.29 4.74 ... ## $ T : num [1:106] 7.17 6.41 7.34 6.13 6.01 6.04 5.93 7.38 7.49 7.84 ... ## $ V : num [1:106] 27 10 43 15 30 14 43 66 31 60 ... geomorfo_num$AREIA &lt;- geomorfo_num$AG + geomorfo_num$AM + geomorfo_num$AF + geomorfo_num$AMF Vamos selecionar alguams variáveis geomorfo_nun_selecionada &lt;- geomorfo_num[c(&quot;AREIA&quot;,&quot;SILTE&quot;,&quot;ARGILA&quot;, &quot;P&quot;, &quot;pH&quot;, &quot;K&quot;,&quot;Ca&quot;,&quot;Mg&quot;,&quot;H_Al&quot;)] matriz_correlacao_selecionada &lt;- cor(geomorfo_nun_selecionada) corrplot.mixed(matriz_correlacao_selecionada,lower.col = &quot;black&quot;, upper = &quot;ellipse&quot;) Para a análise de componente principais, primeiramente devemos padronizar o banco de dados, assim utilizamos o argumento scale = TRUE da funçaõ prcompr. da &lt;- decostand(geomorfo_nun_selecionada,method = &quot;standardize&quot;,na.rm = TRUE) pca &lt;- prcomp(geomorfo_nun_selecionada,scale. = TRUE) summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.9208 1.5089 1.1682 0.89358 0.69509 0.51859 0.24150 ## Proportion of Variance 0.4099 0.2530 0.1516 0.08872 0.05368 0.02988 0.00648 ## Cumulative Proportion 0.4099 0.6629 0.8146 0.90328 0.95697 0.98685 0.99333 ## PC8 PC9 ## Standard deviation 0.19093 0.15359 ## Proportion of Variance 0.00405 0.00262 ## Cumulative Proportion 0.99738 1.00000 Extraindo os autovalores eig &lt;- pca$sdev^2 eig ## [1] 3.68947383 2.27684828 1.36474606 0.79848178 0.48314722 0.26893875 0.05832218 ## [8] 0.03645306 0.02358884 Porcentagem da variância explicada por cada componente ve&lt;-eig/sum(eig) ve ## [1] 0.409941537 0.252983142 0.151638451 0.088720198 0.053683025 0.029882083 ## [7] 0.006480242 0.004050340 0.002620982 Porcentagem acumulada cumsum(ve) ## [1] 0.4099415 0.6629247 0.8145631 0.9032833 0.9569664 0.9868484 0.9933287 ## [8] 0.9973790 1.0000000 # pca # Autovalores das variáveis # pca$x # coordenadas dos acessos Correlações entre cada variável - poder discriminante mcor&lt;-cor(da, pca$x) corrplot(mcor) Screeplot, para verificar os PC maiores que a Unidade, de acordo com Kaiser, valores maiores que 1 screeplot(pca) barplot(eig, names.arg=colnames(pca$rotation), ylim=c(0,(pca$sdev^2)[1]*1.1), ylab=&quot;Inertia&quot;,xlab=&quot;Principal Component&quot;,cex.lab=1.2 ,main=&quot;Sreeplot&quot;) abline(h=1,lty=2) Construção do Biplot e sua tabela grupos &lt;- geomorfologia$SUP biplot(rda(da),pch=2) ordihull(rda(da), group = grupos, col=1:nlevels(as.factor(grupos))) Construção do biplot usando as funções básicas pc1V&lt;-cor(da,pca$x)[,1]/sd(cor(da,pca$x)[,1]) pc2V&lt;-cor(da,pca$x)[,2]/sd(cor(da,pca$x)[,2]) pc1c&lt;-pca$x[,1]/sd(pca$x[,1]) pc2c&lt;-pca$x[,2]/sd(pca$x[,2]) nv&lt;-ncol(da) # número de variáveis utilizadas na análise plot(pc1V,pc2V, xlim=c(min(-3,pc1V,pc1c), max(pc1V,pc1c,5)), ylim=c(min(pc2V,pc2c), max(pc2V,pc2c)),pch=&quot;&quot;,las=1, xlab=paste(&quot;PC1 (&quot;,round(100*ve[1],2),&quot;%)&quot;,sep=&quot;&quot;), ylab=paste(&quot;PC2 (&quot;,round(100*ve[2],2),&quot;%)&quot;,sep=&quot;&quot;), font.lab=2) abline(v=0,h=0) arrows(rep(0,nv),rep(0,nv),pc1V*.90,pc2V*.90,lwd=1.5,length=.1) text(pc1V,pc2V,names(pc1V),font=4) lv&lt;-as.factor(grupos);nlv&lt;-levels(lv) # Adicionando os identificadores dos acesso partir de uma variável categórica for(i in 1:length(nlv)){ ff&lt;-lv==nlv[i] # points(pc1c[ff],pc2c[ff],cex=1.3,col=i,pch=i+15) # df$Municípios[ff] text(pc1c[ff],pc2c[ff],grupos[ff],cex=.8,col=i)} Tabela do Biplot com as correlações com as PCs e seus respectivos autovetores ck&lt;-sum(pca$sdev^2&gt;=1) tabelapca&lt;-vector() for( l in 1:ck) tabelapca&lt;-cbind(tabelapca,mcor[,l])#,pca$rotation[,l]) colnames(tabelapca)&lt;-paste(rep(c(&quot;PC&quot;),ck),1:ck,sep=&quot;&quot;) pcat&lt;-round(tabelapca,3) summary(pcat) ## PC1 PC2 PC3 ## Min. :-0.7740 Min. :-0.6470 Min. :-0.7450 ## 1st Qu.: 0.0860 1st Qu.:-0.5990 1st Qu.:-0.2750 ## Median : 0.4590 Median :-0.4460 Median : 0.0560 ## Mean : 0.2561 Mean :-0.2092 Mean :-0.1451 ## 3rd Qu.: 0.7170 3rd Qu.: 0.0650 3rd Qu.: 0.1340 ## Max. : 0.8960 Max. : 0.8150 Max. : 0.2860 tabelapca&lt;-tabelapca[order(abs(tabelapca[,1])),] tabelapca ## PC1 PC2 PC3 ## P 0.08588849 0.06523595 -0.74469586 ## K 0.17453865 -0.09655659 -0.72460429 ## AREIA 0.45919323 0.81459224 0.17361541 ## SILTE 0.62733966 -0.59857849 0.28629486 ## ARGILA -0.68956146 -0.60629820 -0.27491327 ## Ca 0.71745561 -0.64652319 0.05554402 ## H_Al -0.77429361 -0.44592300 0.13433788 ## Mg 0.80916440 -0.49156593 0.05874232 ## pH 0.89593784 0.12384873 -0.26991837 Uma opção à tabela resumo library(plotrix) ## Warning: package &#39;plotrix&#39; was built under R version 4.1.1 pyramid.plot(abs(tabelapca[,1]),abs(tabelapca[,2]), labels = names(tabelapca[,1]), unit=&quot;Correlation&quot;,gap=.1,raxlab=c(0,0.25,.5,.75,1), top.labels = c(&quot;PC1 (33.85%)&quot;, &quot;Variable&quot;, &quot;PC2 (25.16%)&quot;), laxlab=c(0,0.25,.5,.75,1), lxcol=c(&quot;darkgray&quot;,&quot;white&quot;,&quot;darkgray&quot;,&quot;darkgray&quot;,&quot;darkgray&quot;, &quot;darkgray&quot;),rxcol=c(&quot;white&quot;,&quot;white&quot;,&quot;darkgray&quot;,&quot;darkgray&quot;, &quot;white&quot;,&quot;white&quot;)) ## 1 1 ## [1] 5.1 4.1 4.1 2.1 text(-1*abs(tabelapca[,1]),1:6,round(tabelapca[,1],2)) text(abs(tabelapca[,2]+c(0,0,0,0,-.04,0)),1:6,round(tabelapca[,2],2)) ## Warning in tabelapca[, 2] + c(0, 0, 0, 0, -0.04, 0): comprimento do objeto maior ## não é múltiplo do comprimento do objeto menor box() 4.6 Comunicação É a última etapa do Data Science, a mais importante. Não importa quão bem seus modelos e visualizações o levaram a entender os dados, a menos que você também consiga comunicar seus resultados para outras pessoas. Uma dica importante, é estudar RMarkdown! Ótima linguagem de marcação de texto, utilizada para gerar relatórios em HTML, .doc e pdf. Inclusive, esse material que estamos utilizando no curso. Figure 4.10: https://bookdown.org/yihui/rmarkdown-cookbook/ 4.7 Programação Permeia todas as etapas da Ciência dos Dados. Ao nosso ver é uma questão recursiva O computador é a ferramenta do cientista de dados. R é um ambiente e linguagem de programação de código aberto para computação estatística, bioinformática e gráficos. Como linguagem de programação, garante a reprodutibilidade das análises. Estudantes podem usar as mesmas ferramentas que os profissionais. Todos podem usar as melhores ferramentas independente do poder financeiro. Qualquer um pode reproduzir as suas análises. Você pode corrigir problemas. Você pode desenvolver suas próprias ferramentas. Linguagem orientada a objetos R (Ross Ihaka e Robert Gentleman). R é uma poderosa linguagem, flexível e possui excelentes facilidades gráficas. R é um projeto open-source e está disponível na internet sobre a General Public License (&lt;www.gnu.org/copyleft/gpl.html&gt; e &lt;www.fsf.org&gt;). "],["tópicos-iniciais.html", "5 Tópicos iniciais 5.1 Identificadores 5.2 Palavras Reservadas 5.3 Regras para os nomes de Identificadores 5.4 Tipos de dados 5.5 Atribuição 5.6 Operadores 5.7 Operação aritmética 5.8 Funções matemáticas e trigonométricas 5.9 Operações Relacionais 5.10 Operações Lógicas 5.11 Exercícios", " 5 Tópicos iniciais 5.1 Identificadores São nomes únicos definidos pelos programadores para identificar/distinguir os elementos de um programa. 5.2 Palavras Reservadas São instruções primitivas que têm significados pré-determinados e fazem parte da estrutura de qualquer linguagem de programação. 5.3 Regras para os nomes de Identificadores 1) Devem começar por um caractere alfabético; 2) Podem ser seguidos por mais caracteres alfabéticos e/ou numéricos; 3) Não é permitido o uso de espaço em branco ou de caracteres especiais, como: @, #, &amp;, ?, $ (exceto o _ e o .); 4) Não poderá ser uma palavra reservada a uma instrução do algoritmo (if, else, for, while, entre outras); *5) Devem ser significativos. Geralmente, desejamos que os nomes dos objetos sejam descritivos, então precisaremos de uma convenção para várias palavras. Recomendamos snake_case, onde você separa palavras minúsculas com _ . eu_uso_snake_case outrasPessoasUsamCamelCase algumas.pessoas.usam.pontos E_algumasPoucas.Pessoas_RENUNCIAMconvenções 5.4 Tipos de dados As fases de Entrada, Processamento e Saída podem manipular vários tipos primitivos de dados, a saber: Figure 5.1: Tipos primitivos de dados Um Caractere SEMPRE deve estar entre aspas duplas \" \" ou simples  . Por exemplo: \"A\", \"Fone 3333-33333\", \"1\", '3.1415', 'a mais bonita',etc. &quot; Olá, eu sou um texto.&quot; ## [1] &quot; Olá, eu sou um texto.&quot; &#39;1&#39; # não pode ser feito cálculos ## [1] &quot;1&quot; 3.14 # DOUBLE: valor real, de ponto flutuante. ## [1] 3.14 1 # valor numérico inteiro, ## [1] 1 4L #sufixo L temos números inteiros em vez de double ## [1] 4 pi # constante pré definida com o valor de pi ## [1] 3.141593 TRUE # valor lógico verdadeiro ## [1] TRUE FALSE # valor lógico falso ## [1] FALSE 5.5 Atribuição Serve para atribuir um valor a uma variável/objeto no R. A expressão do lado direito do operador é avaliada e seu resultado é armazenado na variável à esquerda. O operador de atribuição pode ser o &lt;- ou o =, preferencialmente utilizaremos o &lt;- para a criação dos objetos e deixaremos o = para a definição de valores em argumentos dentro da funções. O atalho para o símbolo de atribuição é ALT+\\(-\\). texto&lt;-&quot; Olá, eu sou um texto.&quot; x_chr&lt;-&quot;1&quot; # não pode ser feito cálculo c&lt;-1 # valor numérico inteiro PI&lt;-3.14 # valor real, de ponto flutuante logico_1&lt;-TRUE # valor lógico logico_2&lt;-FALSE # valor lógico 5.6 Operadores Aritméticos: são as operações aritméticas básicas. Operador Tipo Operação Prioridade - Unário Inversão do Sinal 1 + Unário Manutenção do Sinal 1 sqrt(x) Binário Radiciação 2 x^y ou x**y Binário Potenciação 2 % Binário Resto da divisão inteira 3 / Binário Divisão 3 * Binário Multiplicação 3 - Binário Subtração 4 + Binário Adição 4 Relacionais: são operadores binários (de mesma prioridade) que somente retornam os valores lógicos Verdadeiro (TRUE) ou Falso (FALSE). Operador Comparação &gt; maior que &lt; menor que &gt;= maior ou igual &lt;= menor ou igual == igual a != diferente de Estes somente são usados para efetuar comparações, as quais só podem ser feitas entre dados do mesmo tipo. O resultado de uma comparação é sempre um valor lógico V (TRUE) ou F(FALSE). Lógicos ou Booleanos: são usados para combinar expressões relacionais e lógicas. Também retornam como resultado valores lógicos Verdadeiro (TRUE) ou Falso (FALSE). Operador Tipo Operação Prioridade NÃO (!) Unário Negação 1 E (&amp;) Binário Conjunção 2 OU (|) Binário Disjunção 3 5.7 Operação aritmética \\[ \\begin{aligned} &amp; a) 1+7 = 8\\\\ &amp; b)1 - 2 \\times10 = 19\\\\ &amp; c) 2^{10} = 1024\\\\ &amp; d) \\frac{10}{3} = 3,33333 \\end{aligned} \\] 1 + 7 1 - 2 * 10 2 ** 10 10/3 5.8 Funções matemáticas e trigonométricas \\[ \\begin{aligned} &amp; a)\\;\\sqrt{9} = 3\\\\ &amp; b)\\;seno\\;\\pi = 0\\\\ &amp; c)\\;5! =120\\\\ &amp; d)\\; e^5 = 148,4132 \\\\ &amp; e=2,718282 \\end{aligned} \\] sqrt(9) sin(pi) factorial(5) exp(5) exp(1) 5.9 Operações Relacionais \\[ \\begin{aligned} &amp; a)\\;7 &gt; 5 \\text{ retorna &#39;verdadeiro&#39;}\\\\ &amp; b)\\;8 \\leq 4 \\text{ retorna &#39;falso&#39;}\\\\ &amp; c)\\;5 = \\frac{25}{5} \\text{ retorna &#39;verdadeiro&#39;}\\\\ &amp; d)\\; 4\\neq 8 \\text{ retorna &#39;verdadeiro&#39;} \\end{aligned} \\] 7 &gt; 5 ; 4 != 8 ## [1] TRUE ## [1] TRUE nome &lt;- &#39;Alan&#39; nome == &#39;Rodrigo&#39; ## [1] FALSE nome == &quot;Alan&quot; ## [1] TRUE 5.10 Operações Lógicas \\[ \\begin{aligned} &amp; a)\\;7 &gt; 5 \\; OU \\; 8 \\leq 4 \\text{ retorna &#39;verdadeiro&#39;}\\\\ &amp; b)\\;5 = \\frac{25}{5} \\; E \\; 4 &gt; 8 \\text{ retorna &#39;falso&#39;} \\\\ &amp; c) \\text{ Não} \\; \\text{TRUE}\\; \\text{retorna &#39;falso&#39;} \\\\ &amp; d) \\text{ Não} \\; \\text{FALSE} \\; \\text{retorna &#39;verdadeiro&#39;} \\end{aligned} \\] 7 &gt; 5 | 8 &lt;= 4 5 == 25/5 &amp; 4 &gt; 8 !TRUE !FALSE x &lt;- c(1, 4, 2, NA, 8) is.na(x) x[!is.na(x)] 5.11 Exercícios Resolva as seguintes expressões: \\[ \\begin{aligned} &amp; a)\\; log\\;3 \\\\ &amp; b)\\; ln\\;10\\\\ &amp; c)\\; e^{2,302585}\\\\ &amp; d)\\; \\sqrt{225}\\\\ &amp; e)\\; 5!\\\\ &amp; f)\\; seno\\;30°\\\\ \\end{aligned} \\] Algumas funções do R Função Significado/ação q() Sair do programa. save.image() Salva o trabalho realizado. ls() Lista todos os objetos da área de trabalho atual. rm(x) Remove o objeto x. rm(x,y) Remove os objetos x e y. rm(list=ls(all=TRUE)) Remove todos os objetos (R Console  Misc/Remover todos os objetos). is.na(x) Verdadeiro se existir dado(s) ausente(s) no objeto x. sqrt(x) raiz quadrada de x. log(x,n) logaritmo de x na base n. log(x) logaritmo neperiano de x. log10(x) logaritmo decimal de x. exp(x) antilogaritmo - ex. sin(x) seno de x (em radianos). asin(x) arco-seno de x. abs(x) modulo(x). factorial(x) x !. floor(x) maior inteiro &lt; x. ceiling(x) menor inteiro &gt;x. trunc(x) inteiro de x, descartando seus decimais. round(x, digits=0) arredondando o valor x para um inteiro. signif(x, digits=6) apresentar 6 dígitos significativos de x. runif(n) gera n números aleatórios entre 0 e 1 a partir de uma distribuição uniforme. c() concatenação, criação de vetores. "],["algoritmos.html", "6 Algoritmos 6.1 Estruturas de Controle 6.2 Exercícios", " 6 Algoritmos Definição: Sequência lógica e não ambígua de instruções que levam à solução de um problema num tempo finito. -Sequência lógica: As instruções devem ser definidas em uma ordem correta. -Não ambígua: A sequência lógica e as instruções não devem dar margem à dupla interpretação. -Solução de um problema: A sequência lógica deve resolver exatamente (nem mais e nem menos) o problema identificado. -Tempo finito: A sequência lógica não deve possuir iterações infinitas. -Um algoritmo é uma solução e não a solução de um problema. -Um problema pode ser resolvido por mais de um algoritmo! SEMPRE. -Tarefas que possuem padrão de comportamento podem ser descritas por um algoritmo. Entender algoritmos é fundamental para desenvolver o raciocínio lógico e conceber uma solução a um dado problema, independente de uma linguagem de programação. (Ex: Fortran, Pascal, C e Python, R). A partir do algoritmo desenvolvido, fica mais fácil implementar o respectivo programa. 6.1 Estruturas de Controle Na criação de algoritmos, utilizamos os conceitos de bloco lógico, entrada e saída de dados, variáveis, constantes, atribuições, expressões lógicas, relacionais e aritméticas, bem como comandos que traduzam esses conceitos de forma a representar o conjunto de ações. Para que esse conjunto de ações se torne viável, deve existir uma perfeita relação lógica intrínseca ao modo pelo qual essas ações são executadas, ao modo pelo qual é regido o fluxo de execução do algoritmo. Por meio das estrutura básicas de controle do fluxo de execução  sequencial, seleção, repetição e da combinação delas  poderemos criar algoritmos para solucionar vários problemas. Estruturas básicas de um algoritmo: 6.1.1 Sequêncial Representa o Início/Fim, e define uma estrutura onde as instruções serão executadas na ordem que aparecem. Corresponde ao fato de que o conjunto de ações primitivas será executado em uma sequência linear de cima para baixo da esquerda para a direita. Para exemplificarmos, vamos resolver o seguinte exercício. Exemplo: Vamos construir um algoritmo que calcule a média aritmética entre quatro notas bimestrais fornecidas: # Entrada n1 &lt;- 4 n2 &lt;- 5 n3 &lt;- 6 n4 &lt;- 7 # Processamento media &lt;- (n1+n2+n3+n4) / 4 # Saída media ## [1] 5.5 6.1.2 Seleção a) Se-Então/Senão Define uma estrutura condicional que, dada a sua avaliação (Verdadeira ou Falsa), determina qual caminho do algoritmo será executado. Permite a escolha de um grupo de ações (bloco) a ser executado quando determinadas condições, representadas por expressões lógicas e/ou relacionais, são satisfeitas ou não. Os tipo de seleção apresentados serão: Simples, Composta e Encadeada. Exemplo - Seleção Simples: Classifique um valor qualquer fornecido X, se é maior que zero. Quando precisamos testar uma certa condição antes de executar uma ação. X &lt;- 7 if(X &gt; 0) { # Início do bloco Verdadeiro print(&quot;Valor maior que zero&quot;) } # Final do bloco Verdadeiro ## [1] &quot;Valor maior que zero&quot; OBS: Quando houver somente uma ação primitiva, a estrutura pode ser: X &lt;- 7 if(X &gt; 0) print(&quot;Valor maior que zero&quot;) ## [1] &quot;Valor maior que zero&quot; Contudo, se atribuírmos um valor negativo a X, esse algoritimo não apresentará retorno. X &lt;- -5 if(X &gt; 0) print(&quot;Valor maior que zero&quot;) Exemplo - Seleção Composta: Classifique um valor qualquer fornecido X, se é maior que zero, ou menor ou igual a zero. Utilizadas em situações em que duas alternativas dependem de uma mesma condição: uma da condição VERDADEIRA, e outra da condição FALSA. X &lt;- -0.8987 if(X &gt; 0){ print(&quot;Valor maior que zero&quot;) }else{ # Início do bloco Falso print(&quot;Valor menor ou igual a zero&quot;) } # Fim do bloco Falso ## [1] &quot;Valor menor ou igual a zero&quot; Exemplo - Seleção Encadeada: Classifique um valor qualquer fornecido X, se é maior que zero, menor do que zero ou igual a zero. É o agrupamento de várias seleções, ocorre quanto uma determinada ação, ou bloco deve ser executado se um grande conjunto de possibilidades ou combinações de situações for satisfeito. X &lt;- 0.1 if(X &gt; 0){ print(&quot;Valor maior que zero&quot;) }else{ if(X&lt; 0 ){ print(&quot;Valor menor que zero&quot;) }else{ print(&quot;Valor igual a zero&quot;) } } ## [1] &quot;Valor maior que zero&quot; Observe que as etruturas acima apresentadas são funcionais para classificação de um único valor. Contudo para a classificação de vários valores em um vetor, recomenda-se o uso da função if_else do pacote dplyr disponível no tidyverse ou a função ifelse do pacote base do R. library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 4.1.2 ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.4 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.0.2 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x dplyr::recode() masks car::recode() ## x dplyr::select() masks MASS::select() ## x purrr::some() masks car::some() numeros &lt;- c(-0.8, 1, 5, 0, -4) if_else(numeros&lt;0,&quot;Negativo&quot;,if_else(numeros==0,&quot;Nulo&quot;,&quot;Positivo&quot;)) ## [1] &quot;Negativo&quot; &quot;Positivo&quot; &quot;Positivo&quot; &quot;Nulo&quot; &quot;Negativo&quot; ifelse(numeros&lt;0,&quot;Negativo&quot;,if_else(numeros==0,&quot;Nulo&quot;,&quot;Positivo&quot;)) ## [1] &quot;Negativo&quot; &quot;Positivo&quot; &quot;Positivo&quot; &quot;Nulo&quot; &quot;Negativo&quot; b) Selecione caso Muitas vezes nos deparamos com situações onde são necessários duas ou mais operações condicionais para classificação. Nessa situações poderemos utilizar a estrutura Selecione caso, que no R pode ser acessada com a função case_when (do pacote dplyr). Essa função é útil para vetorizar instruções condicionais. Isso é semelhante a if_else mas pode gerar qualquer número de valores, em vez de apenas TRUE ou FALSE. Aqui está um exemplo que retorna o dia da semana em função do número de 1 a 7, onde qualquer valor fora desse intervalo seja classificado como número inválido. dia &lt;- c(0,1,2,3,4,5,6,7,8) case_when( dia == 1 ~ &quot;Domingo&quot;, dia == 2 ~ &quot;Segunda-feira&quot;, dia == 3 ~ &quot;Terça-feira&quot;, dia == 4 ~ &quot;Quarta-feira&quot;, dia == 5 ~ &quot;Quinta-feira&quot;, dia == 6 ~ &quot;Sexta-feira&quot;, dia == 7 ~ &quot;Sábado&quot;, TRUE ~ &quot;número inválido&quot; ) ## [1] &quot;número inválido&quot; &quot;Domingo&quot; &quot;Segunda-feira&quot; &quot;Terça-feira&quot; ## [5] &quot;Quarta-feira&quot; &quot;Quinta-feira&quot; &quot;Sexta-feira&quot; &quot;Sábado&quot; ## [9] &quot;número inválido&quot; 6.1.3 Repetição Enquanto, Faça-Enquanto ou Para Define uma estrutura de iteração condicional (V ou F) ou contada (pré-definida) de instruções. É uma estrutura da controle do fluxo de execução que permite repetir diversas vezes um mesmo trecho do algoritmo, porém, sempre verificando ANTES de cada execução se é permitido executar o mesmo trecho. a) Repetição com teste no Início O Enquanto (while) permite que um determinado comando (ou bloco) seja repetido enquanto uma determinada for VERDADEIRA. Muitas vezes precisamos estabelecer um modo de contagem (contador), ou seja, uma variável (\\(i\\) por exemplo) com um dado valor inicial que é incrementado a cada repetição. Quando o resultado da for FALSO o comando de repetição é abandonado. Se na primeira vez o resultado for FALSO, os comandos NÃO SÃO EXECUTADOS. Exemplo: Imprima os números menores ou iguais a 10. i=1 while(i&lt;=10){ print(i) i=i+1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 b) Repetição com teste no Final Uma das estruturas com teste no final é a Repita (repeat), que permite que um comando, ou bloco de comandos sejam executados enquanto uma determinada seja FALSA. Verificamos que, devido a sua sintaxe os comandos dentro do bloco São Executados Pelo Menos Uma Vez, independentemente da validade da condição. Isso ocorre pois a inspeção da ocorre no FINAL da estrutura. Quando o resultado da for VERDADEIRO o comando de repetição é abandonado. Entretanto, os comandos SÃO EXECUTADOS PELO MENOS UMA VEZ. Exemplo: Imprima os números menores ou iguais a 10. Observe que utilizamos o comando break juntamente com o a estrutura if, ou seja, se a for verdadeira o break interromperá a iteração o controle flui para fora do laço. i=1 repeat{ print(i) i=i+1 if(i&gt;10) break } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 c) Repetição com Variável de Controle As estruturas while e repeat ocorrem em casos de difícil determinação do número de vezes que um comando, ou bloco, será executado. A Estrutura Para (for) é diferente, já que sempre repete a execução do bloco um número pré-determinado de vezes, pois ela não prevê uma condição e possui limites fixos. Exemplo: Imprima os números menores ou iguais a 10. for(i in 1:10){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 Exemplo: Elabore um algoritmo que, utilizando uma das estruturas de repetição imprima a tabuada do número 5. for(i in 1:10){ print(paste(&quot;5 x&quot;,i,&quot;=&quot;,i*5)) } ## [1] &quot;5 x 1 = 5&quot; ## [1] &quot;5 x 2 = 10&quot; ## [1] &quot;5 x 3 = 15&quot; ## [1] &quot;5 x 4 = 20&quot; ## [1] &quot;5 x 5 = 25&quot; ## [1] &quot;5 x 6 = 30&quot; ## [1] &quot;5 x 7 = 35&quot; ## [1] &quot;5 x 8 = 40&quot; ## [1] &quot;5 x 9 = 45&quot; ## [1] &quot;5 x 10 = 50&quot; 6.2 Exercícios 1) Crie uma script/função que peça dois números e imprima o maior deles. 2) Crie um script/função que peça um número e informe se o número é inteiro ou decimal. 3) Crie um script/função que leia um número de 1 a 7 e exiba o dia correspondente da semana. (1- Domingo, 2- Segunda, , 7-Sábado.). Caso o usuário digitar um número diferente o programa deve escrever Valor invalido. 4) Crie um script/função que verifique se uma letra digitada é vogal ou consoante. 5) Crie um script/função que leia três números e mostre o maior e o menor deles. 6) Elabore um algoritmo que, classifique um número \\(X\\), fornecido pelo usuário, em par ou ímpar, utilize o operador % para o cálculo do resto da divisão. 7) Construa um algoritmo sequencial que calcule as raízes de uma equação do 2º grau (\\(ax^2 + bx + c\\)), sendo os valores de \\(a\\), \\(b\\) e \\(c\\) devem ser fornecidos pelo usuário. Considere: \\[ \\Delta = b^2-4 a c \\] se \\(\\Delta &gt; 0\\) \\[ x_1 = \\frac{-b+\\sqrt{\\Delta} }{2a} \\\\ x_2 = \\frac{-b-\\sqrt{\\Delta} }{2a} \\] se \\(\\Delta = 0\\) \\[ x= \\frac{-b}{2a} \\] se \\(\\Delta &lt; 0\\) As raízes são imaginárias 8) Faça um programa que imprima uma frase \\(n\\) vezes na tela do computador, \\(n\\) deve ser um número fornecido pelo usuário. 9) Faça um programa que imprima na tela os números de 1 a 20. 10) Faça um programa para obter as sequências de 0 a 25 com passo igual a 2. 11) Escreva um programa no qual o usuário digite dois números e o programa deve apresentar todos os números inteiros entre esses dois números. Se os números forem iguais, o programa deve exibir uma mensagem dizendo para o usuário digitar dois números inteiros diferentes. 12) Elabore um algoritmo que calcule e escreva o valor S, em que: \\[ S = 1 - \\frac{2}{4} + \\frac{3}{9} - \\frac{4}{16} + \\cdots - \\frac{10}{100} = 0,6456349 \\] 13) Construa um algoritmo que verifique se o número fornecido pelo usuário (inteiro maior que 1) é primo ou não (números primos são os números naturais que têm apenas dois divisores o 1 e ele mesmo, exemplo (2, 3, 5, 7, 11, 13, 17). "],["estrutura-de-dados.html", "7 Estrutura de dados 7.1 Atomic vector 7.2 Factor 7.3 Matrizes e Arrays 7.4 Listas 7.5 Data frames", " 7 Estrutura de dados A declaração de variáveis, uma a uma, é suficiente para a codificação algorítmica da solução de uma ampla gama de problemas, entretanto, esse tipo de declaração é insuficiente para resolver um grande número de problemas computacionais. A quantidade de tipos de dados primitivos (caractere, real, inteiro, lógico) não é suficiente para representar toda e qualquer informação que possa surgir. Assim, em muitas situações, esses recursos de representação são escassos, o que poderá ser suprimido se existisse mais tipos de dados ou, ainda melhor, se esses tipos pudessem ser construídos, à medida que fossem necessários. Portanto, vamos construir novos tipos de dados a partir da composição de tipos primitivos já estudados. Esses novos tipos tem o formato denominado estrutura de dados que define como os tipos primitivos são organizados. Tipos básicos de estrutura no R: Atomic vector: homogêneo e unidimensional Factor: homogêmeo e unidimensional Matriz: homogêneo e bidimensional Array: homogêneo e multidimensional Lista: heterogêneo Data frame: heterogêneo 7.1 Atomic vector Atomic vectors são a estrutura de objetos mais simples do R, caracterizados por não terem dimensão. Podem ser vistos como uma caixa com um rótulo ou nome colado a ela, que num dado instante guarda um determinado objeto, essa caixa pode ter seu conteúdo alterado diversas vezes. Tipos de atomic vectos: lógico integer double complexo character Figure 7.1: O objeto identificado como X possui um tipo numérico inteiro cujo valor é 5. (meu_inteiro &lt;- 5) ## [1] 5 (meu_double &lt;- 8.50) ## [1] 8.5 (meu_logico &lt;- TRUE) ## [1] TRUE (meu_char &lt;- &quot;A&quot;) ## [1] &quot;A&quot; De forma análoga, no conceito de estrutura de dados uma caixa poderá comportar não apenas uma e somente uma informação, a caixa comportará um conjunto de dados, desde que previamente organizada, ou seja, dividida em compartimentos. A função c(), o c é de concatenate utilizada para criação de um atomic vector com mais de um valor. Figure 7.2: O objeto identificado como X possui um tipo numérico inteiro cujos valores são 5, 8, 0, 2, 1 e 9. X &lt;- c(5,8,0,2,1,9) Y &lt;- c(0.5, 0.8, 1.5, 6.8) L &lt;- c(TRUE, FALSE, T, F) M &lt;- c(&quot;A&quot;, &quot;mais&quot;, &quot;bonita&quot;) As funções class() e mode() auxiliam na determinação do tipo de objeto. class(X) ## [1] &quot;numeric&quot; mode(X) ## [1] &quot;numeric&quot; class(M) ## [1] &quot;character&quot; mode(M) ## [1] &quot;character&quot; Observação: Coerção ocorre quando dois tipos de objetos são inseridos uma estrutura homogênea (atomic vectors, arrays ou matrizes), o R converterá o objeto para o tipo mais flexível, na ordem: 1-Lógico 2-Inteiro 3-Double 4-Caracter c(T,3,5.5,&quot;bela&quot;) ## [1] &quot;TRUE&quot; &quot;3&quot; &quot;5.5&quot; &quot;bela&quot; c(T,3,5.5) ## [1] 1.0 3.0 5.5 7.2 Factor Factors ou simplesmente Fatores são utilizados para armazernar dados categorizados e são caracterizados por conterem apenas valores pré-definidos, chamados de níveis do fator (levels) e se basearem num vetor de inteiros. Dentro do objeto, os levels são organizados em ordem alfabética. # Função factor trat&lt;-factor(c(&quot;T1&quot;,&quot;T1&quot;,&quot;T1&quot;,&quot;T2&quot;,&quot;T2&quot;,&quot;T2&quot;,&quot;T3&quot;,&quot;T3&quot;,&quot;T3&quot;)) trat ## [1] T1 T1 T1 T2 T2 T2 T3 T3 T3 ## Levels: T1 T2 T3 # Função gl (Generate factors levels) TRAT &lt;- gl(3,3, labels = c(&quot;T1&quot;,&quot;T2&quot;,&quot;T3&quot;) ) TRAT ## [1] T1 T1 T1 T2 T2 T2 T3 T3 T3 ## Levels: T1 T2 T3 # Extraindo o níveis de um fator levels(trat) ## [1] &quot;T1&quot; &quot;T2&quot; &quot;T3&quot; 7.3 Matrizes e Arrays Matrizes e arrays são definidos usando as funções matrix() e array(), respectivamente. São multidimensionais. No caso da matriz esses objetos tem 2 dimensões e os compartimentos podem estar arranjados dessa maneira: Figure 7.3: O objeto identificado como X possui um tipo numérico inteiro bidimensional organizado na forma matricial com 6 linhas e 6 colunas totalizando 36 elementos. No R dois argumentos são utilizados na função matrix() um para determinar o número de colunas e a forma com a qual será o preeenchimento dos elementos na matriz, por linha (byrow=FALSE, default) ou por coluna (byrow=TRUE). Compare as duas matriz produzidas no exemplo abaixo. Preenchimento da matriz por coluna. mat1 &lt;- matrix(1:36, ncol = 6, byrow = FALSE) mat1 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 7 13 19 25 31 ## [2,] 2 8 14 20 26 32 ## [3,] 3 9 15 21 27 33 ## [4,] 4 10 16 22 28 34 ## [5,] 5 11 17 23 29 35 ## [6,] 6 12 18 24 30 36 Preenchimento da matriz por linha. mat2 &lt;- matrix(1:36, ncol = 6, byrow = TRUE) mat2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 3 4 5 6 ## [2,] 7 8 9 10 11 12 ## [3,] 13 14 15 16 17 18 ## [4,] 19 20 21 22 23 24 ## [5,] 25 26 27 28 29 30 ## [6,] 31 32 33 34 35 36 Arrays são objetos que podem conter dois ou mais dados bidimensionais. Por exemplo, em matrizes quadradas podem conter duas linhas e duas colunas e a dimensão pode ter cinco. Os arrays podem armazenar os valores tendo apenas um tipo semelhante de tipos de dados. Os dados podem ser mais de uma dimensão, onde existem linhas e colunas e dimensões de algum comprimento. A função array() possui dois argumentos, data que receberá o vetor contendo os elementos e dim que receberá um vetor decrevendo o tamanho de cada dimensão. Vamos criar um array com duas matrizes de 4 linhas e 3 colunas: meu_array &lt;- array(1:24, c(4,3,2)) meu_array ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 13 17 21 ## [2,] 14 18 22 ## [3,] 15 19 23 ## [4,] 16 20 24 Os nomes das linhas, colunas e matrizes do array podem ser definidos a partir de uma lista aplicada ao argumento dimnames da função array(), como apresentado abaixo: linhas &lt;- c(&quot;Lin1&quot;, &quot;Lin2&quot;, &quot;Lin3&quot;, &quot;Lin4&quot;) colunas &lt;- c(&quot;Col1&quot;,&quot;Col2&quot;,&quot;Col3&quot;) matrizes&lt;- c(&quot;Matriz1&quot;, &quot;Matriz2&quot;) meu_array &lt;- array(1:24, c(4,3,2), dimnames = list(linhas,colunas, matrizes) ) meu_array ## , , Matriz1 ## ## Col1 Col2 Col3 ## Lin1 1 5 9 ## Lin2 2 6 10 ## Lin3 3 7 11 ## Lin4 4 8 12 ## ## , , Matriz2 ## ## Col1 Col2 Col3 ## Lin1 13 17 21 ## Lin2 14 18 22 ## Lin3 15 19 23 ## Lin4 16 20 24 Para saber qual é o tipo de um objeto, vamos utilizar as funções typeof() e class. # Array typeof(meu_array) ## [1] &quot;integer&quot; class(meu_array) ## [1] &quot;array&quot; # Matriz typeof(mat1) ## [1] &quot;integer&quot; class(mat1) ## [1] &quot;matrix&quot; &quot;array&quot; # Fator typeof(trat) ## [1] &quot;integer&quot; class(trat) ## [1] &quot;factor&quot; # Atomic vector typeof(Y) ## [1] &quot;double&quot; class(Y) ## [1] &quot;numeric&quot; As funções length(), dim(), nrow() e ncol() são usadas para determinar o comprimento de cada dimensão de um objeto. Observe que atomic vector e factor são estruturas unidimensionais, assim, somente a função length() retornará o número de elementos do objeto, e as demais função retornarão valor nulo NULL. # Array length(meu_array) ## [1] 24 dim(meu_array) ## [1] 4 3 2 nrow(meu_array) ## [1] 4 ncol(meu_array) ## [1] 3 # Matriz length(mat1) ## [1] 36 dim(mat1) ## [1] 6 6 nrow(mat1) ## [1] 6 ncol(mat1) ## [1] 6 # Fator length(trat) ## [1] 9 dim(trat) ## NULL nrow(trat) ## NULL ncol(trat) ## NULL # Atomic vector length(Y) ## [1] 4 dim(Y) ## NULL nrow(Y) ## NULL ncol(Y) ## NULL Todos os objetos apresentados até agora (atomic vector, factor, matrix e arrays) podem ser classificados como estruturas de dados HOMOGÊNEA, pois são compostas por variáveis do mesmo tipo primitivo. Figure 7.4: Estrutura de dados Homogênea é uma coleção de variáveis de mesmo tipo, acessíveis com um único nome e armazenados contiguamente (um após o outro) na memória. Se um conjunto homogêneo de dados é composto por variáveis do mesmo tipo, um conjunto HETEROGÊNEO de dados é composto por elementos que não são do mesmo tipo primitivo. No R temos as listas e os data frames. Figure 7.5: As estruturas heterogêneas constituem um recurso importante para a organização dos dados utilizados devido à possibilidade de tratar um grupo de valores como uma única variável, similar às estruturas homogêneas. 7.4 Listas São estrutras de dados heterogêneas, ou seja, podem ser formadas por vetores de diferentes tipos de dados e de diferentes comprimentos. Em R são frequentemente utilizadas para a passagem de vários argumentos de controle dentro de uma função. Começaremos criando uma lista que consiste em três partes: 4 informações de caracteres em um vetor denominado a, 8 informações numéricas em um vetor denominado b e 3 informações lógicas em um vetor denominado c: a&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;) b&lt;-c(1,2,3,4,4,3,2,1) c&lt;-c(T,T,F) Agora vamos utilizar a função list() para agrupar esses objetos. minha_lista &lt;- list(a,b,c) minha_lista ## [[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## ## [[2]] ## [1] 1 2 3 4 4 3 2 1 ## ## [[3]] ## [1] TRUE TRUE FALSE As funções typeof() e class retornam list e a função length retorna o número de atomic vectors utilizados na criação da lista. Funções como dim(), nrow() e ncol() retornarão NULL. typeof(minha_lista) ## [1] &quot;list&quot; class(minha_lista) ## [1] &quot;list&quot; length(minha_lista) ## [1] 3 Para conseguirmos o número de elementos em cada objeto dentro da lista, poderemos utlizar a função lapply() junto com a função length(). lapply(minha_lista,length) ## [[1]] ## [1] 4 ## ## [[2]] ## [1] 8 ## ## [[3]] ## [1] 3 A função lapply pode ser utilizada juntamente com typeof e/ou class. lapply(minha_lista,typeof) ## [[1]] ## [1] &quot;character&quot; ## ## [[2]] ## [1] &quot;double&quot; ## ## [[3]] ## [1] &quot;logical&quot; 7.5 Data frames Semelhantes às listas, os data frames São estrutras de dados heterogêneas, que podem ser formadas por vetores de diferentes tipos de dados, contudo, todos os vetores têm o mesmo comprimento. São definidos usando a função data.frame(). Um data frame é um objeto com linhas e colunas (parecido com a matriz). As linhas de um data frame contêm diferentes observações do estudo em questão, denominadas registros. As colunas, por sua vez, contêm os valores de diferentes variáveis. Os valores no corpo de uma matriz podem ser apenas números; aqueles em um data.frame também podem ser números, mas também podem ser texto (por exemplo, os nomes dos níveis de fator para variáveis categóricas, como masculino ou feminino em uma variável chamada gênero), eles podem ser datas do calendário (por exemplo, 05/01/21) , ou podem ser variáveis lógicas (TRUE ou FALSE). A etapa mais importante do Data Science é obter o data frame absolutamente correto. A expectativa é que você tenha usado uma planilha eletrônica, como o Excel, para inserir e editar os dados e o que requer alguma prática é aprender exatamente como colocar seus números na planilha. Existem inúmeras maneiras de fazer errado, mas apenas uma maneira de fazer certo. E esta não é a maneira que a maioria das pessoas acha intuitivamente a mais óbvia. O principal é o seguinte: todos os valores da mesma variável devem estar na mesma coluna. Se você fez um experimento com três tratamentos (controle, pré-aquecido e pré-resfriado) e quatro medições por tratamento, pode parecer uma boa ideia criar a planilha como esta: controle pré-aquecido pré-resfriado 6.1 6.3 7.1 5.9 6.2 8.2 5.8 5.8 7.3 5.4 6.3 6.9 No entanto, este não é um data frame, pois os valores da variável resposta aparecem em três colunas diferentes, em vez de todos na mesma coluna. A forma correta de inserir esses dados é ter duas colunas: uma para a variável resposta e outra para os níveis do fator experimental denominada Tratamento (controle, pré-aquecido e pré-resfriado). Aqui estão os mesmos dados, inseridos corretamente como um data.frame: Resposta Tratamento 6.1 controle 5.9 controle 5.8 controle 5.4 controle 6.3 pré-aquecido 6.2 pré-aquecido 5.8 pré-aquecido 6.3 pré-aquecido 7.1 pré-resfriado 8.2 pré-resfriado 7.3 pré-resfriado 6.9 pré-resfriado Para construir esse data frame, primeiramente vamos criar os distintos vetores, numérico e fator. Resposta &lt;- c(6.1,5.9,5.8,5.4,6.3,6.2,5.8,6.3,7.1,8.2,7.3,6.9) Tratamento &lt;- gl(3,4,labels = c(&quot;controle&quot;,&quot;pré-aquecido&quot;,&quot;pré-resfriado&quot;) ) Agrora vamos agrupar esses vetores em um data frame denomidao dados. dados &lt;- data.frame(Resposta, Tratamento) dados ## Resposta Tratamento ## 1 6.1 controle ## 2 5.9 controle ## 3 5.8 controle ## 4 5.4 controle ## 5 6.3 pré-aquecido ## 6 6.2 pré-aquecido ## 7 5.8 pré-aquecido ## 8 6.3 pré-aquecido ## 9 7.1 pré-resfriado ## 10 8.2 pré-resfriado ## 11 7.3 pré-resfriado ## 12 6.9 pré-resfriado Agora vamos extrair algumas informações a respeito desse data frame: Utilize a função length() para retornar o número de vetores (colunas) utilizados para criação do data frame. length(dados) ## [1] 2 A função dim() retornará o número de linhas e o número de colunas do objeto, em analogia, poder-se-ia utilizar as funções nrow() e ncol(). dim(dados) ## [1] 12 2 nrow(dados) ## [1] 12 ncol(dados) ## [1] 2 A função glimpse() do pacote dplyr fornece um resumo rápido a erspeito do data frame, semelhante à função, já estudada str(): Observe que vamos chamar a função glimpse utilizado o :: definindo anteriormente o nome do pacote dplyr na qual ela se encontra, sem a necessidade prévia de carregarmos esse pacote no ambiente de trabalho do R. dplyr::glimpse(dados) ## Rows: 12 ## Columns: 2 ## $ Resposta &lt;dbl&gt; 6.1, 5.9, 5.8, 5.4, 6.3, 6.2, 5.8, 6.3, 7.1, 8.2, 7.3, 6.9 ## $ Tratamento &lt;fct&gt; controle, controle, controle, controle, pré-aquecido, pré-a~ A função names() retorna o nome das colunas (vetores) de um data frame. names(dados) ## [1] &quot;Resposta&quot; &quot;Tratamento&quot; A função names() também pode ser utilizada para atribuir/modificar os nomes das colunas do data frame. names(dados) &lt;- c(&quot;Altura_cm&quot;, &quot;Método&quot;) dados ## Altura_cm Método ## 1 6.1 controle ## 2 5.9 controle ## 3 5.8 controle ## 4 5.4 controle ## 5 6.3 pré-aquecido ## 6 6.2 pré-aquecido ## 7 5.8 pré-aquecido ## 8 6.3 pré-aquecido ## 9 7.1 pré-resfriado ## 10 8.2 pré-resfriado ## 11 7.3 pré-resfriado ## 12 6.9 pré-resfriado O ponto principal sobre como trabalhar efetivamente com data frames é entendermos o uso de subscritos (ou indexação). No R, os subscritos aparecem entre colchetes [ , ]. Lembre-se que um data frame é um objeto bidimensional, compreendendo linhas e colunas. As linhas são referenciadas pelo primeiro subscrito (à esquerda da vírgula) , as colunas pelo segundo subscrito (à direita da vírgula). Portanto, para extrairmos a segunda observação da variável resposta Altura_cm usamos: dados[2,1] ## [1] 5.9 Observe que o retorno foi um atomic vector (vetor). Para extrairmos um subconjuto desse data.frame, composto somente pelos valores de altura referentes ao método pré-aquecido (linhas 5 a 8), vamos utilizar os :, deixando o índice das colunas em branco na indexação. dados[5:8,] ## Altura_cm Método ## 5 6.3 pré-aquecido ## 6 6.2 pré-aquecido ## 7 5.8 pré-aquecido ## 8 6.3 pré-aquecido Observe que o retorno foi um outro data frame. Se o objetivo for apenas os valores numéricos dos pré-aquecidos, temos. dados[5:8,1] ## [1] 6.3 6.2 5.8 6.3 Observe que o retorno foi um atomic vector. Poderíamos extrair todos os valores a partir do operador de acesso de coluna $, chamando primeiramente o objeto e em seguida o nome da coluna. O retorno sempre será um vetor. dados$Altura_cm ## [1] 6.1 5.9 5.8 5.4 6.3 6.2 5.8 6.3 7.1 8.2 7.3 6.9 Observe a diferença no retorno das diferentes formas de acesso a uma coluna. No caso abaixo, vamos acessar a coluna Altura_cm, cujo retorno será um vetor, semelhante ao exemplo anterior. dados[,1] ## [1] 6.1 5.9 5.8 5.4 6.3 6.2 5.8 6.3 7.1 8.2 7.3 6.9 Compare com extração abaixo, nesse caso, temos como retorno um data frame. dados[1] ## Altura_cm ## 1 6.1 ## 2 5.9 ## 3 5.8 ## 4 5.4 ## 5 6.3 ## 6 6.2 ## 7 5.8 ## 8 6.3 ## 9 7.1 ## 10 8.2 ## 11 7.3 ## 12 6.9 Transformações nos dados podem ser realizadas utilizando esse memso operador ($), por exemplo, vamos criar uma nova variável denominada Altura_m a partir da Altura_cm, dividindo cada um de seus valores por 100. dados$Altura_m &lt;- dados$Altura_cm / 100 dados ## Altura_cm Método Altura_m ## 1 6.1 controle 0.061 ## 2 5.9 controle 0.059 ## 3 5.8 controle 0.058 ## 4 5.4 controle 0.054 ## 5 6.3 pré-aquecido 0.063 ## 6 6.2 pré-aquecido 0.062 ## 7 5.8 pré-aquecido 0.058 ## 8 6.3 pré-aquecido 0.063 ## 9 7.1 pré-resfriado 0.071 ## 10 8.2 pré-resfriado 0.082 ## 11 7.3 pré-resfriado 0.073 ## 12 6.9 pré-resfriado 0.069 Agora criaremos a coluna Bloco para identificação dos blocos do experimento. dados$Bloco &lt;- gl(4, 1, 12, labels = c(&quot;I&quot;,&quot;II&quot;,&quot;III&quot;,&quot;IV&quot;)) dados ## Altura_cm Método Altura_m Bloco ## 1 6.1 controle 0.061 I ## 2 5.9 controle 0.059 II ## 3 5.8 controle 0.058 III ## 4 5.4 controle 0.054 IV ## 5 6.3 pré-aquecido 0.063 I ## 6 6.2 pré-aquecido 0.062 II ## 7 5.8 pré-aquecido 0.058 III ## 8 6.3 pré-aquecido 0.063 IV ## 9 7.1 pré-resfriado 0.071 I ## 10 8.2 pré-resfriado 0.082 II ## 11 7.3 pré-resfriado 0.073 III ## 12 6.9 pré-resfriado 0.069 IV Agora podemos modificar a posição das colunas dentro do data frame. Ou seja, vamos reciclar o objeto dados guardando esse novo data frame no mesmo identificador. dados &lt;- dados[,c(2,4,1,3)] dados ## Método Bloco Altura_cm Altura_m ## 1 controle I 6.1 0.061 ## 2 controle II 5.9 0.059 ## 3 controle III 5.8 0.058 ## 4 controle IV 5.4 0.054 ## 5 pré-aquecido I 6.3 0.063 ## 6 pré-aquecido II 6.2 0.062 ## 7 pré-aquecido III 5.8 0.058 ## 8 pré-aquecido IV 6.3 0.063 ## 9 pré-resfriado I 7.1 0.071 ## 10 pré-resfriado II 8.2 0.082 ## 11 pré-resfriado III 7.3 0.073 ## 12 pré-resfriado IV 6.9 0.069 Agora devemos criar uma coluna para a identificação (\\(ID\\)) para cada observação. dados$ID &lt;- paste(&quot;obs&quot;, 1:nrow(dados), sep=&quot;_&quot; ) dados ## Método Bloco Altura_cm Altura_m ID ## 1 controle I 6.1 0.061 obs_1 ## 2 controle II 5.9 0.059 obs_2 ## 3 controle III 5.8 0.058 obs_3 ## 4 controle IV 5.4 0.054 obs_4 ## 5 pré-aquecido I 6.3 0.063 obs_5 ## 6 pré-aquecido II 6.2 0.062 obs_6 ## 7 pré-aquecido III 5.8 0.058 obs_7 ## 8 pré-aquecido IV 6.3 0.063 obs_8 ## 9 pré-resfriado I 7.1 0.071 obs_9 ## 10 pré-resfriado II 8.2 0.082 obs_10 ## 11 pré-resfriado III 7.3 0.073 obs_11 ## 12 pré-resfriado IV 6.9 0.069 obs_12 E vamos reordenar o data frame por Altura_cm da menor para a maior (utilize a função order()), em seguida, da maior para a menor nota (argumento decreasing = T). # Classificação do menor para o maior dados[order(dados$Altura_cm),] ## Método Bloco Altura_cm Altura_m ID ## 4 controle IV 5.4 0.054 obs_4 ## 3 controle III 5.8 0.058 obs_3 ## 7 pré-aquecido III 5.8 0.058 obs_7 ## 2 controle II 5.9 0.059 obs_2 ## 1 controle I 6.1 0.061 obs_1 ## 6 pré-aquecido II 6.2 0.062 obs_6 ## 5 pré-aquecido I 6.3 0.063 obs_5 ## 8 pré-aquecido IV 6.3 0.063 obs_8 ## 12 pré-resfriado IV 6.9 0.069 obs_12 ## 9 pré-resfriado I 7.1 0.071 obs_9 ## 11 pré-resfriado III 7.3 0.073 obs_11 ## 10 pré-resfriado II 8.2 0.082 obs_10 # Classificação do maior para o menor dados[order(dados$Altura_cm,decreasing=TRUE),] ## Método Bloco Altura_cm Altura_m ID ## 10 pré-resfriado II 8.2 0.082 obs_10 ## 11 pré-resfriado III 7.3 0.073 obs_11 ## 9 pré-resfriado I 7.1 0.071 obs_9 ## 12 pré-resfriado IV 6.9 0.069 obs_12 ## 5 pré-aquecido I 6.3 0.063 obs_5 ## 8 pré-aquecido IV 6.3 0.063 obs_8 ## 6 pré-aquecido II 6.2 0.062 obs_6 ## 1 controle I 6.1 0.061 obs_1 ## 2 controle II 5.9 0.059 obs_2 ## 3 controle III 5.8 0.058 obs_3 ## 7 pré-aquecido III 5.8 0.058 obs_7 ## 4 controle IV 5.4 0.054 obs_4 "],["visualização-de-dados-com-o-ggplot2.html", "8 Visualização de dados com o ggplot2 8.1 Criando um gráfico 8.2 Definindo temas 8.3 Controlando elementos do tema 8.4 Mapeamento estéticos 8.5 Facetas 8.6 Objetos geométricos", " 8 Visualização de dados com o ggplot2 O R possui vários sistemas para fazer gráficos, mas o pacote ggplot2 é um dos mais elegantes e versáteis para realizar essa tarefa. O ggplot2 implementa a gramática dos gráficos, um sistema coerente para descrever e construir gráficos. Vamos carregar o tidyverse, que tem o pacote ggplot2 como um de seus elementos centrais. library(tidyverse) Importação dos dados Vamos realizar a importação, via web, do banco de dados geomorfologia.txt. URL &lt;- &quot;https://raw.githubusercontent.com/arpanosso/r_data_science_fcav/master/dados/geomorfologia.txt&quot; geomorfologia&lt;-read.table(URL, header = TRUE) glimpse(geomorfologia) ## Rows: 106 ## Columns: 22 ## $ SUP &lt;chr&gt; &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I~ ## $ Solo &lt;chr&gt; &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;L~ ## $ Amostra &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,~ ## $ X &lt;int&gt; 0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 32~ ## $ AMG &lt;dbl&gt; 0.2, 0.1, 0.7, 0.4, 0.4, 0.4, 1.2, 0.8, 1.1, 1.2, 0.1, 0.2, 0.~ ## $ AG &lt;dbl&gt; 3.72, 4.27, 5.00, 3.80, 3.10, 3.80, 3.60, 4.70, 4.50, 5.90, 5.~ ## $ AM &lt;dbl&gt; 20.4, 22.6, 22.7, 23.7, 22.3, 23.8, 23.1, 25.8, 25.5, 32.8, 33~ ## $ AF &lt;dbl&gt; 22.9, 23.6, 22.2, 24.4, 24.6, 19.1, 21.7, 21.1, 18.9, 19.8, 19~ ## $ AMF &lt;dbl&gt; 30.0, 28.4, 26.9, 26.7, 26.9, 27.1, 26.5, 24.7, 25.4, 21.7, 19~ ## $ SILTE &lt;dbl&gt; 1.2, 1.2, 1.2, 0.6, 2.1, 2.2, 0.7, 0.2, 2.5, 0.2, 2.5, 2.6, 0.~ ## $ ARGILA &lt;dbl&gt; 21.5, 20.4, 21.4, 20.5, 20.7, 23.5, 23.1, 22.7, 22.0, 18.5, 20~ ## $ S_A &lt;dbl&gt; 0.05, 0.05, 0.05, 0.02, 0.10, 0.09, 0.03, 0.01, 0.11, 0.01, 0.~ ## $ AF_AG &lt;dbl&gt; 6.16, 5.53, 4.44, 6.42, 7.94, 5.03, 6.03, 4.49, 4.20, 3.36, 3.~ ## $ P &lt;dbl&gt; 42, 22, 41, 27, 11, 12, 11, 16, 38, 25, 25, 6, 6, 7, 5, 4, 4, ~ ## $ pH &lt;dbl&gt; 4.2, 3.8, 4.8, 4.0, 4.4, 4.0, 4.8, 5.4, 4.4, 5.2, 4.5, 5.1, 4.~ ## $ K &lt;dbl&gt; 0.27, 0.11, 0.34, 0.13, 0.11, 0.14, 0.23, 0.28, 0.19, 0.14, 0.~ ## $ Ca &lt;dbl&gt; 1.4, 0.4, 2.4, 0.7, 1.4, 0.6, 1.6, 3.3, 1.6, 2.9, 1.3, 1.6, 0.~ ## $ Mg &lt;dbl&gt; 0.3, 0.1, 0.4, 0.1, 0.3, 0.1, 0.7, 1.3, 0.5, 1.7, 0.6, 0.8, 0.~ ## $ H_Al &lt;dbl&gt; 5.2, 5.8, 4.2, 5.2, 4.2, 5.2, 3.4, 2.5, 5.2, 3.1, 4.2, 2.5, 4.~ ## $ SB &lt;dbl&gt; 1.97, 0.61, 3.14, 0.93, 1.81, 0.84, 2.53, 4.88, 2.29, 4.74, 1.~ ## $ T &lt;dbl&gt; 7.17, 6.41, 7.34, 6.13, 6.01, 6.04, 5.93, 7.38, 7.49, 7.84, 6.~ ## $ V &lt;dbl&gt; 27, 10, 43, 15, 30, 14, 43, 66, 31, 60, 32, 50, 22, 35, 36, 34~ 8.1 Criando um gráfico Vamos criar um gráfico do teor de argila do solo ao longo do transecto estudado. Para isso utilizaremos dois operadores: 1) %&gt;% - chamado PIPE do pacote magrittr o qual pode ser construído com o atalho Ctrl + Shift + M. 2) + - adição, para o controle das camadas gráficas e parâmetros específicos após aplicar a função ggplot(). REGRA: o PIPE sempre opera um DATA FRAME e tem como retorno um novo DATA FRAME e para a ggplot(), o primeiro argumento SEMPRE será um data frame. A forma mais simples para iniciarmos a confecção do gráfico é construírmos cada etapa individual deste, estudando os códigos etapa por etapa. Portanto, mãos a obra. No código abaixo, construiremos somente a tela de plotagem do gráfico. geomorfologia %&gt;% ggplot() Se a nossa inteção é visualizar as alterações dos teores de argila ao longo do transecto nas diferentes superfícies geomórficas, precisamos construir uma gráfico de dispersão (scatter) entre a posição X e o teor de argila do solo. Para isso vamos utilizar a função de estética aes(), dentro da função ggplot(), para definir quais as colunas do objeto geomorfologia serão utilizadas para criação do gráfico. Observe que os eixos agora são desenhados. geomorfologia %&gt;% ggplot(aes(x=X, y =ARGILA)) A geometria de pontos agora deve ser indicada, ou seja, vamos adicionar uma nova camada a esse gráfico. Sempre utilizaremos o operador de adição + para adicioar uma camada de pontos a esse gráfico geom_point(). geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_point() ## Controlando geometrias Outras geometrias poderiam ser utlizadas, ao invés de pontos, poderíamos pedir uma geometria de linha por exemplo: geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_line() Ou poderíamos mesclar as duas geometrias: geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_line()+ geom_point() Agora, finalmente, podemos controlar algumas propriedades dentro de cada geometria. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_line(color=&quot;blue&quot;, # cor da linha lwd= 0.7, # espessura da linha linetype=2)+ # tipo de linha geom_point(shape=21, # tipo do marcador 0-14 são ocas de 15-18 sólidas e de 21-24 possuem borda e preenchimento fill=&quot;green&quot;, # cor do preenchimento do marcador color=&quot;red&quot;, # cor da borda do marcador size=3) # tamanho do marcador OBS: A forma de um ponto (marcador) é controlada pelo argumento shape. Há algumas duplicadas aparentes nesse argumento, que recomendamos qua sejam testadas por exemplo, 0, 15 e 22 são quadrados. A diferença vem da interação das estéticas color e fill. As formas ocas são de 0 a 14 e a cor de sua borda é determinada por color, as formas sólidas são de 15 a 18 e a sua cor de preenchimento é definida por color, já as formas de 21 a 24 têm a cor de sua borda definida por color e a cor de seu preenchimento definida por fill. Veja os exemplos: geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_line(color=&quot;red&quot;, lwd= 0.7, linetype=1) + geom_point(shape=0, size=5, color = &quot;blue&quot;) geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_line(color=&quot;red&quot;, lwd= 0.7, linetype=1) + geom_point(shape=15, size=5, color = &quot;blue&quot;) geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_line(color=&quot;red&quot;, lwd= 0.7, linetype=1) + geom_point(shape=22, size=5, color = &quot;blue&quot;, fill=&quot;green&quot;) Nosso próximo passo será controlar os títulos dos eixos a partir da função labs(). geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_line(col=&quot;blue&quot;, lwd= 0.7, lty=1)+ geom_point(shape=21, fill=&quot;green&quot;, col=&quot;red&quot;, size=3) + labs(x=&quot;Eixo x (m)&quot;,y=&quot;Teor de argila do solo (%)&quot;, title = &quot;Gráfico de Linha + Pontos&quot;) 8.2 Definindo temas Podemos utilizar diferentes temas, para isso vamos guardar o gráfico anterior em um objeto denominado meu_plot. meu_plot &lt;- geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_line(col=&quot;blue&quot;, lwd= 0.7, lty=1)+ geom_point(shape=21, fill=&quot;green&quot;, col=&quot;red&quot;, size=3) + labs(x=&quot;Eixo x (m)&quot;,y=&quot;Teor de argila do solo (%)&quot;, title = &quot;Gráfico de Linha + Pontos&quot;) meu_plot Agora podemos aplicar um tema pré-definido ao gráfico a partir da família de função theme_. Compare os diferentes temas diponíveis. meu_plot + theme_minimal() meu_plot + theme_bw() meu_plot + theme_dark() # ... 8.3 Controlando elementos do tema Vários elementos do tema podem ser controlados a partir de funções específicas associadas à função theme(). A família de funções element_ especificam a exibição de como os componentes sem dados do gráfico são desenhados. *element_blank: não desenha nada e não atribui espaço; *element_rect: bordas e fundos; *element_line: linhas; *element_text: texto; *rel() é usado para especificar os tamanhos relativos ao objeto original; *margin() é usado para especificar as margens dos elementos. Por exemplo, vamos duplicar o tamanho de fonte (rel(2)) do título do gráfico já criado meu_plot. meu_plot + theme(plot.title = element_text(size=rel(2))) Vamos modificar o alinhamento do e a cor do título do gráfico já criado meu_plot. meu_plot + theme(plot.title = element_text(hjust = 0.5, color = &quot;red&quot;)) Alterando a área de plotagem. meu_plot + theme(panel.background = element_rect(fill=&quot;lightblue&quot;, color = &quot;red&quot;, linetype = &quot;dashed&quot;)) Agora vamos fazer várias modificações, adicionando linhas de grade na área de plotagem, mudar as cores do texto dos eixos para vermelho, as cores dos títulos dos eixos para verde escuro e alterar a cor do retângulo de plotagem exterior para cinza. meu_plot + theme(panel.grid.major = element_line(color=&quot;gray&quot;, linetype = &quot;dashed&quot;), panel.grid.minor = element_line(color=&quot;gray&quot;, linetype = &quot;dashed&quot;), axis.text = element_text(colour = &quot;red&quot;), axis.title = element_text(colour = &quot;darkgreen&quot;,size=rel(1.2)), plot.background = element_rect(fill=&quot;gray&quot;)) Maiores informações a respeito dos parâmetros de controle dos elementos dos gráficos podem ser encontrados em https://ggplot2.tidyverse.org/reference/theme.html. 8.4 Mapeamento estéticos Vamos voltar ao gráfico de dispersão do teor de argila do solo ao longo do transecto estudado. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_point() Podemos agora adicionar uma terceira variável a esse gráfico, como tipo de solo (Solo) ou a superfície geomórfica (SUP), e mapeá-la a partir de um estético (aesthetic). Esse estético é uma propriedade visual dos objetos no gráfico. Estéticos incluem coisas como tamanho, forma ou cor dos pontos. Em nosso exemplo inicial Vamos mapear as cores dos pontos para cada superfície geomórfica (SUP). Para mapear a estética cor à variável SUP, devemos associar o nome da estética ao nome da variável dentro de aes(). O ggplot2 atribuirá automaticamente uma cor singular para cada valor singular da variável, um processo conhecido como escalar (scaling). Automaticamente será adicionado uma legenda que explica quais níveis correspondem a quais valores. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA, col = SUP) ) + geom_point() No exemplo anterior mapeamos SUP à estética cor, mas poderíamos ter mapeado SUP à estética de tamanho (size) da mesma maneira. Neste caso, o tamanho exato de cada ponto revela a sua superfície. Entretanto, não recomendamos o mapeamento de uma variável por meio do tamanho. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA, size = SUP) ) + geom_point() ## Warning: Using size for a discrete variable is not advised. Poderíamos ter mapeado SUP à estética alpha, que controla a transparência e à forma dos pontos controlada pelas estética shape. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA, alpha = SUP, shape=SUP) ) + geom_point() 8.5 Facetas As facetas são consideradas uma maneira prática de adicionar mais variáveis categóricas às representações gráficas. Para criar facetas use a função facet_wrap(), cujo primeiro argumento deve ser uma fórmula (definida com ~ e o nome da variável categórica subsequente) e o segundo argumento é o número de linhas do painel gráfico. Compare os painéis abaixo: geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_point() + facet_wrap(~SUP, nrow=1) geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_point() + facet_wrap(~SUP, nrow=3) Outra importante e útil ferramenta é a função facet_grid que permite a incorporação de mais de uma variável na representação, no nosso exemplo, vamos adicionar o tipo de solo Solo. A fórmula conterá dois nomes de variáveis categóricas sepradas pelo ~ geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_point() + facet_grid(SUP ~ Solo) Agora podemos mesclar as visualisações, utilizando o mapeamento estético e as facetas. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA, color=Solo) ) + geom_point() + facet_wrap(~SUP, nrow=1) ou, geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA, color=SUP) ) + geom_point() + facet_wrap(~Solo, nrow=4) + labs(x=&quot;Eixo x (m)&quot;, y= &quot;Teor de argila (%)&quot;, color=&quot;Superfície&quot;) 8.6 Objetos geométricos 8.6.1 Gráfico de Colunas Um geom_ é o objeto geométrico que um gráfico usa para representar os dados. Gráficos de colunas, por exemplo, usam o geom_col(). O preenchimento e as cores das bordas das colunas são controlados pelos argumentos fill e color. Gráfico de coluas, linhas e pontos, plotam os valores brutos. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_col(color=&quot;black&quot;,fill=&quot;aquamarine4&quot;) O gráfico de colunas também pode ser apresentadorotacionado em 90º a partir do uso da funçãocoord_flip(). geomorfologia %&gt;% ggplot( aes(x=X, y=ARGILA) ) + geom_col(color=&quot;black&quot;,fill=&quot;aquamarine4&quot;) + coord_flip() 8.6.2 Gráfico de Barras Diferente do gráfico de colunas, o gráfico de barras possibilita a representação de dados categóricos, vamos supor que devemos contar o número de observações em cada superfície geomórfica. A função geom_bar() conta os pontos em cada categoria de SUP automaticamente. geomorfologia %&gt;% ggplot( aes(x=SUP) ) + geom_bar(color=&quot;black&quot;, fill=&quot;lightblue&quot;) Podemos mapear a superfície geomófica associando SUP à estética fill: geomorfologia %&gt;% ggplot( aes(x=SUP, fill=SUP)) + geom_bar(color=&quot;black&quot;) Se adicionarmos uma segunda variáveis categórica, como o tipo de solo Solo, as barras são automaticamente empilhadas, ondem cada retângulo colorido representa uma combinação de SUP e Solo. geomorfologia %&gt;% ggplot( aes(x=SUP, fill=Solo)) + geom_bar(color=&quot;black&quot;) O empilhamento é realizado automaticamente, caso não queiramos o gráfico empilhado, utilizamos um dos três valores identity, dodge ou fill associados ao argumento position. geomorfologia %&gt;% ggplot( aes(x=SUP, fill=Solo)) + geom_bar(position = &quot;fill&quot;, color=&quot;black&quot;) geomorfologia %&gt;% ggplot( aes(x=SUP, fill=Solo)) + geom_bar(position = &quot;dodge&quot;, color=&quot;black&quot;) 8.6.3 Smoothers Vamos ajustar uma linha suave aos dados de teor de argila ao longo do transecto X a partir da função geom_smooth(). geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_point() + geom_smooth() Dentro de geom_smooth() podemos controlar o tipo da linha e a cor da linha a partir de linetype e color, respectivamente. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA) ) + geom_point() + geom_smooth(linetype=2, color=&quot;blue&quot;) Agora vamos aplicar uma linha suave para cada uma das superfícies geomórficas. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA, color=SUP) ) + geom_point() + geom_smooth() Agora em diferentes facetas. geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA, color=SUP) ) + geom_point() + geom_smooth() + facet_wrap(~SUP, nrow=1) Para apresentarmos as facetas em diferentes escalas (diferentes valores de X e Y em cada faceta), utilize o argumento scales = \"free\" em facet_wrap(). geomorfologia %&gt;% ggplot( aes(x=X, y =ARGILA, color=SUP) ) + geom_point() + geom_smooth() + facet_wrap(~SUP, nrow=1, scales = &quot;free&quot;) 8.6.4 Boxplot Conhecido como gráfico dos 5 números representa um resumos dos valores mínimo, primeiro quartil, mediana, terceiro quartil e máximo. Podem ser construídos para uma variável contínua a partir da geometria geom_boxplot(). Observe que dentro da função ggplot() não é necessário especificar o y, somente é atribuído a x a variável contínua ARGILA. geomorfologia %&gt;% ggplot( aes(x=ARGILA) ) + geom_boxplot() O boxplot é uma caixa que vai do 25º percentil ao 75º percentil da distribuição, uma distância conhecida como a amplitude interquartil (IIQ). No meio da caixa há uma linha que exibe a mediana, isto é, 50º percentil, da distribuição. Essas três linhas lhe dão um sentido da dispersão da distribuição e se ela é ou não simétrica sobre a mediana ou enviesada para um lado. Pontos visuais que exibem observações que caem mais do que 1,5 vez o IIQ de cada limite da caixa. Esses pontos foram da curva são incomuns, entãosão plotados individuaalmente. Uma linha (ou bigode de gato, dai o nome Box and Whiskers) que se estende de cada lado da caixa e vai até o ponto mais distante da distribuição que não seja um outlier. Podemos modificar a orientação do boxplot por meio de aes(y=ARGILA). geomorfologia %&gt;% ggplot( aes(y=ARGILA) ) + geom_boxplot() Outra possibilida é diminuir o tamanho da caixa com coord_cartesian(). geomorfologia %&gt;% ggplot( aes(y=ARGILA) ) + geom_boxplot() + coord_cartesian(xlim=c(-1,1)) Outra alternativa para exibir a distribuição de uma variável contínua é desmembrá-la por uma variável categórica no boxplot. geomorfologia %&gt;% ggplot( aes(y=ARGILA, fill=Solo) ) + geom_boxplot() 8.6.5 Histograma O gráfico histograma é facilmente construído a partir da função geom_histogram(). Semelhante ao geom_boxplot() dentro da função ggplot() é atribuído a x a variável contínua ARGILA. geomorfologia %&gt;% ggplot(aes(x=ARGILA)) + geom_histogram() Observe que o histograma foi construído com a frequencia absoluta de cada classe, ou seja, o número de observações (contagem) dentro de cada classe de teor de argila construída automaticamentem pela função. Se ao invés da contagem, quiséssemos a densidade de frenquência, deveremos utilizar y=..density.. dentro de aes(). geomorfologia %&gt;% ggplot(aes(x=ARGILA, y=..density..)) + geom_histogram() Podemos estabelecer a amplitude dos intervalos em um histograma com o argumento bidwidth, que é medido nas unidades da variável x. geomorfologia %&gt;% ggplot(aes(x=ARGILA, y=..density..)) + geom_histogram(binwidth = 1) Ou você pode especificar o número de classes a partir do argumento bins, que por default é igual a 30. geomorfologia %&gt;% ggplot(aes(x=ARGILA, y=..density..)) + geom_histogram(bins = 15) Vamos alterar as cores das bordas e do preeenchimento das colunas do histograma. geomorfologia %&gt;% ggplot(aes(x=ARGILA, y=..density..)) + geom_histogram(bins = 15, color=&quot;black&quot;,fill=&quot;white&quot;) Podemos associar a curva suave de densidade no hitograma, a partir da função geom_density() e controlar a transparência e seu preenchimento a partir dos argumentos alpha e fill, respectivamente. geomorfologia %&gt;% ggplot(aes(x=ARGILA, y=..density..)) + geom_histogram(bins = 15,color=&quot;black&quot;,fill=&quot;white&quot;)+ geom_density(alpha=.10, fill=&quot;red&quot;) Finalmente, podemos construir um histograma da variável ARGILA para cada superfície geomórfica. geomorfologia %&gt;% ggplot(aes(x=ARGILA, y=..density.., color=SUP, fill=SUP)) + geom_histogram(bins = 15, color=&quot;black&quot;,fill=&quot;white&quot;)+ geom_density(alpha=.2)+ facet_wrap(~SUP, scales=&quot;free&quot;)+ labs(x=&quot;Teor de argila do solo (%)&quot;, y = &quot;Densidade&quot;, fill=&quot;Superfície&quot;, color=&quot;Superfície&quot;) + theme_minimal() "],["manipulação-de-dados-com-o-dplyr.html", "9 Manipulação de dados com o dplyr 9.1 Principais funções (verbos) do dplyr. 9.2 Regras para manipulação: 9.3 filter() 9.4 arrange() 9.5 select() 9.6 mutate() 9.7 Resumos Agrupados com summarize() 9.8 Exercício", " 9 Manipulação de dados com o dplyr A visualização de dados é uma ferramenta importante para a geração de insights, mas é raro que tenhamos os dados exatamente na forma necessário para essa tarefa. Muitas vezes precisamos realizar recortes no banco de dados, selecionar certas observações, criar novas variáveis, transformá-la, renomeá-las, reordenar observações entre outras manipulações. Para a manipulação dos nossos dados, vamos utilizar o pacote dplyr que, unido ao pacote ggplot2, é um poderoso recurso para exploração, transformação e visualização de dados. Ambos os pacotes fazem parte do tidyverse, que, como já dito, é um pacote de pacotes. 9.1 Principais funções (verbos) do dplyr. filter(): seleciona/filtra por linhas (observações) a base de dados. arrange(): ordena a base de dados de acordo com alguma coluna (variável). select(): seleciona colunas (variáveis). mutate(): modifica/transforma/cria variáveis (colunas). summarise(): resume/agrega, variáveis (colunas) de uma base de dados. 9.2 Regras para manipulação: 1- O primeiro argumento sempre será um data frame. 2- Demais argumentos, descrevem como deve ser a manipulação. 3- A resposta (retorno) sempre será um data frame. 4- Manipulação é realizada com o operador PIPE (%&gt;%) - CTRL + SHIFT + M. Importação dos dados Vamos realizar a importação, via web, do banco de dados geomorfologia.txt. library(tidyverse) URL &lt;- &quot;https://raw.githubusercontent.com/arpanosso/r_data_science_fcav/master/dados/geomorfologia.txt&quot; geomorfologia&lt;-read.table(URL,header = TRUE) glimpse(geomorfologia) ## Rows: 106 ## Columns: 22 ## $ SUP &lt;chr&gt; &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I~ ## $ Solo &lt;chr&gt; &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;LV&quot;, &quot;L~ ## $ Amostra &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,~ ## $ X &lt;int&gt; 0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 32~ ## $ AMG &lt;dbl&gt; 0.2, 0.1, 0.7, 0.4, 0.4, 0.4, 1.2, 0.8, 1.1, 1.2, 0.1, 0.2, 0.~ ## $ AG &lt;dbl&gt; 3.72, 4.27, 5.00, 3.80, 3.10, 3.80, 3.60, 4.70, 4.50, 5.90, 5.~ ## $ AM &lt;dbl&gt; 20.4, 22.6, 22.7, 23.7, 22.3, 23.8, 23.1, 25.8, 25.5, 32.8, 33~ ## $ AF &lt;dbl&gt; 22.9, 23.6, 22.2, 24.4, 24.6, 19.1, 21.7, 21.1, 18.9, 19.8, 19~ ## $ AMF &lt;dbl&gt; 30.0, 28.4, 26.9, 26.7, 26.9, 27.1, 26.5, 24.7, 25.4, 21.7, 19~ ## $ SILTE &lt;dbl&gt; 1.2, 1.2, 1.2, 0.6, 2.1, 2.2, 0.7, 0.2, 2.5, 0.2, 2.5, 2.6, 0.~ ## $ ARGILA &lt;dbl&gt; 21.5, 20.4, 21.4, 20.5, 20.7, 23.5, 23.1, 22.7, 22.0, 18.5, 20~ ## $ S_A &lt;dbl&gt; 0.05, 0.05, 0.05, 0.02, 0.10, 0.09, 0.03, 0.01, 0.11, 0.01, 0.~ ## $ AF_AG &lt;dbl&gt; 6.16, 5.53, 4.44, 6.42, 7.94, 5.03, 6.03, 4.49, 4.20, 3.36, 3.~ ## $ P &lt;dbl&gt; 42, 22, 41, 27, 11, 12, 11, 16, 38, 25, 25, 6, 6, 7, 5, 4, 4, ~ ## $ pH &lt;dbl&gt; 4.2, 3.8, 4.8, 4.0, 4.4, 4.0, 4.8, 5.4, 4.4, 5.2, 4.5, 5.1, 4.~ ## $ K &lt;dbl&gt; 0.27, 0.11, 0.34, 0.13, 0.11, 0.14, 0.23, 0.28, 0.19, 0.14, 0.~ ## $ Ca &lt;dbl&gt; 1.4, 0.4, 2.4, 0.7, 1.4, 0.6, 1.6, 3.3, 1.6, 2.9, 1.3, 1.6, 0.~ ## $ Mg &lt;dbl&gt; 0.3, 0.1, 0.4, 0.1, 0.3, 0.1, 0.7, 1.3, 0.5, 1.7, 0.6, 0.8, 0.~ ## $ H_Al &lt;dbl&gt; 5.2, 5.8, 4.2, 5.2, 4.2, 5.2, 3.4, 2.5, 5.2, 3.1, 4.2, 2.5, 4.~ ## $ SB &lt;dbl&gt; 1.97, 0.61, 3.14, 0.93, 1.81, 0.84, 2.53, 4.88, 2.29, 4.74, 1.~ ## $ T &lt;dbl&gt; 7.17, 6.41, 7.34, 6.13, 6.01, 6.04, 5.93, 7.38, 7.49, 7.84, 6.~ ## $ V &lt;dbl&gt; 27, 10, 43, 15, 30, 14, 43, 66, 31, 60, 32, 50, 22, 35, 36, 34~ As práticas abaixo serão realizadas sem o armazenamento dos resultados em novos objetos, assim ao final de cada operação será utilizado a função View() para gerar a visualização do data frame resultante (.) em uma nova aba do RStudio. 9.3 filter() O filter() permite que você crie um subconjuto de observações com base em seus valores. Por exemplo, vamos selecionar somente as observações linhas para superfície geomórfica II. Para isso, dentro da função filter() faremos uma operação relacional, perguntando quais valores de SUP são iguais a II. geomorfologia %&gt;% filter(SUP == &quot;II&quot;) %&gt;% View() Quando executamos essa linha de código o dplyr realiza a operação de filtragem e retorna um novo data frame. As entradas não são modificadas, então, para salvar o resultado em um novo objeto, será necessário a operação de atribuição (&lt;-) a um novo objeto, por exemplo dado_auxiliar: dado_auxiliar &lt;- geomorfologia %&gt;% filter(SUP == &quot;II&quot;) glimpse(dado_auxiliar) ## Rows: 62 ## Columns: 22 ## $ SUP &lt;chr&gt; &quot;II&quot;, &quot;II&quot;, &quot;II&quot;, &quot;II&quot;, &quot;II&quot;, &quot;II&quot;, &quot;II&quot;, &quot;II&quot;, &quot;II&quot;, &quot;II&quot;, &quot;I~ ## $ Solo &lt;chr&gt; &quot;LVp&quot;, &quot;LVp&quot;, &quot;LVp&quot;, &quot;LVp&quot;, &quot;LVp&quot;, &quot;LVp&quot;, &quot;LVp&quot;, &quot;LVp&quot;, &quot;LVp&quot;,~ ## $ Amostra &lt;int&gt; 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33~ ## $ X &lt;int&gt; 425, 450, 475, 500, 525, 550, 575, 600, 625, 650, 675, 700, 72~ ## $ AMG &lt;dbl&gt; 0.2, 0.3, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.5, 0.4, 0.2, 0.~ ## $ AG &lt;dbl&gt; 3.64, 3.43, 3.23, 2.83, 3.85, 3.64, 3.64, 4.45, 4.24, 6.14, 5.~ ## $ AM &lt;dbl&gt; 27.5, 24.6, 23.7, 19.2, 16.6, 19.0, 19.0, 23.3, 20.6, 30.3, 28~ ## $ AF &lt;dbl&gt; 24.5, 34.2, 25.7, 27.0, 15.9, 18.4, 18.4, 23.3, 21.9, 20.5, 20~ ## $ AMF &lt;dbl&gt; 20.4, 12.6, 24.7, 28.0, 35.3, 32.1, 32.1, 26.4, 27.3, 20.2, 21~ ## $ SILTE &lt;dbl&gt; 1.8, 4.0, 2.1, 4.2, 2.7, 1.0, 1.0, 1.0, 1.2, 1.6, 1.8, 0.6, 1.~ ## $ ARGILA &lt;dbl&gt; 22.0, 21.0, 20.5, 18.5, 25.5, 26.6, 26.6, 21.4, 24.4, 20.7, 21~ ## $ S_A &lt;dbl&gt; 0.08, 0.19, 0.10, 0.23, 0.11, 0.04, 0.04, 0.05, 0.05, 0.08, 0.~ ## $ AF_AG &lt;dbl&gt; 6.73, 9.97, 7.96, 9.54, 4.13, 5.05, 5.05, 5.24, 5.17, 3.34, 3.~ ## $ P &lt;dbl&gt; 3, 2, 3, 3, 2, 2, 2, 3, 4, 4, 6, 2, 3, 2, 14, 106, 107, 83, 12~ ## $ pH &lt;dbl&gt; 4.0, 3.9, 3.9, 3.8, 3.8, 3.8, 3.8, 4.0, 3.8, 3.9, 4.0, 3.8, 3.~ ## $ K &lt;dbl&gt; 0.05, 0.01, 0.04, 0.01, 0.02, 0.01, 0.01, 0.16, 0.04, 0.08, 0.~ ## $ Ca &lt;dbl&gt; 0.6, 0.3, 0.4, 0.2, 0.2, 0.2, 0.2, 0.5, 0.2, 0.4, 0.6, 0.4, 0.~ ## $ Mg &lt;dbl&gt; 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.~ ## $ H_Al &lt;dbl&gt; 5.2, 5.2, 5.8, 6.4, 6.4, 6.4, 5.8, 4.2, 5.2, 5.2, 5.8, 5.8, 5.~ ## $ SB &lt;dbl&gt; 0.85, 0.41, 0.54, 0.31, 0.32, 0.31, 0.31, 0.76, 0.34, 0.58, 0.~ ## $ T &lt;dbl&gt; 6.05, 5.61, 6.34, 6.71, 6.72, 6.71, 6.11, 4.96, 5.54, 5.78, 6.~ ## $ V &lt;dbl&gt; 14, 7, 9, 5, 5, 5, 5, 15, 6, 10, 12, 8, 12, 22, 54, 38, 46, 44~ Agora vamos selecionar as observações com teor de argila maior do que \\(10\\%\\), provenientes da superfície III. Para realizarmos essa operação, serão necessárias duas operações relacionais, uma com a coluna ARGILA e outra com a coluna SUP, tais operações serão realizadas dentro de filter() separadas por uma vírgula. geomorfologia %&gt;% filter(ARGILA &gt; 10, SUP == &quot;III&quot;) %&gt;% View() Selecione todas as observações que não sejam Regossolo, ou seja, todas os registros cujo tipo de solo seja diferente de R. geomorfologia %&gt;% filter(Solo != &quot;R&quot;) %&gt;% View() ou geomorfologia %&gt;% filter(!(Solo == &quot;R&quot;)) %&gt;% View() Anteriormente utilizamos a geometria geom_bar() para contar o número de pontos amostrais em uma categoria. Para observarmos a tabela com a contagem poderíamos utilizar as função group_by() para agruparmos as categorias da variável SUP e, posteriormente, a função count() para realizar o processo de contagem das observações em cada grupo, gerando uma tabela com as colunas SUP para as categorias de superfície e n para a contagem. geomorfologia %&gt;% group_by(SUP) %&gt;% count() %&gt;% View() Pronto, agora podemos associar o ggplot() a essa filtragem: geomorfologia %&gt;% group_by(SUP) %&gt;% count() %&gt;% ggplot(aes(x=SUP, y=n)) + geom_col(color=&quot;black&quot;,fill=&quot;lightblue&quot;) Agora vamos selecionar somente os registros referentes aos Latossolos, ou seja, todos aqueles que tenham nomes iguais a LV ou LVp. Para essa manipulação, podemos utilizar o operador lógico de disjunção OU |. geomorfologia %&gt;% filter(Solo == &quot;LV&quot; | Solo == &quot;LVp&quot;) %&gt;% View() Um atalho útil para a solução desse problema é o operador x %in% y. Isso selecionará toda linha em que x seja um dos valores em y, então, vamos reescrever o código acima de uma manira mais elegante. geomorfologia %&gt;% filter(Solo %in% c(&quot;LV&quot;,&quot;LVp&quot;)) %&gt;% View() 9.3.1 Valores Faltantes Um recurso importante do R que pode complicar as comparações e, consequentemente as operações de filtragem, são os valores faltantes ou NAs (not availables). NA representa um valor desconhecido, então valores faltantes se propagam nas operações, são contagiosos, ou seja, quase toda operação envolvendo um valor desconhecido terá como resultado um valor desconhecido. NA &gt; 5 ## [1] NA 10 == NA ## [1] NA NA + 10 ## [1] NA NA / 2 ## [1] NA NA == NA ## [1] NA Seja x a idade de Maria. Não sabemos a idade de Maria: x &lt;- NA Seja y a idade de João. Não sabemos a idade de João: y &lt;- NA Maria e João têm a mesma idade? x == y ## [1] NA # Não sabemos. Se quisermos determinar se há um valor faltante, usamos a função is.na(). is.na(x) ## [1] TRUE is.na(y) ## [1] TRUE A função filter() somente inclui linhas onde a condição é TRUE, ou seja, ela exclui valores FALSE e NA automaticamente. Vamos criar um exemplo rápido referente a nota de 7 alunos na primeira prova da disciplina de estatística da FCAV do curso de Agronomia. df &lt;- tibble(nome = c(&quot;Ana&quot;,&quot;Beatriz&quot;,&quot;Douglas&quot;,&quot;Luis&quot;,&quot;Marcela&quot;,&quot;Paulo&quot;,&quot;Renata&quot;), P1 = c(8,NA,6,3,NA,6,3), P2=c(9,7,4,2,NA,NA,4), Psub=c(NA,8,7,9,NA,5,6)) df ## # A tibble: 7 x 4 ## nome P1 P2 Psub ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ana 8 9 NA ## 2 Beatriz NA 7 8 ## 3 Douglas 6 4 7 ## 4 Luis 3 2 9 ## 5 Marcela NA NA NA ## 6 Paulo 6 NA 5 ## 7 Renata 3 4 6 Se quisermos realizar a filtragem para todos os valores de P1 maiores que 4: df %&gt;% filter(P1 &gt; 4) ## # A tibble: 3 x 4 ## nome P1 P2 Psub ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ana 8 9 NA ## 2 Douglas 6 4 7 ## 3 Paulo 6 NA 5 Se quisermos preservar o NA, devemos pedir explicitamente, caso contrário eles serão excluídos da filtragem. df %&gt;% filter(P1 &gt; 4 | is.na(P1)) ## # A tibble: 5 x 4 ## nome P1 P2 Psub ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ana 8 9 NA ## 2 Beatriz NA 7 8 ## 3 Douglas 6 4 7 ## 4 Marcela NA NA NA ## 5 Paulo 6 NA 5 Vamos selecionar os alunos que fizeram, pelo menos, uma prova. df %&gt;% filter(!is.na(P1) | !is.na(P2) | !is.na(Psub)) ## # A tibble: 6 x 4 ## nome P1 P2 Psub ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ana 8 9 NA ## 2 Beatriz NA 7 8 ## 3 Douglas 6 4 7 ## 4 Luis 3 2 9 ## 5 Paulo 6 NA 5 ## 6 Renata 3 4 6 Agora vamos selecionar os alunos que fizeram as 3 provas. df %&gt;% filter(!is.na(P1), !is.na(P2), !is.na(Psub)) ## # A tibble: 3 x 4 ## nome P1 P2 Psub ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Douglas 6 4 7 ## 2 Luis 3 2 9 ## 3 Renata 3 4 6 9.3.2 Strings e Regex A função str_detect() pertence ao pacote stringr e tem a finalidade de detectar a presença ou a ausência de um padrão de caracteres dentro de uma string (cadeia de caracteres). O pacote stringr faz parte do pacote tidyverse, cuja sintaxe é o Regex (do inglês Regular Expressions), ou seja, são expressões regulares para descrever os padrões dentro da string. Portanto, para uma boa utilização das facilidades do Regex, uma breve explicação se faz necessária. Vamos criar duas strings: string_1 &lt;- &quot;Isso é uma string&quot; string_2 &lt;- &#39;Se você quiser adicionar &quot;aspas duplas&quot; dentro da string, utilize aspas simples&#39; string_1 ## [1] &quot;Isso é uma string&quot; string_2 ## [1] &quot;Se você quiser adicionar \\&quot;aspas duplas\\&quot; dentro da string, utilize aspas simples&quot; A representação impressa de uma string mostra as barras transversas \\, já o conteúdo bruto da string pode ser apresentado por writeLines(). writeLines(string_2) ## Se você quiser adicionar &quot;aspas duplas&quot; dentro da string, utilize aspas simples Observe mais esse exemplo. * \\\" aspas duplas * \\\\ caractere de barra transversa * \\n nova linha * \\t tabulação * \\u00b5 caracteres que não pertencem ao inglês. string_3 &lt;- c(&quot;\\&quot;&quot;, &quot;\\n&quot;, &quot;\\\\&quot;, &quot;A\\tB&quot;, &quot;\\n&quot;, &quot;\\u00b5&quot;) string_3 ## [1] &quot;\\&quot;&quot; &quot;\\n&quot; &quot;\\\\&quot; &quot;A\\tB&quot; &quot;\\n&quot; &quot;µ&quot; writeLines(string_3) ## &quot; ## ## ## \\ ## A B ## ## ## µ O pacote básico do R contém muitas funções para trabalhar com strings, mas vamos evitá-las porque podem ser inconsistentes, o que as torna difíceis de lembrar. Já o pacote stringr é de mais simples utilização, pois têm nomes intuitivos, pois todos começam com str_. Por exemplo, str_length() informa o número de caracteres em uma string: meu_texto &lt;- c(&#39;a&#39;,&#39;R para data science&#39;, NA, &#39;UNESP-FCAV&#39;) str_length(meu_texto) ## [1] 1 19 NA 10 A combinação de strings pode ser feita com o str_c(), utilizando o argumento sep para controlar o separador entre as strings. str_c(&quot;objeto&quot;, 1:5, sep=&quot;_&quot;) ## [1] &quot;objeto_1&quot; &quot;objeto_2&quot; &quot;objeto_3&quot; &quot;objeto_4&quot; &quot;objeto_5&quot; OBS Poderíamos ter o mesmo efeito com a função paste() ou paste0() do pacote base. paste(&quot;objeto&quot;, 1:5) # sep = &quot; &quot; ## [1] &quot;objeto 1&quot; &quot;objeto 2&quot; &quot;objeto 3&quot; &quot;objeto 4&quot; &quot;objeto 5&quot; paste0(&quot;objeto&quot;,1:5) # sep = &quot;&quot; ## [1] &quot;objeto1&quot; &quot;objeto2&quot; &quot;objeto3&quot; &quot;objeto4&quot; &quot;objeto5&quot; Contudo, observe a forma com a qual as funções operam valores faltantes NA. paste(&quot;objeto&quot;, c(1,NA,3), sep=&quot;_&quot;) ## [1] &quot;objeto_1&quot; &quot;objeto_NA&quot; &quot;objeto_3&quot; str_c(&quot;objeto&quot;, c(1,NA,3), sep=&quot;_&quot;) ## [1] &quot;objeto_1&quot; NA &quot;objeto_3&quot; Como a maioria das outras funções em R, os valores ausentes se propagam nas operações. Se você quiser que eles sejam impressos como NA, use str_replace_na(): str_c(&quot;objeto&quot;, str_replace_na(c(1,NA,3)), sep=&quot;_&quot;) ## [1] &quot;objeto_1&quot; &quot;objeto_NA&quot; &quot;objeto_3&quot; str_c(&quot;---&quot;, meu_texto, &quot;---&quot;) ## [1] &quot;---a---&quot; &quot;---R para data science---&quot; ## [3] NA &quot;---UNESP-FCAV---&quot; str_c(&quot;---&quot;, str_replace_na(meu_texto), &quot;---&quot;) ## [1] &quot;---a---&quot; &quot;---R para data science---&quot; ## [3] &quot;---NA---&quot; &quot;---UNESP-FCAV---&quot; Para colapsar um vetor de strings em uma única string, use o argumento colappse na função str_c(): y &lt;- c(&quot;A&quot;,&quot;mais&quot;,&quot;bonita&quot;) y ## [1] &quot;A&quot; &quot;mais&quot; &quot;bonita&quot; str_c(y, collapse = &quot; &quot; ) ## [1] &quot;A mais bonita&quot; str_c(meu_texto, collapse = &quot;, &quot;) # propagação do NA ## [1] NA str_c(str_replace_na(meu_texto), collapse = &quot;, &quot;) ## [1] &quot;a, R para data science, NA, UNESP-FCAV&quot; Podemos extrair partes de uma string usando str_sub(), que leva os argumentos inicial e final que fornecem a posição (inclusiva) da substring. No exemplo abaixo, vamos extrair do primeiro ao quarto caracter de cada string do objeto x. x &lt;- c (&quot;testemunha&quot;,&quot;escarificador&quot;,&quot;arado de disco&quot;,&quot;grade aradora&quot;,&quot;subsolador&quot;) str_sub(x,1,4) ## [1] &quot;test&quot; &quot;esca&quot; &quot;arad&quot; &quot;grad&quot; &quot;subs&quot; Números negativos contam de trás para frente para a realizar a extração. str_sub(x,-4,-1) ## [1] &quot;unha&quot; &quot;ador&quot; &quot;isco&quot; &quot;dora&quot; &quot;ador&quot; Podemos utilizar essa função para alterar uma string, por exemplo, passar as primeira letras para maiúsculas str_sub(x,1,1) &lt;- str_to_upper(str_sub(x,1,1)) x ## [1] &quot;Testemunha&quot; &quot;Escarificador&quot; &quot;Arado de disco&quot; &quot;Grade aradora&quot; ## [5] &quot;Subsolador&quot; Ou podemos passar todas para maiúsculas. str_sub(x,1,str_length(x)) &lt;- str_to_upper(str_sub(x,1,str_length(x))) x ## [1] &quot;TESTEMUNHA&quot; &quot;ESCARIFICADOR&quot; &quot;ARADO DE DISCO&quot; &quot;GRADE ARADORA&quot; ## [5] &quot;SUBSOLADOR&quot; Ou podemos passar todas para minúsculas, novamente str_sub(x,1,str_length(x)) &lt;- str_to_lower(str_sub(x,1,str_length(x))) x ## [1] &quot;testemunha&quot; &quot;escarificador&quot; &quot;arado de disco&quot; &quot;grade aradora&quot; ## [5] &quot;subsolador&quot; Podemos buscar combinações simples, para facilitar a verificação, vamos utilizar str_view(). str_view(x, &quot;ad&quot;) Podemos utilizar o ponto . para encontrar a combinação com qualquer caractere (exceto no início ou no final da string - nova linha). str_view(x, &quot;.a.&quot;) Se estivermos procurando o caractere ponto ., vamos utilizar \\\\.. meu_texto_2 &lt;- c(&quot;abc&quot;, &quot;a*b&quot;, &quot;a b&quot; ,&quot;a.c&quot;) str_view(meu_texto_2, &quot;\\\\.&quot;) Por default, as expressões regulares corresponderão a qualquer parte de uma string. Muitas vezes, é útil ancorar a expressão regular de forma que corresponda desde o início ou ao final da string. Assim, podemos utilizar as âncoras: *^ para combinar com o início da string. *$ para combinar com o final da string. str_view(x, &quot;^a&quot;) str_view(x, &quot;r$&quot;) Para forçar uma expressão regular a corresponder apenas a uma string completa, ancore-a com ^ e $: z&lt;-c(&quot;Arado&quot;,&quot;Arado de disco&quot;,&quot;Arado de aiveca&quot;, &quot;Grade aradora&quot;) str_view(z,&quot;Arado&quot;) str_view(z,&quot;^Arado$&quot;) Se acaso quiséssemos ignorar se a letra é maiúscula ou minúscula, utilizamos o argumento ignore_case = TRUE. Observe que antes de utilizarmos o referido argumento, envolvemos a string Arado com a função regex() que é o mecanismos padrão no pacote stringr. Assim, utilizamos a função para substituir as opções padrões do mecanismo. str_view(z,regex(&quot;Arado&quot;,ignore_case=TRUE)) Voltando à manipulação dos dados de geomorfologia, agora podemos filtrar todos os Argissolos utilizando o regex, ou seja, podemos selecionar todos aqueles solos que começam com a letra P. geomorfologia %&gt;% filter(str_detect(Solo,&quot;^P&quot;)) %&gt;% View() Selecione os registros cujo solo é PV4. geomorfologia %&gt;% filter(str_detect(Solo,&quot;PV4&quot;)) %&gt;% View() geomorfologia %&gt;% filter(str_detect(Solo,&quot;4$&quot;)) %&gt;% View() Podemos criar nossas próprias classes de personagens de busca usando [ ]: [abc]: corresponde a, b ou c. [a-z]: corresponde a todos os caracteres entre a e z. [^abc]: corresponde a qualquer coisa, exceto a, b ou c. [\\^\\-]: corresponde a ^ ou -. Por exemplo, retirando, novamente, o Regossolo. geomorfologia %&gt;% filter(str_detect(Solo,&quot;[^R]&quot;)) %&gt;% View() [:punct:]: pontuação. [:alpha:]: letras. [:lower:]: letras minúsculas. [:upper:]: letras maiúsculas. [:digit:]: dígitos. [:xdigit:]: dígitos hexadecimais. [:alnum:]: letras e números. [:cntrl:]: caracteres de controle. [:graph:]: letras, números e pontuação. [:print:]: letras, números, pontuação e espaços em branco. [:space:]: caracteres de espaço (basicamente equivalente a \\s). [:blank:]: espaço e tabulação. Selecione os registros que terminam com um número. geomorfologia %&gt;% filter(str_detect(Solo,&quot;[:digit:]$&quot;)) %&gt;% View() Selecione todos aqueles que não tenham dígitos nos nomes. geomorfologia %&gt;% filter(!str_detect(Solo,&quot;[:digit:]&quot;)) %&gt;% View() OBS: Todos eles vão dentro de [ ] para classes de caracteres, ou seja, [[:dígito:]AX] corresponde a todos os dígitos, a letra A e a letra X. Selecione todos Regossolos ou Latossolos. geomorfologia %&gt;% filter(str_detect(Solo,&quot;R|LV&quot;)) %&gt;% View() Selecione todos Regossolos ou Latossolos LV, excluindo o LVp. geomorfologia %&gt;% filter(str_detect(Solo,&quot;R|^LV$&quot;)) %&gt;% View() 9.4 arrange() A funçao arrange() funciona de maneira similar a filter(), mas, ao invés de selecionar linhas, ela muda a ordem delas. Ela recebe um data frame e um conjunto de nomes de colunas pelos quais ordenar. Se fornecermos mais de um nome de coluna, cada coluna adicional será usada para desempate nos valores das colunas anteriores. Vamos classificar os registros por ordem crescente de teor de argila. geomorfologia %&gt;% arrange(ARGILA) %&gt;% View() Classifique os registros por ordem decrescente de teor de acidez \\(H+Al\\). Para isso vamos utilizar a função desc() que reordena a coluna em ordem descendente. geomorfologia %&gt;% arrange(desc(H_Al)) %&gt;% View() Classifique os registros por ordem alfabéticas de solos e utilize como crotério de desempate o teor de fósforo disponível no solo em ordem decrescente. geomorfologia %&gt;% arrange(Solo, desc(P)) %&gt;% View() Valores faltantes são sempre colocados no final, veja o exemplo abaixo utilizando as notas de alunos anterioremente criadas. df %&gt;% arrange(desc(Psub)) ## # A tibble: 7 x 4 ## nome P1 P2 Psub ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luis 3 2 9 ## 2 Beatriz NA 7 8 ## 3 Douglas 6 4 7 ## 4 Renata 3 4 6 ## 5 Paulo 6 NA 5 ## 6 Ana 8 9 NA ## 7 Marcela NA NA NA 9.5 select() Não é incomum obter conjuntos de dados com centenas ou até milhares de variáveis. Neste caso, o primeiro desafio frequentemente é limitar-se às variávies em que você realmente está interessado. A função select() permite que você foque em um subconjunto útil usando operações baseadas em nomes de variáveis. Seleciona colunas de uma tabela e pode ser utilizado com as funções: * starts_with(&quot;abc&quot;): seleciona nomes que começam com &quot;abc&quot; * ends_with(&quot;xyz&quot;): seleciona nomes que terminam com &quot;xyz&quot; * contains(&quot;ijk&quot;): seleciona nomes que contêm &quot;ijk&quot; * matches(&quot;(.)\\\\1&quot;): seleciona nomes usando Expressões Regulares * num_range(&quot;x&quot;, 1:3): seleciona x1, x2 e x3 Selecione as colunas SUP, Solo e ARGILA. geomorfologia %&gt;% select(SUP, Solo, ARGILA) %&gt;% View() Selecione as colunas que comecem com a letra A. geomorfologia %&gt;% select(starts_with(&quot;A&quot;)) %&gt;% View() Selecione as colunas que NÃO comecem com a letra A. geomorfologia %&gt;% select(-starts_with(&quot;A&quot;)) %&gt;% View() 9.6 mutate() Vamos voltar ao exemplo das notas dos alunos, que estão armazenadas no data frame df. df ## # A tibble: 7 x 4 ## nome P1 P2 Psub ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ana 8 9 NA ## 2 Beatriz NA 7 8 ## 3 Douglas 6 4 7 ## 4 Luis 3 2 9 ## 5 Marcela NA NA NA ## 6 Paulo 6 NA 5 ## 7 Renata 3 4 6 Devemos agora calcular a média final do aluno, seguindo o critério: \\[ MF = \\frac{P1+P2}{2} \\] Se o aluno fez a prova substitutiva (Psub) ela deve substituir a prova faltante, ou a menor das notas das provas. df %&gt;% mutate( SOMA = if_else(is.na(Psub),P1+P2, if_else(is.na(P1),P2+Psub, if_else(is.na(P2),P1+Psub, if_else(P1&lt;P2,P2+Psub,P1+Psub)))), MF=SOMA/2 ) ## # A tibble: 7 x 6 ## nome P1 P2 Psub SOMA MF ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ana 8 9 NA 17 8.5 ## 2 Beatriz NA 7 8 15 7.5 ## 3 Douglas 6 4 7 13 6.5 ## 4 Luis 3 2 9 12 6 ## 5 Marcela NA NA NA NA NA ## 6 Paulo 6 NA 5 11 5.5 ## 7 Renata 3 4 6 10 5 Vamos criar duas novas variáveis, primeiro vamos calcular a soma dos teores de ARGILA + SILTE e, em seguida, passar o teor de fósforo para a escala logarítmica. geomorfologia %&gt;% mutate( ARG_SILT = ARGILA + SILTE, log_P = log10(P)) %&gt;% View() Classifique a Textura do solo. Figure 9.1: A figura 1 apresenta os intervalos de argila e a respectiva classificação do solo. . FONTE (https://www.pedologiafacil.com.br/textura.php) geomorfologia %&gt;% mutate( TEXTURA = case_when( ARGILA &lt; 15 ~ &quot;Arenosa&quot;, ARGILA &lt;= 35 ~ &quot;Media&quot;, ARGILA &lt;= 60 ~ &quot;Argilosa&quot;, ARGILA &gt; 60 ~ &quot;Muito argilosa&quot;, TRUE ~ &quot;Sem classificação&quot;) ) 9.7 Resumos Agrupados com summarize() A função summarize() reduz um data frame a uma única linha: geomorfologia %&gt;% summarize(media_ARGILA = mean(ARGILA, na.rm = TRUE)) ## media_ARGILA ## 1 14.60189 Essa função é extremamente útil quando combinada à função group_by(). Isso muda a unidade da análise de todo o conjunto de dados para os grupos de individuais. Então, ao usarmos os verbos do dplyr em um data frame agrupado, eles são automaticamente aplicados por grupo. Por exemplo, vamos aplicar o mesmo código anterior ao data frame agrupado por superfície geomórfica, obteremos: geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(media_ARGILA = mean(ARGILA, na.rm = TRUE)) ## # A tibble: 3 x 2 ## SUP media_ARGILA ## &lt;chr&gt; &lt;dbl&gt; ## 1 I 21.1 ## 2 II 14.8 ## 3 III 10.1 9.7.1 Valores faltantes O argumento na.rm = TRUE é utilizado para remoção dos valores faltantes da variáviel. Isso porque as funções de agregação obedecem à regra usual de NAs, ou seja, qualquer valor faltando na entrada, a saída será um valor faltante. Assim o uso desse argumento é essencial. 9.7.2 Counts Sempre que fizer agregação, sugerimos que inclua uma contagem n() ou uma contagem de valores não faltantes (sum(!is.na(x))). Deste modo podemos verificar que não estamos tirando conclusões com base em quantidades muito pequenas de dados. Por exemplo, vamos pedir a média da variável AMG (areia muito grossa) para cada tipo de solo identificado no levantamento em função do número de observações (n). Para isso vamos unir as funções group_by(), summarize() e ggplot(). geomorfologia %&gt;% group_by(Solo) %&gt;% summarize( media_AMG = mean(AMG, na.rm=TRUE), n = n()) %&gt;% ggplot(aes(x=n, y=media_AMG, fill=Solo)) + geom_point(shape=21, size=3) Observe que PV1, PV5 e R apresentaram \\(n &lt; 10\\). 9.8 Exercício 1) Explorar graficamente os dados: dados "],["análise-exploratória-de-dados-aed.html", "10 Análise Exploratória de Dados (AED) 10.1 Algumas funções de resumo 10.2 Descrição gráfica 10.3 Aplicação de modelos 10.4 Exercícios", " 10 Análise Exploratória de Dados (AED) A AED não é um processo com um conjunto de regras rígidas. Mais do que qualquer coisa, é uma estado de espírito. Durante as fazes iniciais de AED, devemos ser livres para investigar cada ideia que ocorra. À medida que a exploração segue, nos direcionamos para algumas áreas particularmente produtivas que, por fim, deverão ser escritas e comunicadas. É considera uma fase importante de qualquer análise, pois mesmo se as perguntas já foram formuladas, sempre será necessário pesquisar a qualidade dos dados que temos em mãos. Assim, a limpeza de dados é apenas uma aplicação da AED, na qual serão necessárias as ferramentas de visualização, transformação e modelagem. A AED é o cálculo das estatísticas tradicionais, alguns exemplos: Univariadas: média, mediana, desvio padrão, 1º e 3º quartis, mínimo, máximo, coeficientes die variação, de assimetria e curtosis, histogramas, boxplot, gráficos de regressão, gráficos de dispersão, entre outros. Bivariadas  análise de cluster, análise de componentes principais, análise de fatores e análise de variância. 10.1 Algumas funções de resumo Aliadas à média e à contagem, várias outras funções podem ser utilizadas em R. Medidas de localização A função median() retorna a mediana do conjunto de dados, ou seja, um valor onde \\(50\\%\\) de \\(x\\) está acima e \\(50\\%\\) está abaixo dela. Já a média mean() é a soma dividida pelo comprimento. Assim, a mediana não é influenciada por valores extremos, diferente da média. Compare a diferença entre média e mediana para as variáveis ARGILA e em seguida P. $$ = \\[\\begin{cases} \\text{Se n é ímpar, n=2k+1, então } x_{(k+1)} \\\\ \\text{Se n é par, n=2k, então } \\frac{x_{(k)} + x_{(k+1)}}{2} \\end{cases}\\] $$ Entrada de dados library(tidyverse) URL &lt;- &quot;https://raw.githubusercontent.com/arpanosso/r_data_science_fcav/master/dados/geomorfologia.txt&quot; geomorfologia&lt;-read.table(URL,header = TRUE) ARGILA # Para Argila geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(Media = mean(ARGILA, na.rm = TRUE), Mediana = median(ARGILA, na.rm = TRUE)) ## # A tibble: 3 x 3 ## SUP Media Mediana ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 I 21.1 20.7 ## 2 II 14.8 14.3 ## 3 III 10.1 10.1 geomorfologia %&gt;% ggplot(aes(x=ARGILA,fill=SUP)) + geom_histogram(bins=15,color=&quot;black&quot;)+ facet_wrap(~SUP,scales=&quot;free&quot;) Fósforo (P) # Para P geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(Media = mean(P, na.rm = TRUE), Mediana = median(P, na.rm = TRUE)) ## # A tibble: 3 x 3 ## SUP Media Mediana ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 I 17.8 12 ## 2 II 34.9 12.5 ## 3 III 14.0 9 geomorfologia %&gt;% ggplot(aes(x=P,fill=SUP)) + geom_histogram(bins=15,color=&quot;black&quot;)+ facet_wrap(~SUP,scales=&quot;free&quot;) Medidas de dispersão O desvio padrão sd() é a medida de dispersão padrão. Outras medidas podem ser utilizadas, como a variação interquartil IQR() e o desvio absoluto médio mad() são equivalentes robustos que podem ser mais úteis se você tiver outliers. # Para P geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(Media = mean(P, na.rm = TRUE), Mediana = median(P, na.rm = TRUE), DP=sd(P, na.rm = TRUE), IQR=IQR(P, na.rm = TRUE), DA_mediano = mad(P, na.rm = TRUE)) ## # A tibble: 3 x 6 ## SUP Media Mediana DP IQR DA_mediano ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 I 17.8 12 13.2 19 11.9 ## 2 II 34.9 12.5 47.5 39.5 15.6 ## 3 III 14.0 9 14.7 10 7.41 Medidas de classificação São os valores mínimo/máximo e os quantis. Quantis são generalizações da mediana. Por exemplo, quantile(x, 0.25) encontrará o valor de x que é maior que \\(25\\%\\) dos valores e menor do que os \\(75\\%\\) restantes. Já quantile(x, 0.75) encontrará o valor de x que é maior que \\(75\\%\\) dos valores e menor do que os \\(25\\%\\) restantes. # Para P geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(Media = mean(P, na.rm = TRUE), Mediana = median(P, na.rm = TRUE), DP=sd(P, na.rm = TRUE), IQR=IQR(P, na.rm = TRUE), DA_mediano = mad(P, na.rm = TRUE), Mínimo = min(P, na.rm = TRUE), Q1 = quantile(P, 0.25, na.rm = TRUE), Q3 = quantile(P, 0.75, na.rm = TRUE), Máximo = max(P, na.rm = TRUE)) ## # A tibble: 3 x 10 ## SUP Media Mediana DP IQR DA_mediano Mínimo Q1 Q3 Máximo ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 I 17.8 12 13.2 19 11.9 4 6 25 42 ## 2 II 34.9 12.5 47.5 39.5 15.6 2 4 43.5 209 ## 3 III 14.0 9 14.7 10 7.41 3 4 14 56 Contagens Vamos agora contar o número de observações e o número de valores não faltantes, utilizando as funções n() e sum(!is.na(x)): # Para P geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(Media = mean(P, na.rm = TRUE), Mediana = median(P, na.rm = TRUE), DP=sd(P, na.rm = TRUE), IQR=IQR(P, na.rm = TRUE), DA_mediano = mad(P, na.rm = TRUE), Mínimo = min(P, na.rm = TRUE), Q1 = quantile(P, 0.25, na.rm = TRUE), Q3 = quantile(P, 0.75, na.rm = TRUE), Máximo = max(P, na.rm = TRUE), N=n(), N_na = sum(!is.na(P))) ## # A tibble: 3 x 12 ## SUP Media Mediana DP IQR DA_mediano Mínimo Q1 Q3 Máximo N ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 I 17.8 12 13.2 19 11.9 4 6 25 42 17 ## 2 II 34.9 12.5 47.5 39.5 15.6 2 4 43.5 209 62 ## 3 III 14.0 9 14.7 10 7.41 3 4 14 56 27 ## # ... with 1 more variable: N_na &lt;int&gt; Contagens e proporções de valores lógicos sum(x &gt; 10), mean(y == 0). Quando usado com funções numéricas, TRUE é convertido em \\(1\\), e FALSE em \\(0\\). Isso torna sum() e mean() muito úteis: sum(x) retorna o número de TRUE em x e mean(x) retorna a proporção de valores verdadeiros. Por exemplo, em cada supefície geomórfica, vamos calcular o número e a proporção de valores de tor de fósforo disponível no solo (P) inferiores a 10 \\(mg /dm^3\\). # Para P geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(Media = mean(P, na.rm = TRUE), Mediana = median(P, na.rm = TRUE), DP=sd(P, na.rm = TRUE), IQR=IQR(P, na.rm = TRUE), DA_mediano = mad(P, na.rm = TRUE), Mínimo = min(P, na.rm = TRUE), Q1 = quantile(P, 0.25, na.rm = TRUE), Q3 = quantile(P, 0.75, na.rm = TRUE), Máximo = max(P, na.rm = TRUE), N=n(), N_na = sum(!is.na(P)), n10=sum(P&lt;10), n10p=mean(P&lt;10)) ## # A tibble: 3 x 14 ## SUP Media Mediana DP IQR DA_mediano Mínimo Q1 Q3 Máximo N ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 I 17.8 12 13.2 19 11.9 4 6 25 42 17 ## 2 II 34.9 12.5 47.5 39.5 15.6 2 4 43.5 209 62 ## 3 III 14.0 9 14.7 10 7.41 3 4 14 56 27 ## # ... with 3 more variables: N_na &lt;int&gt;, n10 &lt;int&gt;, n10p &lt;dbl&gt; Medidas da Forma da Distribuição As medidas da forma da distribuição são os coeficientes de assimetria e curtose. Assimetria - é uma medida da simetria da distribuição de frequência. Ela mostra se os desvios da média são maiores para um lado da distribuição do que para o outro. Usualmente, a estimativa do Coeficiente de Assimetria pode ser calculada pela fórmula: \\[ G_1 = \\frac{n}{(n-1)(n-2)} \\cdot \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^3}{s^3} \\] Vamos utilizar a função skewness() do pacote agricolae para calcular o coeficente de assimetria. # Para P geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(Media = mean(P, na.rm = TRUE), Mediana = median(P, na.rm = TRUE), DP=sd(P, na.rm = TRUE), IQR=IQR(P, na.rm = TRUE), DA_mediano = mad(P, na.rm = TRUE), Mínimo = min(P, na.rm = TRUE), Q1 = quantile(P, 0.25, na.rm = TRUE), Q3 = quantile(P, 0.75, na.rm = TRUE), Máximo = max(P, na.rm = TRUE), N=n(), N_na = sum(!is.na(P)), n10=sum(P&lt;10), n10p=mean(P&lt;10), G1 = agricolae::skewness(P)) ## # A tibble: 3 x 15 ## SUP Media Mediana DP IQR DA_mediano Mínimo Q1 Q3 Máximo N ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 I 17.8 12 13.2 19 11.9 4 6 25 42 17 ## 2 II 34.9 12.5 47.5 39.5 15.6 2 4 43.5 209 62 ## 3 III 14.0 9 14.7 10 7.41 3 4 14 56 27 ## # ... with 4 more variables: N_na &lt;int&gt;, n10 &lt;int&gt;, n10p &lt;dbl&gt;, G1 &lt;dbl&gt; Se as observações apresentam distribuição simétrica temos \\(G_1=0\\), ou próximas a \\(0\\). O coeficiente de assimetria é o indicativo mais comum de Normalidade. Em dados assimétricos, existem dúvidas sobre qual medida de tendência central devemos utilizar para resumir os dados. Nesses casos, comparações entre médias de diferentes conjuntos de observações são não confiáveis, uma vez que a variância pode diferir substancialmente de um conjunto de observações para outro. geomorfologia %&gt;% ggplot(aes(x=P,y=..density..)) + geom_histogram(bins=30, color=&quot;black&quot;, fill=&quot;white&quot;)+ facet_wrap(~SUP, scales = &quot;free&quot;)+ geom_density(alpha=0.1,fill=&quot;red&quot;) Curtose - Indica o grau de achatamento de uma distribuição, é a medida do peso das caudas da distribuição. Se as observações seguem uma distribuição normal, então o coeficiente de curtose é zero, e sua estimativa é dado por: \\[ G_2 = \\frac{\\frac{\\sum{i=1}^n(x-\\bar{x})^4}{s^4}}{(n-3)(n-2)(n-1)}-3 \\cdot \\frac{(n-1)^2}{(n-2)(n-3)} \\] Vamos utilizar a função kurtosis() do pacote agricolae para calcular o coeficente de curtose. # Para P geomorfologia %&gt;% group_by(SUP) %&gt;% summarize(Media = mean(P, na.rm = TRUE), Mediana = median(P, na.rm = TRUE), DP=sd(P, na.rm = TRUE), IQR=IQR(P, na.rm = TRUE), DA_mediano = mad(P, na.rm = TRUE), Mínimo = min(P, na.rm = TRUE), Q1 = quantile(P, 0.25, na.rm = TRUE), Q3 = quantile(P, 0.75, na.rm = TRUE), Máximo = max(P, na.rm = TRUE), N=n(), N_na = sum(!is.na(P)), n10=sum(P&lt;10), n10p=mean(P&lt;10), G1 = agricolae::skewness(P), G2 = agricolae::kurtosis(P)) ## # A tibble: 3 x 16 ## SUP Media Mediana DP IQR DA_mediano Mínimo Q1 Q3 Máximo N ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 I 17.8 12 13.2 19 11.9 4 6 25 42 17 ## 2 II 34.9 12.5 47.5 39.5 15.6 2 4 43.5 209 62 ## 3 III 14.0 9 14.7 10 7.41 3 4 14 56 27 ## # ... with 5 more variables: N_na &lt;int&gt;, n10 &lt;int&gt;, n10p &lt;dbl&gt;, G1 &lt;dbl&gt;, ## # G2 &lt;dbl&gt; geomorfologia %&gt;% ggplot(aes(x=P,y=..density..)) + geom_histogram(bins=30, color=&quot;black&quot;, fill=&quot;white&quot;)+ facet_wrap(~SUP, scales = &quot;free&quot;)+ geom_density(alpha=0.1,fill=&quot;red&quot;) 10.2 Descrição gráfica Visualizar a distribuição de uma variável dependerá se a variável é categórica ou contínua. Uma variável categórica só pode assumir um pequeno conjunto de valores. Em R essas variáveis são geralmente salvas na forma de fatores ou strings. Importação dos dados Vamos realizar a importação, via web, do banco de dados transectos.txt. URL &lt;- &quot;https://raw.githubusercontent.com/arpanosso/r_data_science_fcav/master/dados/transectos.txt&quot; transectos&lt;-read.table(URL,header = TRUE) glimpse(transectos) ## Rows: 302 ## Columns: 7 ## $ Amostra &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1~ ## $ Transecto &lt;chr&gt; &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, ~ ## $ X &lt;int&gt; 0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600~ ## $ Y &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ ## $ Argila &lt;int&gt; 57, 58, 64, 65, 66, 70, 71, 72, 73, 72, 70, 64, 69, 69, 67, ~ ## $ Silte &lt;int&gt; 13, 13, 16, 16, 14, 15, 17, 18, 19, 20, 19, 21, 19, 18, 21, ~ ## $ Areia &lt;int&gt; 30, 29, 20, 19, 20, 15, 12, 10, 8, 8, 11, 15, 12, 13, 12, 12~ As colunas X e Y denotam as coordenadas de cada ponto amostral dentro do gradeado experimental. Vamos vizualizar o gradeado. transectos %&gt;% ggplot(aes(x=X, y=Y))+ geom_point(size=2)+ theme_minimal() glimpse(transectos) ## Rows: 302 ## Columns: 7 ## $ Amostra &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1~ ## $ Transecto &lt;chr&gt; &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, ~ ## $ X &lt;int&gt; 0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600~ ## $ Y &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ ## $ Argila &lt;int&gt; 57, 58, 64, 65, 66, 70, 71, 72, 73, 72, 70, 64, 69, 69, 67, ~ ## $ Silte &lt;int&gt; 13, 13, 16, 16, 14, 15, 17, 18, 19, 20, 19, 21, 19, 18, 21, ~ ## $ Areia &lt;int&gt; 30, 29, 20, 19, 20, 15, 12, 10, 8, 8, 11, 15, 12, 13, 12, 12~ Agora, vamos criar um gráfico de barras para visualizar a variável categórica Transecto, ou seja, o número de observações em cada transecto. transectos %&gt;% ggplot(aes(x = Transecto))+ geom_bar(fill=&quot;lightblue&quot;, color=&quot;black&quot;) Muitas vezes precisamos modificar a escala dos eixos para melhorar e focar nossa visualização nas diferenças, então, vamos utilizar a função coord_cartesian(), aliada aos argumentos xmin e/ou ymin, ambos os argumentos deverãp receber um vetor com dois valores para a definição do menor e maior valor da escala: transectos %&gt;% ggplot(aes(x = Transecto))+ geom_bar(fill=&quot;lightblue&quot;,color=&quot;black&quot;) + coord_cartesian(ylim= c(55,63)) Agora vamos visualizar a tabela com os valores contados. transectos %&gt;% count(Transecto) ## Transecto n ## 1 I 58 ## 2 II 61 ## 3 III 63 ## 4 IV 62 ## 5 V 58 Uma variável contínua pode assumir qualquer valor de um conjunto infinito de valores ordenados. Para examinarmos a distribuição de uma variável contínua, vamos utilizar o histograma. Argila transectos %&gt;% ggplot(aes(x=Argila)) + geom_histogram(bins=20, color=&quot;black&quot;,fill=&quot;lightgray&quot;) Podemos sobrepor vários histogramas no mesmo gráfico, para isso, utilize geom_freqpoly(). Observe que vamos criar os polígonos a partir da densidade e frequência \\(d_i\\): \\[ d_i = \\frac{f_i}{\\Delta_i} \\] onde, \\(f_i\\) é a frequência relativa dada pela contagem de cada classe individual \\(n_i\\), dividida pelo número total de observações (\\(n\\)). \\[ f_i = \\frac{n_i}{n} \\] \\(\\Delta_i\\) é o intervalo de cada classe específica \\(i\\). transectos %&gt;% ggplot(aes(x=Argila, color=Transecto, y= ..density..)) + geom_freqpoly() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. O histograma fornece informação sobre: * Tipo de distribuição; * Unimodal ou multimodal; * Presença de valores extremos (outliers); * Variabilidade. Outra possibilidade é o uso do Boxplot, para isso vamos verificar a variável Silte: transectos %&gt;% ggplot(aes(y=Silte)) + geom_boxplot() Vamos modificar o gráfico, alterando as cores e a escala do eixo X para modificar a caixa. transectos %&gt;% ggplot(aes(y=Silte)) + geom_boxplot(color=&quot;black&quot;,fill=&quot;lightblue&quot;)+ coord_cartesian(xlim=c(-1,1))+ theme_minimal() Podemos construir o boxplot segmentado por transecto. transectos %&gt;% ggplot(aes(y=Silte, x=Transecto, fill=Transecto)) + geom_boxplot() Para facilitar a visualização da tendência, pode-se reordenar Transecto com base no valor médio da variável estudada (teor de silte do solo, no caso). transectos %&gt;% ggplot(aes(y=Silte, x=reorder(Transecto,Silte,mean), fill=Transecto)) + geom_boxplot() Se temos nomes longos, podemos rotacionar o gráfico 90º com coord_flip(). transectos %&gt;% ggplot(aes(y=Silte, x=reorder(Transecto, Silte, mean), fill=Transecto)) + geom_boxplot()+ coord_flip() Violin plot O violin plot é um método de visualizar dados numéricos. É semelhante ao boxplot, com a adição de um gráfico de densidade girado em cada lado. Essa representação, portanto, também mostram a densidade de probabilidade dos dados em valores diferentes. Normalmente, um gráfico de violino incluirá todos valores que estão no boxplot: um marcador para a mediana dos dados, uma caixa ou marcador indicando o intervalo interquartil e, possivelmente, todos os pontos de amostra, se o número de amostras não for muito alto. A diferença é particularmente útil quando a distribuição de dados é multimodal (mais de um pico). Neste caso, um gráfico de violino mostra a presença de diferentes picos, sua posição e amplitude relativa. transectos %&gt;% ggplot(aes(y=Silte, x=reorder(Transecto, Silte, mean), fill=Transecto)) + geom_violin(trim = FALSE)+ stat_summary(fun = median, geom = &quot;point&quot;, shape=21, size=3, color=&quot;black&quot;, fill=&quot;gray&quot;) + theme(legend.position=&quot;none&quot;) A densidade de distribuição dos pontos pode ser visualizada com a função geom_dotplot() par aa apresentação dos pontos amostrais. transectos %&gt;% ggplot(aes(x=Transecto,y=Silte))+ geom_violin(trim = FALSE) + geom_dotplot(binaxis=&#39;y&#39;, stackdir=&#39;center&#39;, dotsize=1) Muitas vezes, podemos mesclar o boxplot com o violin plot. A largura das caixas do boxplot pode ser controlada com o argumento width. transectos %&gt;% ggplot(aes(y=Silte, x=reorder(Transecto, Silte, mean), fill=Transecto)) + geom_violin(trim=FALSE, fill=&quot;lightgray&quot;)+ geom_boxplot(width=0.1)+ theme_classic() Função Densidade Acumulada Empírica A função de distribuição cumulativa empírica (empirical cumulative distribution function - ECDF) fornece uma visualização alternativa da distribuição. Em comparação com outras visualizações que dependem da densidade (como geom_histogram()), o ECDF não requer nenhum parâmetro de ajuste e lida com variáveis contínuas e categóricas. A desvantagem é que requer mais treinamento para interpretar com precisão e as tarefas visuais subjacentes são um pouco mais desafiadoras. A função de distribuição acumulada descreve como probabilidades são associadas aos valores ou aos intervalos de valores de uma variável aleatória. Ela representa a probabilidade de uma variável aleatória ser menor ou igual a um valor real \\(x\\). \\[ F(x) = P(X \\le x) \\] Em R pode ser desenhanda com stat_ecdf() transectos %&gt;% ggplot(aes(x=Silte)) + stat_ecdf(geom = &quot;line&quot;) A representação pode ser segmentada por cada categoria da variável categórica. transectos %&gt;% ggplot(aes(x=Silte, color= Transecto)) + stat_ecdf(geom = &quot;line&quot;) As variáveis Argila, Silte e Areia são expressas em porcentagem, assim, poderíamos colocá-las em um mesmo gráfico e adicionar uma legenda para identificar cada uma delas. Podemos realizar essa tarefa utilizando a função gather() do pacote tidyr. Literalmente essa função empilha o banco de dados. Nesse exemplo, vamos criar uma nova variável denominada granulometria que deverá receber repedidamente os nomes das colunas (Areia, Silte e Argila), preservando o valor numérico dessas variáveis que serão empilhados na nova variável valor, como exemlificado na figura abaixo: Figure 10.1: Exemplo simples do comportamento da função gather(). A função tem como argumentos os nomes das duas novas colunas granulometria e valor, sendo o terceiro argumento um vetor identificando as posições das colunas que deverão ser empilhadas (5:7, no caso). transectos %&gt;% gather(granulometria, valor, 5:7) %&gt;% View() Podemos completar o código acima pedindo a partir do ggplot() o boxplot por granulometria em cada posição de X, por exemplo: transectos %&gt;% gather(granulometria, valor, 5:7) %&gt;% ggplot(aes(y=valor, x=as.factor(X),fill=granulometria)) + geom_boxplot()+ theme(axis.text.x=element_text(angle=90, hjus=1,size = rel(0.8))) Ou, podemos pedir para cada transecto Y, no caso: transectos %&gt;% gather(granulometria, valor, 5:7) %&gt;% ggplot(aes(y=valor, x=as.factor(Y),fill=granulometria)) + geom_boxplot()+ theme(axis.text.x=element_text(angle=90, hjus=1,size = rel(0.8))) 10.3 Aplicação de modelos Até agora para a análise exploratória dos dados recomendamos o cálculo de vparias medidas de dispersão e posição, e a representação gráfica, que nos auxiliará na interpretação da forma da distribuição dos dados. O próximo passo é adicionarmos um modelo matemático que descreva o comportamento do dados. Agora vamos aplicar a estimativa da densidade alisa geom_density() ao nosso histograma. transectos %&gt;% ggplot(aes(x=Silte,y=..density..)) + geom_histogram(bins=20, color=&quot;black&quot;,fill=&quot;lightgray&quot;) + geom_density(linetype=2,col=&quot;blue&quot;, lwd=1) Hipóteses estatísticas do teste da normalidade dos dados \\[ \\begin{cases} H_0: \\text{Os dados tem distribuição normal }[X \\sim N(\\mu,\\sigma^2) ]\\\\ H_1: \\text{Os dados não tem distribuição normal} \\end{cases} \\] Todo teste estatístico fornece os valores da estatística do teste e o valor de probabilidade associado a essa estatística \\(p\\), por exemplo: silte &lt;- transectos$Silte shapiro.test(silte) ## ## Shapiro-Wilk normality test ## ## data: silte ## W = 0.98997, p-value = 0.03625 Regra de decisão (\\(\\alpha=0,01\\) ou \\(1\\%\\)) *Se o valor de \\(p \\ge 0,01\\), o teste é não significativo, portanto não rejeitamos \\(H_0\\), e concluímos que os dados suportam a hipótese de uma distribuição normal teórica. *Se \\(p \\le 0,01\\), o teste é significativo, portanto rejeitamos \\(H_0\\), tomamos \\(H_1\\) como verdadeira e concluímos que os dados não suportam a suposição de normalidade. Outros testes de normalidade disponíveis no R com o Pacote nortest library(nortest) ## Warning: package &#39;nortest&#39; was built under R version 4.1.1 lillie.test(silte) #Kolmogorov-Smirnov ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: silte ## D = 0.069523, p-value = 0.001269 cvm.test(silte) #Cramer von Mises ## ## Cramer-von Mises normality test ## ## data: silte ## W = 0.18798, p-value = 0.007586 ad.test(silte) #Anderson-Darling ## ## Anderson-Darling normality test ## ## data: silte ## A = 1.0444, p-value = 0.009437 Um gráfico quantil-quantil (ou QQPlot, stat_qq()) é usado para verificar se uma dada variável segue a distribuição normal. Os dados são considerados normalmente distribuídos quando os pontos seguem aproximadamente a linha de referência de 45º (1:1, stat_qq_line()). transectos %&gt;% ggplot(aes(sample = Silte))+ stat_qq(color=&quot;blue&quot;) + stat_qq_line(color=&quot;red&quot;) Uma vez que os dados seguem a distribuição normal, podemos adicionar ao histograma da variável a curva teórica. Para sobrepor a curva da função, adicionamos a opção stat_function(fun = dnorm) e especificamos a forma usando os argumentos mean = mean(silte) e sd = sd(silte) que deve ser passado apra o argumento args na forma de uma lista list(). Se você tem dados ausentes, certifique-se de passar o argumento com na.rm = TRUE para os parâmetros de mean e sd. Finalmente, podemos alterar a cor usando o argumento color = \"red\". transectos %&gt;% ggplot(aes(x=Silte)) + geom_histogram(aes(y=..density..),bins=20, color=&quot;black&quot;,fill=&quot;lightgray&quot;) + stat_function(fun = dnorm, colour = &quot;red&quot;, args = list(mean = mean(silte, na.rm = TRUE), sd = sd(silte, na.rm = TRUE))) Vamos salvar o gráfico em um novo plot. hist_silte &lt;- transectos %&gt;% ggplot(aes(x=Silte)) + geom_histogram(aes(y=..density..),bins=20, color=&quot;black&quot;,fill=&quot;lightgray&quot;) + stat_function(fun = dnorm, colour = &quot;red&quot;, args = list(mean = mean(silte, na.rm = TRUE), sd = sd(silte, na.rm = TRUE))) Para alterar os rótulos dos eixos, vamos apresentar uma nova opção. Nesse caso, usaremos as opções scale_x_continuous e scale_y_continuous, pois essas funões possuem outros argumentos de personalização para os eixos que usaremos a seguir. Em cada um, adicionamos o nome desejado ao argumento do nome como uma string. hist_silte + scale_x_continuous(name = &quot;Teor de Silte no solo\\n(%)&quot;) + scale_y_continuous(name = &quot;Densidade&quot;) Observe que o ggplot também permite o uso de nomes de várias linhas (em eixos e títulos). Aqui, alteramos o rótulo do eixo x para que passe por duas linhas usando o caractere \"\\n\" para quebrar a linha. Agora vamos fazer com que as marcações do eixo x apareçam a cada 2 unidades em vez de 5 usando o argumento breaks = seq (4, 30, 2) em scale_x_continuous. hist_silte + scale_x_continuous(name = &quot;Teor de Silte no solo\\n(%)&quot;, breaks = seq (4, 30, 2)) + scale_y_continuous(name = &quot;Densidade&quot;) Você também pode adicionar um gradiente ao seu esquema de cores que varia de acordo com a frequência dos valores. Para fazer isso, alteramos o argumento aes(y = ..count ..) em geom_histogram() para aes(fill = ..count..). hist_silte + geom_histogram(aes(fill = ..count..),bins=20)+ scale_x_continuous(name = &quot;Teor de Silte no solo\\n(%)&quot;, breaks = seq (4, 30, 2)) + scale_y_continuous(name = &quot;Densidade&quot;) Podemos, personalizar o gradiente alterando as cores de ancoragem para alto e baixo. Para isso, adicionamos a opção scale_fill_gradient ao gráfico com os argumentos Count (o nome da legenda), low (a cor dos valores menos frequentes) e high (a cor dos valores mais frequentes). hist_silte + geom_histogram(aes(fill = ..count..),bins=20)+ scale_x_continuous(name = &quot;Teor de Silte no solo\\n(%)&quot;, breaks = seq (4, 30, 2)) + scale_y_continuous(name = &quot;Densidade&quot;) + scale_fill_gradient(&quot;Count&quot;, low=&quot;blue&quot;, high = &quot;red&quot;) Para representar os diferentes transectos acrescentamos na função geom_histogram um argumento fill = Transecto a aes(). Em segundo lugar, para ver mais claramente o gráfico, adicionamos dois argumentos, position = \"identity\" e alpha = 0.6. Isso controla a posição e a transparência das colunas, respectivamente. Finalmente, você pode personalizar as cores dos histogramas adicionando scale_fill_brewer ao gráfico do pacoteRColorBrewer. hist_silte + geom_histogram(aes(fill = Transecto),bins=20, position=&quot;identity&quot;, alpha=0.6)+ scale_x_continuous(name = &quot;Teor de Silte no solo\\n(%)&quot;, breaks = seq (4, 30, 2)) + scale_y_continuous(name = &quot;Densidade&quot;) + scale_fill_brewer(palette=&quot;Accent&quot;) Modelos de Distribuição  prós e contras Vantagens: fornece uma descrição da distribuição com poucos parâmetros (geralmente só dois). Desvantagem: Distribuição Normal ou lognormal muito raramente são encontradas nas ciências agrárias. Para análise exploratória de dados, modelos de distribuição, geralmente, não são necessários. 10.4 Exercícios 1) O conjunto de dados denominado de BroomBarnFarm.txt foi coletado em uma área de 80 ha no leste da Inglaterra. O solo foi amostrado em intervalos de 40 m. As amostras de solo na profundidade de 0,20 m foram analisadas em laboratório e como resultado temos: 435 valores para cada variável (K, pH e P). (WEBSTER &amp; OLIVER. Geostatistics for Environmental Scientists, Appendix B). Responda as seguintes questões: Calcular a média, a mediana, o desvio padrão, o 1º (Q1) e o 3º (Q3) quartis, os coeficientes de assimetria, de curtosis, de variação (CV), as observações máxima e mínima das 3 variáveis do conjunto de dados. Construa os gráficos Boxplot, histogramas e da função de distribuição acumulada empírica para as 3 variáveis. Discuta o que você observa. Refaça o item anterior com os dados na escala logarítmica. Discuta os resultados. 2) No arquivo geomorfologia.txt que possui dados de atributos de solo de um transecto existe uma coluna classificatória das superfícies geomórficas identificadas na área de estudo. Faça gráficos de dispersão dos dados das variáveis Arg, P e Ca para todo o transecto e, em seguida, para cada uma das 3 superfícies (I, II e III). calcule as estatísticas descritivas destas superfícies que foram identificadas na área (monte uma tabela com: o tamanho da amostra (n), a média, mediana, o desvio-padrão, os coeficientes de assimetria, curtose e de variação). faça gráficos (histogramas, boxplots para descrever as distribuições, quanto as formas, das três variáveis Arg, P e Ca para cada superfície. "],["distribuição-de-probabilidade.html", "11 Distribuição De Probabilidade 11.1 Distribuição Normal 11.2 Distribuição Normal Padronizada 11.3 Exercícios", " 11 Distribuição De Probabilidade 11.1 Distribuição Normal A exata curva de densidade para uma particular distribuição normal é descrita pela sua média \\(\\mu\\) e pelo seu desvio padrão \\(\\sigma\\), cuja função de densidade de probabilidade é dada por: \\[ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2}}, -\\infty &lt; x &lt; \\infty \\] A curva é totalmente caracterizada por \\(\\mu\\) e \\(\\sigma\\). Vamos utilizar a função curve() junto à função dnorm() para a construção da representação gráfica da distribuição normal. A função dnorm() retornará o valor de densidade de frequência, ou seja, \\(f(x)\\) para uma média e desvio-padrão definidos em seus argumentos mean e sd, respectivamente. # Definião dos parâmteros media &lt;- 15 dp &lt;- 3 # Construção da curva curve(dnorm(x,mean=media,sd=dp),5,25,col=&quot;red&quot;) A curva normal modela a distribuição de frequência de muitos eventos biológicos. Muitas das estatísticas utilizadas na inferência estatística seguem a distribuição normal. É a mais importante das distribuições contínuas por causa do seu papel na teoria amostral. Como visto, a curva é simétrica é relação à média \\(\\mu\\) e tem área sob ela igual a \\(1\\). \\[ \\int_{-\\infty}^{\\infty} f(x) dx = 1 \\] Portanto, vamos adicionar uma linha no gráfico anterior, representando \\(\\mu\\). curve(dnorm(x,mean=media,sd=dp),5,25,col=&quot;red&quot;) abline(v=media,lty=2) Assim, a área (probabilidade) até o valor de \\(\\mu\\) é metade da área total, ou seja \\(0,5\\). Vamos buscar esse valor com a função pnorm(), utiizando como argumento o valor do eixo x, aqui denominado quantil (q) desejado e os parâmetros média e desvio padrão. A função sempre retorna a área acumulada de \\(-\\infty\\) até o valor de q. pnorm(q=media, mean=media, sd=dp) ## [1] 0.5 Em outras palavras, calculamos \\(P(X&lt;\\mu)\\) Calcular a área entre os pontos de inflexão da curva, ou seja: \\[ \\int_{\\mu - \\sigma}^{\\mu+\\sigma} f(x) dx = 0,6826895 \\] curve(dnorm(x,mean=media,sd=dp),5,25,col=&quot;red&quot;) abline(v=media-dp,lty=2) abline(v=media+dp,lty=2) Calcular a área entre: \\[ \\int_{\\mu - 2\\sigma}^{\\mu+2\\sigma} f(x) dx = 0,9544997 \\] curve(dnorm(x,mean=media,sd=dp),5,25,col=&quot;red&quot;) abline(v=media-2*dp,lty=2) abline(v=media+2*dp,lty=2) Calcular a área entre: \\[ \\int_{\\mu - 3\\sigma}^{\\mu+3\\sigma} f(x) dx = 0,9973002 \\] curve(dnorm(x,mean=media,sd=dp),5,25,col=&quot;red&quot;) abline(v=media-3*dp,lty=2) abline(v=media+3*dp,lty=2) Qual o valor de \\(X\\) que supera \\(65\\%\\) das observações, ou seja, qual o valor de \\(X\\) cuja área até ele é \\(0,65\\). Para isso, utilizaremos a função qnorm() que retornará o quantil dado a probabilidade acumulada até ele. qnorm(0.65, media, dp) ## [1] 16.15596 Vamos tirar a prova: pnorm(16.15596,media,dp) ## [1] 0.6499998 Dado a distribuição abaixo, calcular \\(P(X &lt; 99)\\) Figure 11.1: Resumo do funcionamento das funções pnorm, qnorm e dnorm. Estudo dos parâmetros O efeito da mudança dos parâmetros \\(\\mu\\) e \\(\\sigma\\) na distribuição normal: a) Diferentes médias e mesmo desvio padrão. curve(dnorm(x,50,7),xlim=c(20,180),lwd=2,col=2, ylab=&quot;Densidade&quot;,xlab=&quot;Classes&quot;,cex.lab=1.3,las=1) curve(dnorm(x,100,7),lwd=2,col=4,ylab=&quot;Densidade&quot;,add=TRUE) curve(dnorm(x,150,7),lwd=2,col=3,ylab=&quot;Densidade&quot;,add=TRUE) abline(h=0,col=&quot;gray&quot;,lwd=1.6) text(c(50,100,150)+15,.02,expression(sigma==7),cex=1.6) text(c(50,100,150),.0029,cex=1.3, expression(paste(mu[1],&quot; = &quot;,sep=&quot;&quot;,50), paste(mu[2],&quot; = &quot;,sep=&quot;&quot;,100), paste(mu[3],&quot; = &quot;,sep=&quot;&quot;,150)) ) b) Diferentes desvio padrões e a mesma média. curve(dnorm(x,50,7),xlim=c(20,80),lwd=2,col=2,ylim=c(0,0.06), ylab=&quot;Densidade&quot;,xlab=&quot;Classes&quot;,cex.lab=1.3,las=1) curve(dnorm(x,50,11),lwd=2,col=1,ylab=&quot;Densidade&quot;,add=TRUE) curve(dnorm(x,50,16),lwd=2,col=4,ylab=&quot;Densidade&quot;,add=TRUE) abline(h=0,col=&quot;gray&quot;,lwd=1.6) lines(c(50,50),c(0,dnorm(50,50,7)),col=1,lwd=2) text(55,.002,expression(mu==50),cex=1.6) text(c(56,63,75)+2,c(.05,.025,0.01),cex=1.4, expression(paste(sigma[1],&quot; = &quot;,7), paste(sigma[2],&quot; = &quot;,11), paste(sigma[3],&quot; = &quot;,16)) ) 11.2 Distribuição Normal Padronizada É um caso particular da distribuição normal, quando a média é \\(0\\) e o desvio padrão é \\(1\\). A transformação de qualquer variável pode ser realizada por meio de: \\[ Z =\\frac{X-\\mu}{\\sigma} \\] assim, \\[ f(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}(z)^2} \\] Por meio da transformação, podemos calcular probabilidade. \\[ P(x_1 &lt; X &lt; x_2) = P(z_1 &lt; Z &lt; z_2) \\\\ com \\\\ z_1=\\frac{x_1-\\mu}{\\sigma} \\text{. e } z_2 = \\frac{x_2-\\mu}{\\sigma} \\] As funções dnorm(), pnorm() e qnorm() tem como valores default mean = 0 e sd = 1, assim: curve(dnorm(x),-4,4,col=&quot;blue&quot;,xlab=&quot;Z&quot;) abline(v=0,lty=2) Calcular a área entre: \\[ \\int_{-2}^{2} f(z) dz = 0,9544997 \\] curve(dnorm(x),-4,4,col=&quot;blue&quot;) abline(v=-2,lty=2) abline(v=2,lty=2) Qual o valor de \\(Z\\) que supera \\(97,5\\%\\) das observações, ou seja, qual o valor de \\(Z\\) cuja área até ele é \\(0,975\\). qnorm(0.975) ## [1] 1.959964 Vamos tirar a prova: pnorm(1.96) ## [1] 0.9750021 11.3 Exercícios 1)Resolver no R e representar graficamente: Se \\(X \\sim N(\\mu=13,\\sigma^2=16)\\), encontre: a) \\(P(X &lt; 20)\\) b) \\(P(X &gt;10)\\) c) \\(P(10 &lt; X &lt; 20)\\) d) \\(1- P(7&lt; X &lt; 19)\\) e) \\(1- P(14&lt; X &lt; 16)\\) 2) Seja \\(Z\\) uma variável aleatória com distribuição normal padrão \\([Z \\sim N(0,1)]\\) determine o valor de \\(t\\) tal que. a) \\(P(0 &lt; Z &lt; t) = 0,4236\\) b) \\(P(t &lt; Z &lt; 2) = 0,1000\\) c) \\(P(Z &lt; t) = 0,7967\\) d) \\(P(-t &lt; Z &lt; t) = 0,95\\) "],["experimentação-agrícola.html", "12 Experimentação Agrícola 12.1 Conceitos básicos 12.2 Medidas de posição e de dispersão", " 12 Experimentação Agrícola A Estatística Experimental tem como objetivo o estudo dos experimentos, isto é, seu planejamento, execução, análise dos dados e interpretação dos resultados obtidos. Para que um experimentador conduza e avalie uma pesquisa corretamente, é essencial um certo conhecimento de estatística, principalmente no que se refere às potencialidades e às limitações das técnicas utilizadas. Assim sendo, o curso de Experimentação Agrícola visa apresentar aos alunos os métodos estatística mais usado em Agronomia. Vejamos então, alguns conceitos básicos necessários para um bom entendimento da Estatística Experimental. 12.1 Conceitos básicos 12.1.1 POPULAÇÃO Boa parte do conhecimento humano está baseado em um número relativamente reduzido de informações. Isto é verdadeiro, tanto no que se refere aos problemas do cotidiano, como no que se refere à pesquisa científica. Por definição POPULAÇÃO é o conjunto de elementos que têm em comum uma determinada característica. Todo o subconjunto não vazio e com menor número de elementos do que o conjunto definido como população constitui, por definição, uma AMOSTRA desta população. Uma população pode ser o número total de indivíduos de uma determinada espécie animal em uma área definida. Por exemplo, o número total de lagartas de Spodoptera frugiperda em uma cultura de milho constitui uma população. Esta população, embora finita, é considerada para fins de amostragem como uma população infinita. Uma vez definida a unidade amostral (1 planta, um conjunto de 5 plantas, ou um quadrado no qual será contado o número de lagartas), a população pode ser considerada como um conjunto de unidades amostrais e um subconjunto tomado aleatoriamente deste subconjunto é chamado de AMOSTRA ALEATÓRIA DE TAMANHO N. Assim sendo, as observações são obtidas através de contagens do número de indivíduos em cada unidade amostral. Estas observações são chamadas de VARIÁVEL EM ESTUDO. 12.1.2 TRATAMENTO É o método, elemento ou material, cujo efeito se deseja medir ou comparar em um experimento. Por exemplo, um tratamento pode ser: Uma variedade de cana-de-açúcar; um híbrido de sorgo; uma dose de um adubo para a cultura do milho; um espaçamento para cultura do algodão; um recipiente para produção de mudas de eucalipto; um inseticida para controle de pragas, etc. 12.1.3 EXPERIMENTO OU ENSAIO Experimento é um trabalho previamente planejado, no qual se faz comparação dos efeitos dos tratamentos. Fonte: Arquivo pessoal (2008). 12.1.4 UNIDADE EXPERIMENTAL OU PARCELA É a unidade na qual o tratamento é aplicado. É na parcela que obtemos os dados que deverão refletir o efeito de cada tratamento no ensaio. A parcela pode ser constituída por uma planta, uma área com um grupo de plantas, uma placa de petri com um meio de cultura, um animal, um lote de animais, etc. Fonte: Marcelo (2011). 12.1.5 DELINEAMENTO EXPERIMENTAL É o plano utilizado na experimentação, e implica na forma como os tratamento deverão ser distribuídos nas unidades experimentais e como serão analisados os dados a serem obtidos. Como exemplo, temos o delineamento inteiramente casualizado (DIC), o delineamento em blocos casualizados (DBC) o delineamento em quadrado latino (DQL), entre outros. 12.2 Medidas de posição e de dispersão As populações são descritas por certas características chamadas de Parâmetros. A amostras são descritas pelas mesmas características, que, neste caso, são chamadas de Estimativas de Parâmetros, Estatísticas da amostra. Alguns destes parâmetros são chamado de medidas de posição e outros de medida de dispersão 12.2.1 MEDIDAS DE POSIÇÃO OU DE TENDÊNCIA CENTRAL Uma característica comum a todas as populações ou amostras é a varaibilidade dos indivíduos que as constituem. Geralmente, os dados de uma população ou amostra tendem a ser mais numerosos em torno de um valor central e vão se tornando mais raros à medida que no afatamos desse valor. A medida de posição representa o valor em torno do qual os dados observados tendem a se agrupar. Das medidas de posição, a mais utilizada é a Média Aritmética, que pode se definida como: A Soma de todas as observações divididas pelo número delas Assim, para uma população com \\(N\\) elementos, ditos \\(x_1\\), \\(x_2\\),  , \\(x_N\\), a média aritmética será: \\[ m = \\frac{\\sum_{i=1}^{N}x_i}{N} = \\frac{x_1+x_2+...+x_N}{N} \\] Para uma amostra com \\(n\\) elementos, ditos \\(x_1\\), \\(x_2\\),  , \\(x_n\\), a média aritmética será: \\[ \\bar{x}=\\hat{m} = \\frac{\\sum_{i=1}^{n}x_i}{n} = \\frac{x_1+x_2+...+x_n}{n} \\] exemplo Considere como exemplo os dados abaixo, referente à altura de plantas daninha (em cm) em uma amostra de 5 plantas em uma área de pastagem. \\(x_1\\) \\(x_2\\) \\(x_3\\) \\(x_4\\) \\(x_5\\) 5 3 2 4 3 Então, para estes dados, a estimativa da média aritmética será: \\[ \\hat{m} = \\frac{\\sum_{i=1}^{n}x_i}{n} = \\frac{5+3+2+4+3}{5} = \\frac{17}{5} = 3,4\\;cm \\] # Defina o vetor de dados X &lt;- c(5,3,2,4,3) # Calculando a soma dos elementos de X (G) G=sum(X) G ## [1] 17 # Encontrando o número total de elementos de X n=length(X) n ## [1] 5 # Calculando diretamente a média aritmética mean(X) ## [1] 3.4 A diferença entre um valor observado (\\(x_i\\)) e a média aritmética (\\(\\hat{m}\\)) é denominado de desvio (\\(d_i\\)), ou seja: \\[ d_i = x_i - \\hat{m}, \\text{ para i =1, 2, ...,n } \\] \\(d_1 = 5-3.4\\) \\(d_2=3-3.4\\) \\(d_3 = 2-3.4\\) \\(d_4 = 4-3.4\\) \\(d_5 = 3-3.4\\) 1,6 -0,4 -1,4 0,6 -0,4 Podemos mostrar que a soma dos desvios é igual a zero, para qualquer conjunto de dados. \\[ \\sum_{i=1}^{N}d_i = 0 \\] # Calculando os desvios d&lt;-X-mean(X) # Apresentando os valores de desvios d ## [1] 1.6 -0.4 -1.4 0.6 -0.4 # Prova de que o A Soma dos Desvios é igual a Zero round(sum(d)) # a função rond arredonda a saida da função soma para 0 casas decimais ## [1] 0 12.2.2 MEDIDAS DE DISPERSÃO As medidas de dispersão são também chamadas de medidas de variação, e medem o grau com que os dados tendem a se afastar de um valor central, que geralmente é a média aritmética. Como em todas as amostras (ou populações) ocorre variabilidade dos elementos que as constituem, amostra com mesma média pode apresentar distribuições diferentes, e portanto, somente a média não fornece informação clara de como os dados se distribuem. Assim, para representar melhor a maneira pela qual os dados se distribuem, são utilizadas as medidas de dispersão ou de variação. Dentre as medidas de dispersão, discutiremos a Variância, o Desvio Padrão, o Erro padrão da Média e o Coeficiente de variação. 12.2.3 VARIÂNCIA A variância é uma medida de dispersão que leva em conta todas as observações. É indiscutivelmente, a melhor medida de dispersão. A variância de uma população é representada por \\(\\sigma^2\\), lê-se sigma dois, e pode ser definida como A média dos quadrados dos desvios de todos os dados em relação à média aritmética. Então, para uma população com \\(N\\) temos: \\[ \\sigma^2 = \\frac{SQD}{N} = \\frac{d_1+d_2+...+d_N}{N} =\\frac{\\sum_{i=1}^Nd_i^2}{N} \\\\ \\sigma^2 =\\frac{\\sum_{i=1}^N(x_i-m)^2}{N} \\] Note que utilizando esta fórmula o cálculo da variancia seria bastante trabalhoso, no caso de N ser um número muito grande. Existe porém um método mais prático de se calcular a variância, que pode ser obtido desenvolvendo-se as fórmula da soma de quadrados dos desvios (SQD) . Assim, temos: \\[ SQD = \\sum_{i=1}^N(x_i-m)^2 = \\sum_{i=1}^Nx_i^2-\\frac{(\\sum_{i=1}^N x_i)^2}{N} \\] Então, a fórmula simplificada da variância será: \\[ \\sigma^2 =\\frac{\\sum_{i=1}^Nx_i^2-\\frac{(\\sum_{i=1}^N x_i)^2}{N}}{N} \\] A vantagem desta fórmula é que trabalhamos diretamente com os dados originais, não havendo necessidade de calcularmos a média e os desvios em relação a ela. O termo \\(\\frac{(\\sum_{i=1}^Nx_i)^2}{N}\\) é denomidado de Correção devido à média ou simplesmente Correção, representado por \\(C\\) e é de grande utilização nas futuras análise de variâncias. Normalmente, na prática, trabalhamos com amostras, e a estimativa da variância, representada por \\(s^2\\), é calculada, para uma amostra com \\(n\\) elementos, representados por \\(x_1\\), \\(x_2\\), ,\\(x_n\\), por: \\[ s^2 = \\frac{SQD}{n-1} = \\frac{d_1+d_2+...+d_n}{n-1} =\\frac{\\sum_{i=1}^nd_i^2}{n-1} \\\\ s^2 =\\frac{\\sum_{i=1}^n(x_i-\\hat{m})^2}{n-1} \\] ou ainda, \\[ s^2 =\\frac{\\sum_{i=1}^nx_i^2-\\frac{(\\sum_{i=1}^n x_i)^2}{n}}{n-1} \\] Observações: 1 - A variância tem sempre valor positivo, e sua unidade é quadrática. 2 - O denominador utilizado do cálculo da variância é chamado de grau de liberdade da estimativa da variância, sempre dado por \\(n-1\\) exemplo No exemplo anterior de altura de plantas daninhas: No caso temos \\(d_1 = 5-3.4\\) \\(d_2=3-3.4\\) \\(d_3 = 2-3.4\\) \\(d_4 = 4-3.4\\) \\(d_5 = 3-3.4\\) 1,6 -0,4 -1,4 0,6 -0,4 \\[ s^2 =\\frac{(1,6^2+(-0.4)^2+(-1,4)^2+0,6^2+(-0,4)^2)^2}{5-1} = \\frac{5.2}{4} = 1,3 \\;cm^2 \\] Pela fórmula que não utiliza os desvios teríamos: \\(x_1\\) \\(x_2\\) \\(x_3\\) \\(x_4\\) \\(x_5\\) 5 3 2 4 3 \\[ s^2 =\\frac{\\sum_{i=1}^nx_i^2-\\frac{(\\sum_{i=1}^n x_i)^2}{n}}{n-1} = \\frac{(5^2+3^2+2^2+4^2+3^2)-\\frac{(5+3+2+4+3)^2}{5}}{5-1} = \\frac{63-\\frac{(17)^2}{5}}{5-1} = \\frac{63-57,8}{4}=1,3 \\;cm^2 \\] # Forma mais simples de calcular a variância amostral var(X) ## [1] 1.3 12.2.4 DESVIO PADRÃO A variância, pela sua natureza, tem a unidade quadrática. A sua raiz quadrada, que ainda é uma medida de dispersão é denominada desvio padrão: A vantagem do desvio padrão é ter a mesma unidade dos dados originais e, consequentemente, da média. É a mais utilizada das medidas de dispersão, e é representada por \\(\\sigma\\) para a população, com estimativa \\(s\\) para a amostra. Então: \\[ \\sigma = \\sqrt{\\sigma^2} \\text{ e } s = \\sqrt{s^2} \\] exemplo \\[ s = \\sqrt{1,3} = 1,140175\\;cm \\] # Forma mais simples de calcular o desvio padrão sd(X) ## [1] 1.140175 12.2.5 ERRO PADRÃO DA MÉDIA Se em vez de uma amostra tivéssemos várias, provenientes de uma mesma população, obteríamos diversas estimativas da média, e provavelmente distintas umas das outras. A partir dessas diversas estimativas da média, poderíamos estimar uma variância, considerandos-e os desvios de cada média individual, em relação à média de todas elas. Seria então uma estimativa da variância das médias. Entretanto, demonstra-se que a partir de uma única amostra, podemos estimar essa variância, através da fórmula: \\[ Var(\\bar{X}) = \\hat{V}(\\hat{m}) = \\frac{s^2}{n} \\] onde \\(s^2\\) é a estimativa da variância dos \\(n\\) dados, calculada de maneira usual A sua raiz quadrada é denominada Erro Padrão da Média, ou seja: \\[ s(\\hat{m}) = \\frac{s}{\\sqrt{n}} \\] O erro padrão da média fornece uma idéia da precisão da estimativa da média, isto é quanto menor ele for, maior precisão terá a estimativa da média. Assim para os dados de altura de plantas daninhas temos: exemplo \\[ s(\\hat{m}) = \\frac{s}{\\sqrt{n}} = \\frac{1,140175}{\\sqrt{5}} = 0,5099\\;cm \\] # Erro padrão da média sd(X)/sqrt(n) ## [1] 0.509902 Sempre que apresentarmos uma média, é conveniente apresentar também o seu erro padrão. Assim, no exemplo poderíamos apresentar a média e o seu erro padrão, da seguinte maneira: \\[ 3,4 \\pm 0,5099\\;cm \\] Quanto menor o valor do erro padrão da média, mais precisa foi a estimativa da média. 12.2.6 COEFICIENTE DE VARIAÇÃO É uma medida de dispersão que expressa percentualmente o desvio padrão por unidade de média, ou seja: \\[ CV = \\frac{100 \\cdot s}{\\hat{m}} \\] Como \\(s\\) e \\(\\hat{m}\\) são expressos na mesma unidade dos dados, o coeficiente de variação é um número abstrato, isto é, não tem unidade e portanto é expresso em porcentagem da média. Nos ensaios agrícolas de campo, esperam-se coeficientes de variação da ordem de 10 a 20%. Porém em ensaios de levantamento de pragas, normalmente os coeficientes de variação são maiores que 30%. No exemplo de altura de plantas daninhas, temos: exemplo \\[ CV = \\frac{100 \\cdot s}{\\hat{m}} = \\frac{100 \\cdot 1,140175}{3,4} = 33,53\\% \\] # Coeficiente de Variação 100*sd(X)/mean(X) ## [1] 33.53457 "],["comparações-de-parâmetros-de-duas-populações.html", "13 Comparações de parâmetros de duas populações 13.1 Comparação de variâncias de duas populações normais 13.2 Comparação de duas médias de populações normais: amostras independentes", " 13 Comparações de parâmetros de duas populações 13.1 Comparação de variâncias de duas populações normais Suponha duas amostras aleatórias independentes de tamanhos \\(n_1\\) e \\(n_2\\) ou seja, \\(X_1,X_2,...,X_{n_1}\\) e \\(Y_1,Y_2,...,Y_{n_2}\\), respectivamente, de uma população com distribuição \\(N(\\mu_1, \\sigma_1^2)\\) e de uma população com distribuição \\(N(\\mu_2, \\sigma_2^2)\\). Hipóteses: \\[ H_0: \\sigma_1^2 = \\sigma_2^2 \\text{ ou } \\left(\\frac{\\sigma_1^2} {\\sigma_2^2} =1\\right) \\\\ H_1: \\sigma_1^2 \\neq \\sigma_2^2 \\text{ ou } \\left(\\frac{\\sigma_1^2} {\\sigma_2^2} \\neq 1\\right) \\] Estatística do Teste Sendo \\(s_1^2\\) e \\(s_2^2\\) as variâncias amostrais, respectivamente, das amostras \\(n_1\\) e \\(n_2\\), o quociente entre as razões \\[ \\frac{s^2_1/\\sigma^2_1}{s^2_2/\\sigma^2_2} \\] Segue uma distribuição F de Snedecor com parâmetros \\(n_1-1\\) e \\(n_2-1\\) graus de liberdade (GL), denotada por \\(F(n_1-1,n_2-1)\\) Sob a suposição de \\(H_0\\) ser verdadeira, isto é, \\(\\sigma_1^2 = \\sigma_2^2\\), tem-se que: \\[ F= \\frac{s^2_1}{s^2_2} \\approx F(n_1-1,n_2-1) \\] Para informações mais detalhada a respeito dessa distribuição, recomenda-se: An Introduction to the F Distribution A função densidade probabilidade da distribuição \\(F\\) é dada por: \\[ f(x) = \\frac{\\Gamma(\\frac{\\nu1+\\nu2}{2})(\\frac{\\nu1}{\\nu2})^{\\frac{\\nu1}{2}}x^{\\frac{\\nu1}{2}-1}}{\\Gamma(\\frac{\\nu1}{2}) \\Gamma(\\frac{\\nu2}{2}) (1+\\frac{\\nu1}{\\nu2}x)^{\\frac{\\nu1+\\nu2}{2}}} \\text{ para } x&gt; 0 \\] Onde: \\(\\nu1\\) são os graus de liberdade de \\(s^2_1\\), \\(\\nu2\\) são os graus de liberdade de \\(s^2_2\\) e \\(\\Gamma\\) é a função gama, uma extensão da função fatorial para números reais positivos no espaço contínuo. Construção da região crítica: Fixado \\(\\alpha\\) (chance de ocorrer o erro tipo I), os pontos críticos serão F1 e F2 da distribuição \\(F\\), tais que: Se \\(\\alpha = 10\\%\\), por exemplo, podemos encontrar o valor de \\(F_{2(5\\%)}\\), utilizando a Tabela abaixo: Download da Tabela Exemplo Dado: \\(n_1-1 = 5\\) e \\(n_2-1 = 7\\), e \\(\\alpha = 5\\%\\) temos que olhar na coluna \\(5\\) (graus de liberdade do numerador) e na linha \\(7\\) (graus de liberdade do denominador) e teremos o valor \\(F_2 = 3,97\\) Para encontrarmos \\(F_{1(5\\%)}\\), utilizamos uma propriedade de integração, o que na prática significa: \\[ F_{(1-\\alpha, n_1-1, n_2-1)} = \\frac{1}{F_{(\\alpha, n_2-1, n_1-1)}} \\\\ \\text{ou seja} \\\\ F_{(95\\%, n_1-1, n_2-1)} = \\frac{1}{F_{(5\\%, n_2-1, n_1-1)}} \\\\ F_{(95\\%, 5, 7)} = \\frac{1}{F_{(5\\%, 7, 5)}} = \\frac{1}{4,88} = 0,205 \\] O valor é o inverso daquele observado na Tabela, obtido pela inversão dos graus de liberdade. Portanto, a região crítica do teste será: \\(RC = \\{ F &lt; 0,205 \\text{ ou } F &gt; 3,97 \\}\\). O mesmo valor pode ser obtido no \\(R\\) a partir da função qf(p,df1,df2), cujos argumentos são: p o valor da área (probabilidade) acumulada até o valor F desejado, df1 é o grau de liberdade do numerador e df2 é o grau de liberdade do denominador: qf(0.95,5,7) # F2 ## [1] 3.971523 qf(0.05,5,7) # F1 ## [1] 0.2050915 Na prática o procedimento é mais simples: primeiro calcula-se o \\(F\\) da amostra utilizando SEMPRE O MAIOR VALOR DE VARIÂNCIA NO NUMERADOR \\((s^2_1 &gt; s_2^2)\\), portanto \\(F &gt; 1\\) e consideramos o ponto crítico sempre em \\(F_{2(\\alpha, n_1-1, n_2-1)}\\) Assim, a região crítica do teste será \\(RC = \\{F \\ge F_c\\}\\) Calculamos a estatística na amostra como : \\[ F_{obs} = \\frac{s^2_1}{s^2_2} \\text{, com } s_1^2 &gt; s^2_2 \\] As hipóteses sempre serão \\[ H_0: \\sigma_1^2 = \\sigma_2^2 \\text{ ou } (\\sigma_1^2 /\\sigma_2^2 =1) \\\\ H_1: \\sigma_1^2 &gt; \\sigma_2^2 \\text{ ou } (\\sigma_1^2 /\\sigma_2^2 &gt; 1) \\] Conclusão Se \\(F_{obs} \\not\\in RC\\) então não rejeitamos \\(H_0\\) e concluímos que as variâncias são iguais ou seja, as variãncias são HOMOCEDÁSTICAS. Se \\(F_{obs} \\in RC\\) então rejeitamos \\(H_0\\) e concluímos que as variâncias são diferentes ou seja, HETEROCEDÁSTICAS EXEMPLO: Em um ensaio de competição de \\(2\\) cultivares de milho, onde a cultivar \\(A\\) foi plantada e \\(6\\) parcelas (\\(n_A=6\\) amostras) e a cultivar \\(B\\) em 8 parcelas amostrais (\\(n_B=8\\) amostras), as produções em \\(kg\\;ha^{-1}\\) foram obervadas na tabela: Fonte: Cruz (2009) Cultivar \\(A\\) 1470 1920 2340 2100 1920 1480 \\(B\\) 3260 3990 4050 3420 3510 3880 3550 3660 Comparar ao nível de 5% de significância (\\(\\alpha = 0,05\\)) se as variâncias desses cultivares são iguais. PASSO 1: Definir \\(H_0\\) e \\(H_1\\). \\[ H_0: \\sigma_1^2 = \\sigma_2^2 \\text{ ou } (\\sigma_1^2 /\\sigma_2^2 =1) \\\\ H_1: \\sigma_1^2 &gt; \\sigma_2^2 \\text{ ou } (\\sigma_1^2 /\\sigma_2^2 &gt; 1) \\] PASSO 2: Calcular a estatística do teste. Definir os graus de liberdade: \\[ n_A = 6 \\\\ n_B = 8 \\] Assim: \\[ \\nu_1 = 6 - 1 = 5 \\\\ \\nu_2 = 8 - 1 = 7 \\\\ \\] Calcular as variâncias amostrais de \\(A\\) e \\(B\\): \\[ s_A^2 = 118176,7 \\\\ s_B^2 = 80200,00 \\] Como \\(s^2_A &gt; s^2_B\\) vamos calcular o a estatística do teste: \\[ F_{obs} = \\frac{s^2_A}{s^2_B} = \\frac{118076,7}{80200}=1,4735 \\] PASSO 3: Construir a Região Crítica. Olhando na tabela \\(F_{(0,05; 5; 7)} = 3,97\\) Assim, a região crítica do teste será \\(RC = \\{F &gt; 3,97\\}\\) PASSO 4: Comparar \\(F_{obs}\\) com o \\(F_c\\). PASSO 5: Concluir o teste. Como \\(F{obs} \\not\\in RC\\) não rejeitamos \\(H_0\\) ao nível de 5% de significância, e concluímos que as variedades de milhos testadas têm variâncias semelhantes, ou seja, são homocedásticas. 13.1.1 Resolvendo no R # Construir os vetores de dados A&lt;- c(1470,1920,2340,2100,1920,1480) B&lt;- c(3260,3990,4050,3420,3510,3880,3550,3660) # Aplicar o teste de variâncias var.test(A,B,conf.level = 0.95) ## ## F test to compare two variances ## ## data: A and B ## F = 1.4735, num df = 5, denom df = 7, p-value = 0.6176 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.2788001 10.0981750 ## sample estimates: ## ratio of variances ## 1.473525 13.1.2 Interpretação no R Observe que a função retorna a estatística do teste \\(F = 1.4735\\) seguido pelos valores de graus de liberdade (\\(df\\), do inglês degrees of freedom) para o numerador (\\(5\\)) e para o denominador (\\(7\\)), e o valor-p (\\(p-value = 0.6176\\)). Passo 1: Identificar o p-valor (\\(p-value\\)) do teste: O \\(p-value\\) indica a probabilidade (área embaixo da curva) após o valor de \\(F_{obs}\\), ou seja, a probabilidade associada à estatística \\(F_{obs}\\) foi de \\(0,6176\\), como apresentado abaixo: PASSO 2: Comparar \\(p-value\\) com o \\(\\alpha\\). Se \\(p-value &gt; \\alpha\\) então não rejeitamos \\(H_0\\) e concluímos que as variâncias são iguais ou seja HOMOCEDÁSTICAS, caso contrário, rejeitamos \\(H_0\\). PASSO 3: Concluir o teste. Para o nosso exemplo, \\(p-value\\) foi maior que o \\(\\alpha\\), ou seja: Como \\(p-value &gt; \\alpha\\) não rejeitamos \\(H_0\\) ao nível de 5% de significância, e concluímos que as variedades de milhos testadas são homocedásticas. 13.2 Comparação de duas médias de populações normais: amostras independentes Com o objetivo de se comparar duas populações ou, sinonimamente, dois tratamentos, examinaremos a situação na qual os dados estão na forma de realizações de amostras aleatórias de tamanhos \\(n_1\\) e \\(n_2\\), selecionadas, respectivamente, das populações \\(1\\) e \\(2\\). Os dados são as medidas das respostas associadas com o seguinte delineamento experimental. Uma coleção de \\(n_1 + n_2\\) elementos são aleatoriamente divididos em \\(2\\) grupos de tamanhos \\(n_1\\) e \\(n_2\\), onde cada membro do primeiro grupo recebe o tratamento \\(1\\) e do segundo, o tratamento \\(2\\). Especificamente, estaremos interessados em fazer inferência sobre o parâmetro: \\(\\mu_1 - \\mu_2\\) (média da população 1) - (média da população 2) Formalmente, suponha duas amostras aleatórias independentes de tamanhos \\(n_1\\) e \\(n_2\\) ou seja, \\(X_1, X_2,...,X_{n_1}\\) e \\(Y_1, Y_2,...,Y_{n_2}\\), respectivamente, de uma população com distribuição \\(N(\\mu_1, \\sigma_1^2)\\) e de uma população com distribuição \\(N(\\mu_2, \\sigma_2^2)\\). Para cada uma dessas populações nós temos os seguintes estimadores \\(\\bar{x}\\), \\(\\bar{y}\\) médias de população \\(1\\) e \\(2\\) respectivamente e \\(s^2_1\\), \\(s^2_2\\) variâncias amostrais para as populações \\(1\\) e \\(2\\), respectivamente. Hipóteses: \\[ \\begin{cases} H_0:\\mu_1=\\mu_2, \\; ou\\;(\\mu_1-\\mu_2 = 0) \\\\ H_1: \\mu_1\\neq\\mu_2, \\; ou\\;(\\mu_1-\\mu_2 \\neq 0),\\; bilateral \\end{cases} \\] ou \\[ H_1: \\mu_1 &gt;\\mu_2, \\; ou\\;(\\mu_1-\\mu_2 &gt; 0),\\; unilateral \\; direita \\] ou \\[ H_1: \\mu_1 &lt; \\mu_2, \\; ou\\;(\\mu_1-\\mu_2 &lt; 0),\\; unilateral\\; esquerda \\] Estatística do Teste: \\[ z = \\frac{(\\bar{x}-\\bar{y})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}} \\approx N(0,1) \\] 13.2.1 Caso 1: variâncias populacionais conhecidas Para testar a hipótese \\(H_0\\) usa-se a estatística anterior. Ao supormos \\(H_0\\) verdadeira então \\(\\mu_1-\\mu_2 = 0\\), portanto a estatística: \\[ z = \\frac{(\\bar{x}-\\bar{y})}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}} \\] Tem distribuição \\(N(0,1)\\), portanto a região crítica do teste é construída a partir da Tabela Normal Padrão Download da Tabela 13.2.2 Caso 2: variâncias populacionais desconhecidas e iguais (Teste t) Preliminarmente, testa-se se as variâncias das duas populações são iguais, homocedásticas, pelo teste F de Snedecor. Caso a hipótese \\(H_0\\) não seja rejeitada, vamos utilizar a estatística estatística \\(t\\) com \\(n_1 + n_2 - 2\\) graus de liberdade: \\[ t = \\frac{(\\bar{x} - \\bar{y})}{S_p \\cdot \\sqrt{\\frac{1}{n_1} +\\frac{1}{n_2}} } \\approx t(n_1+n_2 - 2) \\] \\(Sp\\) é o estimador não viciado do desvio padrão das populações, o qual é calculado por uma média ponderada: \\[ S_P=\\sqrt{\\frac{(n_1 - 1) s^2_1+(n_2-1)s^2_2}{(n_1 - 1)+(n_2-1)}} \\] Portanto a região crítica do teste é construída a partir da Tabela da distribuição t ao nível \\(\\alpha\\) com \\(n_1 + n_2 - 2\\) graus de liberdade. Download da Tabela 13.2.3 Caso 3: variâncias populacionais desconhecidas e desiguais (Teste de Smith-Satterthwaite) Quando a hipótese de igualdade de variâncias for rejeitada, a estatística do teste fica: \\[ t = \\frac{(\\bar{x}-\\bar{y})}{\\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}}: \\] Essa estatística aproxima-se de uma dirtribuição \\(t\\) de Student com o número de graus de liberdade dado por, aproximadamente: \\[ gl&#39;=\\frac{(\\frac{s^2_1}{n_1}+\\frac{s_2^2}{n_2})^2}{\\frac{(\\frac{s^2_1}{n_1})^2}{n_1-1}+\\frac{(\\frac{s^2_2}{n_2})^2}{n_2-1}} \\] Como o número de graus de liberdade assim calculado, geralmente, é não inteiro, recomenda-se aproximá-lo para o inteiro imediatamente anterior a este. EXEMPLO: Para o exemplo anterior, temos Cultivar 1 1470 1920 2340 2100 1920 1480 2 3260 3990 4050 3420 3510 3880 3550 3660 Comparar ao nível de 5% de significância se as Médias desses cultivares são iguais. PASSO 1: Definir \\(H_0\\) e \\(H_1\\). \\[ H_0: \\mu_1 = \\mu_2 \\text{ ou } (\\mu_1 - \\mu_2 =0) \\\\ H_1: \\mu_1 \\neq \\mu_2 \\text{ ou } (\\mu_1 - \\mu_2\\neq 0) \\] PASSO 2: Calcular a estatística do teste para o caso 2 variâncias populacionais desconhecidas e iguais (Teste t). Calcular a média dos cultivares: Cultivar A \\[ Média: \\bar{x} = 1871,667\\;kg\\;ha^{-1}\\\\ Variância: s^2_1 = 118176.7\\;[kg\\;ha^{-1}]^2 \\] Cultivar B \\[ Média: \\bar{x} = 3665\\;kg\\;ha^{-1}\\\\ Variância: s^2_1 = 80200\\;[kg\\;ha^{-1}]^2 \\] Vamos estimar \\(Sp\\): \\[ S_P=\\sqrt{\\frac{(n_1 - 1) s^2_1+(n_2-1) s^2_2}{(n_1 - 1)+(n_2-1)}} =\\sqrt{\\frac{(6 - 1) 118176.7+(8-1) 80200}{(6 - 1)+(8-1)}} = 309,8768\\;kg\\;ha^{-1} \\] sqrt(((6-1)*118176.7+(8-1)*80200)/(6-1+8-1)) ## [1] 309.8768 Vamos calcular a estatística: \\[ t_{obs} = \\frac{(\\bar{x} - \\bar{y})}{S_p \\cdot \\sqrt{\\frac{1}{n_1} +\\frac{1}{n_2}} }=\\frac{(1871,667 - 3665)}{309,8768 \\cdot \\sqrt{\\frac{1}{6} +\\frac{1}{8}} } = -10,7159 \\] (1871.667-3665)/(309.8768*sqrt(1/6+1/8)) ## [1] -10.7159 PASSO 3: Construir a Região Crítica. A região crítica do teste será construída a partir da Tabela da distribuição t ao nível \\(5\\%\\) com $(6 + 8 - 2) = 12 $ graus de liberdade. Olhando na tabela \\(t_{(0,05; 12)} = 2,179\\) Assim, a região crítica do teste será \\(RC = \\{t &lt; -2,179 \\;ou\\;t&gt;2,179\\}\\) PASSO 4: Comparar \\(t_{obs}\\) com o \\(t_c\\). PASSO 5: Concluir o teste. Como \\(t_{obs} \\in RC\\) rejeitamos \\(H_0\\) ao nível de \\(5\\%\\) de significância, e concluímos que as médias dos cultivares de milhos são diferentes, ou seja, a cultivar \\(B\\) apresenta uma média de produtividade maior que a cultivar \\(A\\). # Construir os vetores de dados A&lt;- c(1470,1920,2340,2100,1920,1480) B&lt;- c(3260,3990,4050,3420,3510,3880,3550,3660) # Aplicar o teste de Médias, pelo teste t t.test(A,B, alternative = &quot;t&quot;, # use t para bilateral, &quot;l&quot; para unilateral a esquerda e &quot;g&quot; para unilateral a direita var.equal = TRUE, # TRUE se as variância forem iguais, caso contrário, FALSE conf.level = 0.95) # Nível de confiança do teste ## ## Two Sample t-test ## ## data: A and B ## t = -10.716, df = 12, p-value = 1.688e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2157.963 -1428.703 ## sample estimates: ## mean of x mean of y ## 1871.667 3665.000 Interpretação no R: Observe que a função retorna a estatística do teste \\(t = -10.716\\) seguido pelos valores de graus de liberdade (\\(df\\)) igual a \\(12\\), e o \\(p-value = 1.688e-07\\) que significa \\(0,0000001688\\) valor bem próximo a \\(0\\). O \\(p-value\\) indica a probabilidade (área embaixo da curva) antes do valor de \\(t_{obs}\\), no caso dele ser negativo, a probabilidade associada à estatística \\(t_{obs}\\) foi de \\(0,0000001688\\), como apresentado abaixo. Se o valor de \\(t_{obs}\\) for positivo, 0 \\(p-value\\) indica a probabilidade (área embaixo da curva) depois do valor de \\(t_{obs}\\) PASSO 2: Comparar \\(p-value\\) com o \\(\\alpha\\). Se \\(p-value &gt; \\alpha\\) então não rejeitamos \\(H_0\\) e concluímos que as médias são iguais, caso contrário, rejeitamos \\(H_0\\). PASSO 3: Concluir o teste. Para o nosso exemplo, \\(p-value\\) foi menor que o \\(\\alpha\\), ou seja: Como \\(p-value &lt; \\alpha\\) rejeitamos \\(H_0\\) ao nível de 5% de significância, e concluímos que os cultivares de milhos testadas são apresentam médias diferentes. "],["planejamento-de-experimentos.html", "14 Planejamento de Experimentos 14.1 Introdução 14.2 Unidade Experimental ou Parcela 14.3 Princípios básicos da experimentação 14.4 Relação entre princípios basicos e delineamentos 14.5 Testes de significância", " 14 Planejamento de Experimentos 14.1 Introdução O principal objetivo da Estatística Experimental é o estudo dos experimentos, seu planejamento, execução, análise e interpretação dos resultados obtidos. Os dados que empregamos na análise estatística constituem uma amostra da população em estudo, e são obtidos de trabalhos previamente planejados, que são os EXPERIMENTOS, sendo por isso chamados de dados experimentais. O que nos obriga utilizar a análise estatística é a presença em todos os dados experimentais, de efeitos de fatores não controlados (que podem ou não ser controláveis) e que causam a variação na característica alvo de estudo (variável resposta). Entre os fatores que não podem ser controlados, podemos citar: pequenas variações nas lâminas de molhamento de irrigação; pequenas variações na infestação das parcelas pelas pragas de estudo; variação na constituição genética das plantas; pequenas variações na fertilidade do solo; profundidade de semeadura, entre outros. Esses efeitos, que estão sempre presentes, não podem ser conhecidos individualmente e, no conjunto alteram, pouco ou muito, os resultados obtidos. Os efeitos desses fatores que não podem ser controlados são chamados de variação do acaso ou variação aleatória. Procurando tornar mínima a variação do acaso, o experimentador deve fazer o planejamento do experimento de tal forma que consiga isolar os efeitos de todos os fatores que podem ser controlados. O planejamento constitui a etapa inicial de qualquer trabalho, e, portanto, um experimento também deve ser devidamente planejado, de modo a atender aos interesses do experimentador e às hipóteses básicas necessárias para a validade da análise estatística. Frequentemente o estatístico é consultado para tirar conclusões com base em dados experimentais. Como as conclusões dependem da forma como foi realizado o experimento, o estatístico solicitará uma descrição detalhada do experimento e de seus objetivos. Muitas vezes, ocorrem casos em que, após a descrição do experimento, o estatístico verifica que não pode chegar a conclusão alguma, uma vez que o experimentador não utilizou um delineamento adequado ou não atendeu às hipóteses básica necessárias para a validade da análise estatística. Para evitar essa perda de tempo e de recursos, é primordial o planejamento adequado do experimento. Ao iniciar o planejamento de um experimento, o experimentador deve formular e responder a uma série de perguntas. Como exemplo, podemos citar: 1. Quais as variáveis que serão analisadas? Num mesmo experimento, várias características podem ser estudadas. Por exemplo, num experimento com a cultura do milho, podemos determinar a altura das plantas, a porcentagem de plantas acamadas, a produção de grãos, o peso de 100 grãos, o comprimento da espiga, o diâmetro da espiga, etc. Portanto, devemos definir adequadamente as características (variáveis) de interesse, para que as mesmas possam ser determinadas no decorrer do experimento. 2. Quais os fatores que afetam essas características? Relacionar todos os fatores que possuem efeito sobre as características que serão estudadas, como por exemplo: variedade, adubação, espaçamento, irrigação, tratos culturais, controle de pragas e doenças, etc. 3. Quais desses fatores serão estudados no experimento? Nos experimentos simples, apenas um tipo de tratamento ou fator pode ser estudado de cada vez, sendo os demais fatores mantidos constantes. Por exemplo, quando fazemos um experimento de competição de variedades, todos os outros fatores tais como espaçamento, adubação, irrigação, tratos culturais devem ser os mesmos para todas as variedades. No caso de experimentos mais complexos, como os experimentos fatoriais e em parcelas subdivididas, podemos simultaneamente estudar os efeitos de dois ou mais tipos de fatores. 4. Como será a unidade experimental ou parcela? A escolha da parcela deve ser feita de forma a minimizar o erro experimental. Devido à importância da definição de unidade experimental, faremos uma discussão mais detalhada sobre o assunto em seguida. 5. Quantas repetições deverão ser utilizadas? O número de repetições de um experimento depende do número de tratamentos a serem utilizados e do delineamento experimental escolhido. De modo geral, recomenda-se que o número de parcelas do experimento não seja inferior a \\(20\\) e que o número de graus de liberdade associado aos efeitos dos fatores não controlado ou acaso não seja inferior a \\(10\\). 14.2 Unidade Experimental ou Parcela Um dos aspectos mais importantes a ser considerado durante o planejamento do experimento é a definição de unidade experimental ou parcela. De um modo geral a escolha da parcela deve ser feita de forma a minimizar o erro experimental, isto é, as parcelas devem ser o mais uniforme possível, para que as mesmas consigam refletir o efeito dos tratamentos aplicados. 14.2.1 TAMANHO DA PARCELA Em experimentos de campo, o tamanho das parcelas pode variar bastante, em função dos seguintes fatores: 1. Material com que se está trabalhando: Dependendo da cultura que está sendo estudada, devemos aumentar ou diminuir o tamanho das parcelas. Por exemplo, parcelas da cultura da soja geralmente são menores que as parcelas para a cultura da cana-de-açúcar ou para a cultura da Laranja. 2. O objetivo da pesquisa: O objetivo do trabalho experimental também influencia no tamanho da parcela. Por exemplo, se desejamos estudar o efeito da profundidade de semeadura do sorgo granífero sobre o desenvolvimento inicial das plantas, não necessitamos trabalhar com parcelas tão grandes quanto as que seriam necessárias para um estudo de produção da cultura. 3. Número de tratamentos em estudo: Quando o número de tratamentos é muito grande, como ocorre com os experimentos de melhoramento vegetal, o tamanho das parcelas deve ser reduzido, para diminuir a distância entre as parcelas extremas, visando a homogeneidade entre elas. 4. Quantidade disponível de sementes: Nos experimentos de melhoramento genético vegetal, este é um fator limitante para o tamanho das parcelas. 5. Uso de máquinas agrícolas: Nos experimentos em que é necessária a utilização de máquinas agrícolas, tais como tratores e colhedoras, o tamanho das parcelas deve ser, obrigatoriamente, grande. 6. Área total disponível para a pesquisa: Frequentemente, o pesquisador tem que ajustar seu experimento ao tamanho da área disponível, que em geral é pequeno, o que resulta na utilização de parcelas pequenas. 7. Custo tempo e mão-de-obra: São os principais fatores que limitam o tamanho das parcelas. 14.2.2 FORMA DA PARCELA No que se refere à forma das parcelas, experimentos realizados em diversos países, como diferentes culturas, têm mostrado que, para se obter maior precisão, as parcelas devem ser compridas e estreitas, evitando-se que todas a parcela ocupem uma mancha de alta ou baixa fertilidade do solo, que possa existir na área experimental. Para parcelas de tamanho pequeno, o efeito da forma é muito pequeno. O tamanho e a forma ideais para a parcela são aqueles que resultem em maior homogeneidade das parcelas. Em alguns experimentos, devemos utilizar bordaduras nas parcelas, para se evitar a influência sobre a parcela, dos tratamentos aplicados nas parcelas vizinhas. Neste caso, teremos a área total e a área útil da parcela, sendo que os dados a serem utilizados na análise estatística serão aqueles coletados na área útil da parcela. Nos experimentos em casa-de-vegetação, para a constituição de cada parcela, podemos utilizar um conjunto de vasos ou então, um único vaso com duas ou três plantas e, às vezes, uma única planta constituindo a unidade experimental. Em experimentos de laboratório, uma amostra simples do material poderá constituir a parcela, porém, às vezes é necessário utilizar uma amostra composta. Quando são feitas várias determinações em uma mesma amostra, o valor da parcela será a média das várias determinações. Não devemos confundir as diversas determinações da mesma amostra, com as repetições do experimento. 14.3 Princípios básicos da experimentação Para assegurar que os dados serão obtidos de forma a propiciar uma análise correta e que conduza a conclusões válidas com relação ao problema em estudo, o experimentador deve levar com conta alguns princípios básicos ao planejar o experimento. 14.3.1 PRINCÍPIO DA REPETIÇÃO O princípio da repetição consiste na reprodução do experimento básico. Sejam, por exemplo as variedade, \\(A\\) e \\(B\\), plantadas em 2 parcelas o mais semelhante possível. O fato da variedade \\(A\\) se comportar melhor que a \\(B\\), pouco ou nada significa, pois a variedade \\(A\\) pode ter tido um melhor comportamento por simples acaso. Podemos tentar contornar o problema, plantando as variedades \\(A\\) e \\(B\\) em diversas parcelas, e considerando o comportamento médio de cada variedade. Aqui intervém o princípio da repetição, ou seja, a reprodução do experimento básico. Entretanto, apenas esse princípio não resolve totalmente o problema, pois se todas as parcelas com a variedade \\(A\\) estiverem agrupadas, e aquelas com a variedade \\(B\\) também, o efeito de fatores não controlados continuará a ser uma hipótese possível para o melhor comportamento da variedade \\(A\\). 14.3.2 PRINCÍPIO DA CASUALIZAÇÃO Princípio da casualização consiste na distribuição dos tratamentos às parcelas de forma casual, para evitar que um determinado tratamento venha a ser beneficiado por sucessivas repetições em parcelas melhores. Se, por exemplo, temos as duas variedades \\(A\\) e \\(B\\) distribuídas ao acaso em \\(6\\) parcelas cada, teremos: Então, se a variedade \\(A\\) se comportar melhor que a \\(B\\) em qualquer das parcelas, pela teoria de probabilidades, a probabilidade de que isso ocorra por acaso é: \\[ p = \\frac{6! \\cdot 6!}{12!} = \\frac{1}{924} = 0,1\\% \\Rightarrow q=1-p=99,9\\% \\] Isso significa que o resultado obtido ainda pode ser devido ao acaso, porém a probabilidade de que isso ocorra por acaso é apenas de \\(0,1\\%\\), ou seja, existe uma probabilidade de \\(99,9\\%\\) de que haja realmente um melhor comportamento de um dos tratamentos. 14.3.3 PRINCÍPIO DO CONTROLE LOCAL Este princípio é frequentemente utilizado, mas não é de uso obrigatório. A função do controle local é tornar o delineamento mais eficiente, reduzindo o erro experimental. O controle local consiste na formação de grupos de parcelas o mais homogêneos possível, de modo a reduzir o erro experimental. Cada grupo constitui um bloco, sendo que os tratamentos devem ser sorteados dentro de cada bloco. Por exemplo: 14.4 Relação entre princípios basicos e delineamentos A análise de variância consiste na decomposição da variância total de um material heterogêneo em partes atribuídas a causas conhecidas e independentes e a uma porção residual de origem desconhecida e de natureza aleatória. Quando planejamos um experimento, levando em conta apenas o princípio da repetição e da casualização, sem considerar o princípio do controle local, temos o Delineamento Inteiramente Casualizado (DIC) ou Delineamento Inteiramente ao Acaso. Só devemos utilizar esse delineamento, quando temos certeza da homogeneidade das condições experimentais. É frequentemente utilizado em experimentos de laboratório onde as condições experimentais podem ser perfeitamente controladas. Num experimento inteiramente Casualizado, com \\(5\\) tratamentos, cada um dos quais foi repetido \\(5\\) vezes, teremos o seguinte esquema de análise de variância: Causas de Variação G.L. Tratamentos 4 Resíduo 20 Total 24 O resíduo ou erro, é a causa de variação que reflete o efeito dos fatores não controlados, também chamado de acaso. Quando não há homogeneidade entre parcelas, devemos utilizar o princípio do controle local, estabelecendo blocos. Neste caso, o delineamento a ser utilizado é o Delineamento de Blocos ao Acaso (DBC). O esquema de análise de variância de um experimento em blocos causalizados com 5 tratamento de 5 repetições é dado por: Causas de Variação G.L. Tratamentos 4 Blocos 4 Resíduo 16 Total 24 Quando necessitamos controlar 2 tipos de heterogeneidade, devemos utilizar o Delineamento em Quadrados Latino (DQL). Neste delineamento, os tratamentos sofrem um duplo controle local, sendo dispostos em linhas e colunas. Para um experimento em quadrado latino com \\(5\\) tratamento, o esquema de análise de variância será: Causas de Variação G.L. Tratamentos 4 Linhas 4 Colunas 4 Resíduo 12 Total 24 14.5 Testes de significância 14.5.1 Introdução Um dos principais objetivos da estatística é a tomada de decisão a respeito da população, com base nas observações de amostra, ou seja, a obtenção de conclusões válidas para toda a população com base em amostra retiradas dessas populações. Ao tentarmos tomar decisões, é conveniente a formulação de hipóteses ou suposições relativas às populações. Essas suposições, que podem ou não ser verdadeiras são chamadas de hipóteses estatísticas e consistem, geralmente, em considerações a respeito das distribuições de probabilidade das populações. Em muitos casos formulamos uma hipótese estatística com o objetivo de rejeitá-la ou invalidá-la. Por exemplo, quando realizamos um experimento com o objetivo de verificar qual é a variedade de cana-de-açúcar mais produtiva, formulamos a hipótese de que não existem diferenças entre as variedades em relação à produção (isto é, que quaisquer diferenças observadas são devidas unicamente aos fatores não controlado ou acaso). Essa hipótese inicial que formulamos, é denominada de **hipótese da nulidade* e é representada por \\(H_0\\)*. Admitindo-se esta hipótese como verdadeira, se verificarmos que os resultados obtidos ao final do experimento em uma amostra aleatória diferem acentuadamente dos resultados esperados para essa hipótese, com ase na teoria das probabilidades, podemos concluir que as diferenças observadas são significativas, e rejeitar essa hipótese \\(H_0\\). Então, rejeitamos a hipótese da nulidade em favor de uma outra, que é representada por \\(H_1\\) e denominada de hipótese alternativa. Por exemplo, no caso da comparação entre variedade, a hipótese alternativa seria: As variedades testadas se comportam de maneira diferente em relação à produção de cana-de-açúcar. Os métodos que nos permitem decidir se aceitamos ou rejeitamos uma determinada hipótese, ou se a amostra observada difere significativamente dos valores esperados, são denominados testes de significância ou testes de hipóteses. Porém, ao tomarmos decisões de rejeitar ou aceitar uma determinada hipótese, estamos sujeitos a incorrer em dois tipos de erros: ERRO TIPO I: é o erro que cometemos ao rejeitar uma determinada hipótese verdadeira, que deveria ser aceita. ERRO TIPO II: é o erro que cometemos ao aceitar uma hipótese falsa, que deveria ser rejeitada. Esses dois tipos de erros são associados de tal forma que à medida que diminuímos a probabilidade de ocorrência de um deles, automaticamente aumentamos a probabilidade de ocorrência de outro. Geralmente, em estatística controla apenas o erro Tipo I, por meio do nível de significância do teste. O nível de significância do teste, representado por \\(\\alpha\\) é a probabilidade máxima com que nos sujeitamos a correr o risco de cometer o erro Tipo I, ao testarmos uma hipótese. Na prática é usual fixarmos esse nível de significância em 5% ou em 1%, ou seja \\(\\alpha = 0,05\\) ou \\(\\alpha = 0,01\\). Então, se por exemplo, escolhermos o nível de significância de 5%\\((\\alpha=0,05)\\), isto indica que temos 5 chances em 100 de rejeitarmos uma hipótese que deveria ser aceita, isto é, há uma confiança de 95% de que tenhamos tomado uma decisão correta. Esta confiança que temos de termos tomada uma decisão correta é denominada de Grau de Confiança do Teste, e é dada por \\(100 \\cdot (1-\\alpha) \\%\\). O teste de significância mais utilizado em estatística experimental é o Teste F, que estudaremos a seguir. 14.5.2 Teste F de Snedecor para análise de variância A Análise de Variância é uma técnica que nos permite fazer a decomposição da variância total em parte atribuídas a causas conhecidas e independentes e uma porção residual de origem desconhecida e de natureza aleatória. O teste F tem por finalidade comparar estimativas de variâncias. Na análise de variância, as estimativas de variância são dadas pelos quadrados médios (Q.M.) e obtemos um Q.M. para cada causa de variação. Assim, em um experimento inteiramente casualizado, temos duas estimativas de variância: uma devido aos efeitos de tratamentos (dadas pelo QM Tratamentos) e outra devida aos efeitos dos fatores não controlados ou acaso (dada pelo QM Resíduo). Para aplicar o teste F na análise de variância, utilizamos sempre no denominador, o QM Resíduo, ou seja, comparamos sempre uma variância devida aos efeitos do fator controlado (Tratamentos, Blocos, Linhas, Colunas, etc.), com a variância devida aos efeitos dos fatores não controlados ou acaso (Resíduos) Então: \\[ F= \\frac{QM_{Tratamentos} }{ QM_{Resíduos}} \\] Sob a hipótese da nulidade, isto é, supondo-se que os efeitos dos tratamentos são todos equivalentes, teríamos duas estimativas de variância (QM Tratamentos e QM Resíduo) que não deveriam diferir, a não ser por flutuações amostrais, pois ambas estimam a variação do acaso. Assim, \\(QM_{Resíduo}\\) - estima a variação do acaso: \\(\\sigma^2\\). \\(QM_{Tratamentos}\\) - estima a variação do acaso mais a variação devida ao efeito de tratamentos: \\(\\sigma^2 + K \\sigma^2_T\\) Portanto, \\[ F=\\frac{QM_{Tratamentos}}{QM_{Resíduo} } = \\frac{\\sigma^2+K\\sigma^2_t}{\\sigma^2} \\] A seguir, comparamos o valor de F calculado com os valores da tabela de distribuição F (geralmente aos níveis de 5% e 1%). Os valores críticos são obtidos na tabela da distribuição F, em função do número de graus de liberdade de tratamentos (ou blocos), na horizontal (numerador) e do número de graus de liberdade do resíduo, na vertical (denominador). DOWNLOAD das tabelas O critério do teste é o seguinte: Se \\(F\\;calculado \\ge F\\;tabelado\\) o teste é significativo ao nível testado. Então, devemos rejeitar a hipótese da nulidade (\\(H_0\\)), e concluir que os efeitos dos tratamentos diferem entre si a esse nível de probabilidade, e essas diferenças não devem ser atribuídas ao acaso, mas sim aos efeitos dos tratamentos testatos, com um grau de confiância \\(100 \\cdot(1-\\alpha)\\%\\). Se \\(F\\;calculado \\le F\\;tabelado\\) o teste é não significativo ao nível testado. Então, não devemos rejeitar a hipótese da nulidade (\\(H_0\\)). Neste caso concluímos que os efeitos dos tratamentos não diferem entre si a esse nível de probabilidae. Abaixo segue o esquema da distribuição F. Resumidamente, temos: Fcalc &lt; Ftab(5%) - O teste F é não significativo ao nível de \\(5\\%\\) de probabilidade. Aceitamos \\(H_0\\) - Utiliza-se a notação \\(Fcalc^{NS}\\). Ftab(5%) &lt; Fcalc &lt; Ftab(1%) - O teste é significativo ao nível de \\(5\\%\\) de probabilidade. Rejeitamos \\(H_0\\) com um grau de confiança superior a \\(95\\%\\). Utiliza-se a notação: \\(Fcalc^*\\). Fcalc &gt; Ftab(1%) - O teste é significativo ao nível de \\(1\\%\\)de probabilidade. Rejeitamos \\(H_0\\) com um grau de confiança superior a \\(99\\%\\). Utiliza-se a notação: \\(Fcalc^{**}\\). 14.5.3 Exemplo de aplicação do teste F Num experimento de competição de cultivares de cana-de-açúcar foram utilizados \\(6\\) Tratamentos e \\(4\\) repetições. As cultivares testadas foram: Co 413 Co 419 CB 40/19 CB 40/69 CB 41/70 CB 41/76 O delineamento experimental utilizado foi em blocos casualizados, com os blocos controlando diferenças na fertilidade do solo entre terraços. Croqui experimental localizada no município de Apareceida do Taboado (MS). Croqui do experimento mostrando a disposição dos blocos na área experimental. Aspectos gerais da uma parcela experimental. Fonte: Arquivo pessoal. Os dados experimentais, de produção da cultura foram tabulados em planilha eletrônica da seguinte forma. dados_prod_cana.xlsx Observe que os dados estão na forma retanrgular, também denominada frame de dados. Nessa forma cada coluna representa um fator ou característica encontrada no experimento e fornece as informações de cada parcela experimental (linhas da tabela). Note que a primeira linha da tabela é destinada para informar o nome de cada coluna (cabeçalho) Os dados poderiam ser apresentados em arquivos tipo txt exemplo dados_prod_cana.txt Para a produção da cultura da cana-de-açúcar, em t/ha foram estimados os seguintes valores de soma de quadrados para a análise de variância. \\(SQ{Trat} =10471,21\\) \\(SQ{Bloco} =1424,792\\) \\(SQ{Total} =12097,96\\) As hipóteses que desejamos testar, para Tratamentos são: \\(H_0\\): As cultivares de cana-de-açúcar testadas não diferem entre si quanto à produção de cana-de-açúcar. \\(H_1\\): As cultivares de cana-de-açúcar testadas possuem efeitos diferentes quanto à produção de cana-de-açúcar. As hipóteses para Blocos são: \\(H_0\\): Os blocos instalados em campo não diferem entre si quanto à produção de cana-de-açúcar. \\(H_1\\): Os blocos possuem efeitos diferentes quanto à produção de cana-de-açúcar, isso implica em dizer que os blocos foram eficiente no controle local. Para testar estas hipóteses, podemos montar o seguinte quadro de análise de variância: Para testar estas hipóteses, podemos montar o seguinte quadro de análise de variância: Causas de Variação GL SQ QM F Tratamentos 5 10471,21 2094,24 155,55** Blocos 3 1424,79 474,93 35,27** Resíduo 15 201,96 13,46 Total 23 12097,96 Valores de \\(F\\) da Tabela: Para Tratamentos \\((5 \\times 15\\; gl): \\begin{cases} 5\\%=2,90 \\\\ 1\\% = 4,56\\end{cases}\\) Para Blocos \\((3 \\times 15\\; gl): \\begin{cases} 5\\%=3,29 \\\\ 1\\% = 5,42\\end{cases}\\) Conclusão para Tratamento: O teste foi significativo ao nível de 1% de probabilidade. Rejeitamos a hipótese \\(H_{0}\\) e concluímos que as cultivares (pelo menos 2) testadas possuem efeitos diferentes quanto à produção de cana-de-açúcar, a esse nível de probabilidade, com um grau de confiança superior a 99% de probabilidade. Conclusão para Blocos: O teste foi significativo ao nível de 1% de probabilidade. Rejeitamos a hipótese \\(H_{0}\\) e concluímos que os terraços utilizados como blocos (pelo menos 2) diferem entre si em relação à produção de cana-de-açúcar, a esse nível de probabilidade, com um grau de confiança superior a 99% de probabilidade. 14.5.4 Passos para fazer a análise de variância no R 14.5.4.1 Carregar o Pacote {ExpDes.pt} library(ExpDes.pt) ## ## Attaching package: &#39;ExpDes.pt&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## ginv 14.5.4.2 Carregar o banco de dados. Neste caso, podemos ler diretamente da internete, uma vez que possuimos o endereço do banco de dados. definimos o endereço na url; utilizamos a função read.table() para ler o banco de dados e salvar no objeto dados. Obseve que utilizamos o argumento h=TRUE, indicando que a primeira linha do banco de dados possui o cabeçalho. # Definindo o caminho url &lt;- &quot;https://raw.githubusercontent.com/arpanosso/experimentacao-agricola-unesp-fcav/master/data/dados_prod_cana.txt&quot; # Entrando com os dados dados &lt;- read.table(url, h= TRUE) # Checando o 6 primeiros registros do banco de dados head(dados) ## Tratamento Bloco Producao ## 1 Co_413 1 88 ## 2 CB_4069 1 55 ## 3 CB_4019 1 39 ## 4 Co_419 1 81 ## 5 CB_4170 1 33 ## 6 CB_4176 1 56 14.5.4.3 Vamos utilizar o operador de acesso $ para extrair as colunas do objeto dados. Observe que para os fatores Tratamento e Bloco, utilizamos a função as.factor() que define o tipo de dados como categórico, necessário para a aplicação da análise de variância. trat &lt;- as.factor(dados$Tratamento) bloco &lt;- as.factor(dados$Bloco) resp &lt;- dados$Producao 14.5.4.4 Utilizar a função do pacote {ExpDes.pt}, respeitando o delineamento experimental, no caso; Delineamento em Blocos ao Acaso (DBC) dbc(trat, bloco, resp) ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Tratamento 5 10471.2 2094.24 155.545 0.0000e+00 ## Bloco 3 1424.8 474.93 35.274 4.8993e-07 ## Residuo 15 202.0 13.46 ## Total 23 12098.0 ## ------------------------------------------------------------------------ ## CV = 6.99 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos ## valor-p: 0.9217893 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## Teste de homogeneidade de variancia ## valor-p: 0.7977011 ## De acordo com o teste de oneillmathews a 5% de significancia, as variancias podem ser consideradas homogeneas. ## ------------------------------------------------------------------------ ## ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a Co_413 81.5 ## a Co_419 77.75 ## b CB_4176 50.5 ## b CB_4069 47 ## c CB_4170 29.5 ## c CB_4019 28.5 ## ------------------------------------------------------------------------ "],["delineamento-inteiramente-casualizado.html", "15 Delineamento Inteiramente Casualizado 15.1 Caracterização 15.2 Modelo matemático 15.3 Hipóteses básicas para análise de variância 15.4 Obtenção da análise de variância 15.5 Delinemaneto inteiramente casualizado com número diferente de repetições por tratamento", " 15 Delineamento Inteiramente Casualizado 15.1 Caracterização O delineamento inteiramente casualizado é o mais simples de todos os delineamentos experimentais, e os experimentos instalados de acordo com este delineamento são chamados de experimentos interiamente casualizados ou experimentos inteiramente ao acaso. Este delineamento apresenta as seguintes características: Leva em consideração os princípios da repetição e da casualização, deixando de lado o princípio do controle local e, portanto, as repetições não são organizadas em blocos. Os tratamentos são designados às parcelas de forma inteiramente casual, com qualquer número de repetições. As principais vantagens desse delineamento são as seguintes: Flexibilidade, uma vez que o número de repetições pode variar de um tratamento para outro, sem causar sérios problemas na análise. Proporciona o maior número de graus de liberdade possível para o resíduo. As principais desvantagens desse delineamento são: As parcelas experimentais devem ser homogêneas. Leva a uma alta estimativa da variância residual \\(QM_{Res}\\), uma vez que todas as variações, exceto aquela devido ao efeito de tratamentos, são tomadas como variação do acaso. Para a instalação desse experimento devemos ter certeza da homogeneidade das condições experimentais. Este delineamento é bastante utilizado em ensaios de laboratório e em ensaios com vasos, realizados dentro de casas de vegetação, em que as condições experimentais podem ser perfeitamente controladas. A distribuição casual dos tratamentos a todas as parcelas do experimento é a principal característica deste delineamento. Por exemplo, num experimento no delineamento inteiramente casualizado com \\(I = 5\\) tratamentos (A, B, C, D e E) e \\(J = 4\\) repetições (1 a 4), a casualização dos tratamentos seria feita sorteando-se para cada uma das \\((5 \\times 4 = 20)\\) parcelas do experimento uma combinação de tratamento e repetição, ou seja: \\[ \\begin{matrix} A1 &amp; A2 &amp; A3 &amp; A4 \\\\ B1 &amp; B2 &amp; B3 &amp; B4 \\\\ C1 &amp; C2 &amp; C3 &amp; C4 \\\\ D1 &amp; D2 &amp; D3 &amp; D4 \\\\ E1 &amp; E2 &amp; E3 &amp; E4 \\end{matrix} \\] Assim, um sorteio para distribuição dos tratamentos às parcelas poderia ser o seguinte: # Definindo o número de tratamentos e repetições I &lt;- 5 J &lt;- 4 # Criando os tratamentos a partir da letras do conjunto LETTERS já definidos no R tratamentos &lt;- rep(LETTERS[1:I], J) # Criando as repetições repeticoes &lt;- rep(1:J, rep(I,J)) # Criando os identificadores (nomes) das parcelas parcelas &lt;- paste0(tratamentos,repeticoes) # definindo a semente aleatória e sorteio das parcelas, sem reposição set.seed(1235) sample(parcelas, size = 20, replace = FALSE) ## [1] &quot;E3&quot; &quot;B3&quot; &quot;C1&quot; &quot;E4&quot; &quot;E1&quot; &quot;D3&quot; &quot;D1&quot; &quot;D4&quot; &quot;C3&quot; &quot;C4&quot; &quot;A4&quot; &quot;B2&quot; &quot;B1&quot; &quot;B4&quot; &quot;A2&quot; ## [16] &quot;E2&quot; &quot;D2&quot; &quot;C2&quot; &quot;A1&quot; &quot;A3&quot; \\[ \\begin{matrix} E3 &amp; B3 &amp; C1 &amp; E4 &amp; E1\\\\ D3 &amp; D1 &amp; D4 &amp; C3 &amp; C4\\\\ A4 &amp; B2 &amp; B1 &amp; B4 &amp; A2\\\\ E2 &amp; D2 &amp; C2 &amp; A1 &amp; A3 \\end{matrix} \\] 15.2 Modelo matemático Todo delineamento experimental possui um modelo matemático que o representa, e deve ser levado em conta na análise de variância, aceitando algumas hipóteses básicas necessárias para a validade da análise. No caso do \\(DIC\\), o modelo matemático é definido como: \\[ y_{ij}= \\mu + \\tau_i + \\epsilon_{ij} \\] onde: \\(y{ij}\\): é o valor observado na parcela que recebeu o tratamento i na repetição j; \\(\\mu\\): é a média geral do experimento; \\(\\tau_i\\) é o efeito devido ao tratamento i que foi aplicado à parcela; \\(\\epsilon_{ij}\\) é o efeito dos fatores não controlados na parcela que recebeu o tratamento i na repetição j. 15.3 Hipóteses básicas para análise de variância As hipóteses básicas que devemos admitir para tornar válida a aplicação da análise de variância são as seguintes: Aditividade: Os efeitos dos fatores do modelo são aditivos. Independência: Os erros (desvios) \\(\\epsilon_{ij}\\), devido aos efeitos de fatores não controlados ou acaso devem ser independentes. Homocedasticidade (homogeneidade de variâncias): Os erros (desvios) \\(\\epsilon_{ij}\\), devido ao acaso devem possuir variância comum \\(\\sigma^2\\) Normalidade dos desvios: Os erros ou desvios \\(\\epsilon_{ij}\\) devido ao acaso devem possuir distribuição normal de probabilidades. Uma forma resumida de apresentar estas quatro pressuposições é apresentada a seguir: \\[ \\epsilon_{ij} \\overset{iid}{\\tilde{} }N(0,\\sigma^2) \\\\ \\] Quando as hipóteses básicas não são satisfeitas, como no caso mais frequente a homocedasticidade, deve-se utilizar uma transformação dos dados para contormar o problema. 1. Raiz Quadrada: \\(y&#39;=\\sqrt{y}\\) Geralmente utilizada para dados de contagem, que frequentemente seguem a distribuição de Poisson, em que a média e a variância são iguais. Exemplo, número de ácaros por folha, número de frutos atacados, número de plantas doentes por parcela, etc. No caso de ocorrência de valores nulos ou baixos, recomenda-se \\(y&#39;=\\sqrt{y+0,5}\\) ou \\(y&#39;=\\sqrt{y+1}\\). 2. Arco Seno: \\(y&#39;=arcoseno\\sqrt{\\frac{y}{100}}\\) Recomendada para dados de porcentagem, proveniente de contagem, que geralmente seguem a distribuição binomial, como exemplo, \\(\\%\\) de plantas atacadas, \\(\\%\\) de germinação, etc. 3. Transformação Logarítmica: \\(y&#39;=log{(y)}\\) Quando é constatada uma certa proporcionalidade entre as médias e os desvios padrões dos tratamentos. Exemplo: No caso de contagem de insetos, se a população é numerosa, as contagens serão altas para a testemunha e para os tratamentos que não controlam a praga, enquanto que, para os tratamentos que controlam a praga, a amplitude de variação será baixa. Quando houver zeros entre as contagens, utiliza-se \\(y&#39;=log(y+1)\\). 15.4 Obtenção da análise de variância Para a obtenção da análise de variância, vamos considerar um experimento inteiramente casualizado com I tratamentos e J repetições. Trat \\ Repetição \\(1\\) \\(2\\)  \\(j\\)  \\(J\\) Total \\(1\\) \\(y_{11}\\) \\(y_{12}\\)  \\(y_{1j}\\)  \\(y_{1J}\\) \\(T_1 = \\sum_{j=1}^J y_{1j}\\) \\(2\\) \\(y_{21}\\) \\(y_{22}\\)  \\(y_{2j}\\) \\(y_{2J}\\) \\(T_2 = \\sum_{j=1}^J y_{2j}\\)         \\(i\\) \\(y_{i1}\\) \\(y_{i2}\\)  \\(y_{ij}\\)  \\(y_{iJ}\\) \\(T_i = \\sum_{j=1}^J y_{ij}\\)         \\(I\\) \\(y_{I1}\\) \\(y_{I2}\\)  \\(y_{Ij}\\)  \\(y_{IJ}\\) \\(T_I = \\sum_{j=1}^J y_{Ij}\\) Total \\(G = \\sum_{i=1}^I\\sum_{j=1}^J y_{ij}\\) De acordo com o modelo matemático deste delienamento, o valor observado na parcela que recebeu o tratamento i na repetição j é representado por: \\[ y_{ij}= \\mu + \\tau_i + \\epsilon_{ij} \\] Porém, os parâmetros do modelo são desconhecido. Assim, devemos incialmente, obter estimativas desses parâmetros. O método utilizado para obtenção das estimativas desses parâmetros é chamado de Método dos Mínimos Quadrados, e consiste em obter as estimativas que minimizam a soma dos quadrados dos erros \\(\\epsilon_{ij}\\) Utilizamos o Métodos dos Mínimos Quadrados, para minimizar a soma dos quadrados dos erros \\(\\epsilon_{ij}\\): \\[ \\epsilon_{ij}= y_{ij} - \\mu - \\tau_i \\] \\[ \\epsilon_{ij}^2= (y_{ij} - \\mu - \\tau_i)^2 \\] \\[ \\sum_{i=1}^I\\sum_{j=1}^J\\epsilon_{ij}^2=\\sum_{i=1}^I\\sum_{j=1}^J( y_{ij} - \\mu - \\tau_i)^2 \\] Fazendo \\(z=\\sum_{i=1}^I\\sum_{j=i}^j\\epsilon_{ij}^2\\), derivando em relação a \\(\\mu\\) e em relação \\(\\tau_i\\) e igualando as equações a \\(0\\), temos: \\[ \\begin{cases} \\sum_{i=1}^I\\sum_{j=1}^Jy_{ij} - IJ\\hat{m} -J\\sum_{i=1}^I \\hat{\\tau_i} =0 \\\\ \\sum_{j=1}^Jy_{ij}-J\\hat{m}-J\\hat{\\tau_i}=0 \\end{cases} \\] Impondo a restrição \\(\\sum_{i=1}^I \\hat{\\tau_i}=0\\), temos: \\[ \\begin{cases} \\sum_{i=1}^I\\sum_{j=i}^Jy_{ij} - IJ\\hat{m} =0 \\\\ \\sum_{j=1}^Jy_{ij}-J\\hat{m}-J\\hat{\\tau_i}=0 \\end{cases} \\] Então: \\[ \\hat{m} = \\frac{\\sum_{i=1}^I\\sum_{j=i}^Jy_{ij}}{IJ}=\\frac{G}{IJ} \\] e, fazendo \\(T_i=\\sum_{j=1}^Jy_{ij}\\), sendo o total para o tratamento específico i: \\[ \\hat{\\tau_i} =\\frac{T_i}{J} - \\hat{m} \\] Podemos agora obter as somas de quadrados: 1. Soma de Quadrados Total: \\(SQ_{Total}\\) - é definida como a soma dos quadrados dos desvios em relação à média aritmética: \\[ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^J(y_{ij} - \\hat{m})^2 \\\\ \\] Manipulando algebricamente, temos: \\[ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jy_{ij}^2 - \\frac{G^2}{IJ} \\] Ao fazermos \\(C = \\frac{G^2}{IJ}\\) temos portanto: \\[ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jy_{ij}^2 - C \\] 2. Soma de Quadrados de Tratamentos: \\(SQ_{Tratamento}\\) - é definida como a soma dos quadrados dos efeitos dos tratamentos: \\[ SQ_{Tratamento} = \\frac{\\sum_{i=1}^IT_i^2}{J} - C \\] Lembrando que: \\[ T_i^2 = \\left ( \\sum_{j=1}^J{y_{ij}} \\right)^2 \\] e \\[ C = \\frac{G^2}{IJ} \\] 3. Soma de Quadrados do resíduo: \\(SQ_{Resíduo}\\) - é definida como a soma dos quadrados do efeito do acaso. Lembrando que no delineamento inteiramente casualizado a variância total é dividida em duas partes, uma devido ao efeito dos tratamentos e outra devido ao efeito dos fatores não controlados ou acaso (Resíduo), então, a soma de quadrados do resíduo pode ser obtida por diferença, ou seja: \\[ SQ_{Resíduo} = SQ_{Total} - SQ_{Tratamento} \\] Podemos, a seguir, montar o seguinte quadro de análise de variância: Causas (ou Fontes) de Variação \\(GL\\) \\(SQ\\) \\(QM\\) \\(F\\) Tratamento \\(I-1\\) \\(SQ_{Tratamento}\\) \\(SQ_{Trat}/(I-1)\\) \\(QM_{Trat}\\)/\\(QM_{Resíduo}\\) Resíduo \\(I(J-1)\\) \\(SQ_{Res}\\) \\(SQ_{Res}/[I(J-1)]\\) Total \\(IJ-1\\) \\(SQ_{Total}\\) As hipóteses testadas são: \\[ \\begin{cases} H_0: \\tau_i = 0, i=1,2,...,I. \\\\ H_1: \\text{pelo menos um valor } \\tau_i \\neq 0 \\end{cases} \\] CRITÉRIO DO TESTE: Comparamos o valor \\(F\\) calculado para tratamentos com o valor de \\(F\\) tabelado em função do número de graus de liberdade (GL) de tratamentos e do resíduo, ao nível \\(\\alpha\\) de significância. Se \\(F_{Trat} &gt; F_{Tab}\\), concluímos que o teste é significativo, portanto, rejeitamos \\(H_0\\) e devemos concluir que existe diferença significativa entre os efeitos dos tratamentos testados em relação às variáveis (característica) em estudo. 15.4.1 EXEMPLO DE APLICAÇÃO Num experimento inteiramente casualizado, de competição de variedades de mandioca, realizado numa área perfeitamente homogênea quanto às condições experimentais, foram utilizadas \\(5\\) repetições das seguintes variedades: 1 - IAC 5 2 - IAC 7 3 - IAC 11 4 - IRACEMA 5 - MANTIQUEIRA A designação dos tratamentos às parcelas no campo, juntamente com as produções, expressa em t/ha, foi a seguinte: Dados originais:DOWNLOAD Com estes dados, podemos organizar o quadro seguinte: Tratamentos Rep.1 Rep.2 Rep.3 Rep.4 Rep. 5 Total 1 - IAC_5 38,9 25,4 20,3 25,7 29,3 139,6 2 - IAC_7 20,9 26,2 32,3 28,3 28,7 136,4 3 - IAC_11 28,1 27,0 25,8 26,9 22,3 130,1 4 - IRACEMA 38,7 43,2 41,7 39,0 40,3 202,9 5 - MANTIQUEIRA 47,8 47,8 44,7 50,5 56,4 247,2 Total 856,2 As hipóteses que desejamos testar são: \\(H_0\\): As variedades de mandioca testadas não são diferentes entre si quanto à produção. \\(H_1\\): As variedade de mandioca testadas diferem entre si quanto à produção. Cálculo das Somas de Quadrados a) Soma de Quadrados Total: \\[ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jy_{ij}^2 - C \\\\ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jy_{ij}^2 - \\frac{G^2}{IJ} \\\\ SQ_{Total} = (38,9^2+25,4^2+...+56,4^2) - \\frac{856,2^2}{5\\cdot5}\\\\ SQ_{Total} = 2509,46 \\] b) Soma de Quadrados devido ao efeito de Tratamentos: \\[ SQ_{Trat} = \\frac{\\sum_{i=1}^IT_i^2}{J} - C \\\\ SQ_{Trat} = \\frac{T_1^2+T_2^2+...+T_I^2}{J} - \\frac{G^2}{IJ}\\\\ SQ_{Trat} = \\frac{139,6^2+136,4^2+...+247,2^2}{5} - \\frac{856,2^2}{5\\cdot5}\\\\ SQ_{Trat}= 2135,94 \\] Soma de Quadrados do Resíduo: \\[ SQ_{Res} = SQ_{Total} - SQ_{Trat} \\\\ SQ_{Res} = 2509,46 - 2135,94 = 373,52 \\] Então, podemos montar o seguinte quadro de análise de variância: Causas de Variação GL SQ QM F Tratamento \\(4\\) \\(2135,94\\) \\(533,99\\) \\(28,59\\)** Resíduo \\(20\\) \\(373,52\\) \\(18,68\\) - Total \\(24\\) \\(2509,46\\) - - Valores de \\(F\\) da tabela para \\((4 \\times 20\\;GL)\\):\\(\\begin{cases} 5\\%=2,87 \\\\ 1\\%=4,43 \\end{cases}\\) Tabela F a 5% Tabela F a 1% Conclusão: O teste \\(F\\) foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que as variedades diferem entre si em relação à produção de mandioca, com um grau de confiança de \\(99\\%\\) de probabilidade. Conclusões específicas sobre o comportamento das variedades, devemos utilizar um teste de comparação de médias (comparações múltiplas). a) Cálculo das médias de cada tratamento e erros padrões das médias: \\[ \\hat{m_1} = \\frac{T_1}{J} = \\frac{139,6}{5} = 27,9\\;t/ha\\\\ \\hat{m_2} = \\frac{T_2}{J}= \\frac{136,4}{5} = 27,3\\;t/ha\\\\ \\hat{m_3} = \\frac{T_3}{J}= \\frac{130,1}{5} = 26,0\\;t/ha\\\\ \\hat{m_4} = \\frac{T_4}{J}= \\frac{202,9}{5} = 40,6\\;t/ha\\\\ \\hat{m_5} =\\frac{T_5}{J}= \\frac{247,2}{5} = 49,4\\;t/ha \\] E o erro padrão das médias será: \\[ s(\\hat{m_1}) = s(\\hat{m_2})=s(\\hat{m_3})=s(\\hat{m_4})=s(\\hat{m_5})= \\sqrt{\\frac{QM_{Res}}{J}} = \\sqrt{\\frac{18,68}{5}} = 1,93 \\; t/ha \\] b) Aplicação do teste de Tukey para a comparação das médias de tratamentos b.1) Cálculo do valor de \\(\\Delta\\): \\[ q_{(5\\;Trat \\times 20 GL\\;Resíduo)}(5\\%) = 4.23 \\] Tabela das amplitudes totais studentizadas de Tukey a 5% Então, temos: \\[ \\Delta = q \\cdot \\sqrt{\\frac{QM_{Res}}{J}} = 4,23 \\cdot 1,93 = 8,2\\;t/ha \\] b.2) Organizar as médias em ordem decrescente: \\[ \\hat{m_5} = 49,4\\;t/ha\\\\ \\hat{m_4} = 40,6\\;t/ha\\\\ \\hat{m_1} = 27,9\\;t/ha\\\\ \\hat{m_2} = 27,3\\;t/ha\\\\ \\hat{m_3} = 26,0\\;t/ha \\] b.3) Cálculo das estimativas de contrastes entre duas médias: \\(\\hat{m_4}\\) \\(\\hat{m_1}\\) \\(\\hat{m_2}\\) \\(\\hat{m_3}\\) \\(\\hat{m_5}\\) \\(8,8^{*}\\) \\(21,5^{*}\\) \\(22,1^{*}\\) \\(23,4^{*}\\) \\(\\hat{m_4}\\) \\(12,7^{*}\\) \\(13,3^{*}\\) \\(14,6^{*}\\) \\(\\hat{m_1}\\) \\(0,60^{ns}\\) \\(1,9^{ns}\\) \\(\\hat{m_2}\\) \\(1,3^{ns}\\) b.4) Conclusão: Médias seguidas pela mesma letra, não diferem entre si, pelo teste de Tukey ao nível de 5% de probabilidade. \\[ \\hat{m_5} = 49,4-a\\\\ \\hat{m_4} = 40,6-b\\\\ \\hat{m_1} = 27,9-c\\\\ \\hat{m_2} = 27,3-c\\\\ \\hat{m_3} = 26,0-c \\] Médias seguidas pela mesma letra não diferem entre sim pelo teste de Tukey ao nível de 5% de significância Portanto, a melhor variedade é a Mantiqueira, pois difere de todas as outras pelo teste de Tukey e apresenta maior produção de mandioca. c) Cálculo do coeficiente de variação do experimento: \\[ \\hat{m} = \\frac{G}{IJ} = \\frac{856,2}{5 \\cdot 5} = 34,2 \\;t/ha \\\\ s = \\sqrt{QM_{res}} = \\sqrt{18,64} = 4,32\\;t/ha \\\\ CV = 100 \\cdot \\frac{s}{\\hat{m}} = 100 \\cdot \\frac{4,32}{34,2} = 12,63\\% \\] Utilizando o R para obtermos o quadro da análise de variância, os dados estão disponíveis online em: Mandioca. # Carregando o pacote para a análise library(ExpDes.pt) # Caminho dos dados url&lt;-&quot;https://raw.githubusercontent.com/arpanosso/experimentacao-agricola-unesp-fcav/master/img/aula05/mandioca.txt&quot; # Lendo o arquivo de dados dados&lt;-read.table(url, h = TRUE) # verificando os 6 primeiros registros head(dados) ## Trat Rep Y ## 1 IAC_5 1 38.9 ## 2 IAC_5 2 25.4 ## 3 IAC_5 3 20.3 ## 4 IAC_5 4 25.7 ## 5 IAC_5 5 29.3 ## 6 IAC_7 1 20.9 # Análise de variância e teste de Tukey com a função dic trat &lt;- as.factor(dados$Trat) # Criando o vetor de tratamentos resp &lt;- dados$Y # Criando o vetor com a variável resposta # Utilizando a função dic(trat, resp, mcomp = &quot;tukey&quot;) ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Tratamento 4 2135.94 533.98 28.592 5.0773e-08 ## Residuo 20 373.52 18.68 ## Total 24 2509.46 ## ------------------------------------------------------------------------ ## CV = 12.62 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos ( Shapiro-Wilk ) ## Valor-p: 0.2156065 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## Teste de homogeneidade de variancia ## valor-p: 0.1115615 ## De acordo com o teste de bartlett a 5% de significancia, as variancias podem ser consideradas homogeneas. ## ------------------------------------------------------------------------ ## ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a MANTIQUEIRA 49.44 ## b IRACEMA 40.58 ## c IAC_5 27.92 ## c IAC_7 27.28 ## c IAC_11 26.02 ## ------------------------------------------------------------------------ 15.5 Delinemaneto inteiramente casualizado com número diferente de repetições por tratamento 15.5.1 INTRODUÇÃO Muitas vezes pode acontecer de chegarmos ao final do experimento e não conseguirmos obter os dados de algumas parcelas do experimento. Quando isso ocorre, dizemos que temos parcelas perdidas. Algumas razões para a ocorrência de parcelas perdidas: morte das plantas responsáveis pela parcela; falha do experimentador na coleta dos dados (erro de anotação); perda da ficha onde estão anotados os dados da parcela; a parcela apresenta um valor muito discrepante dos demais e não é considerada para efeito de análise; impossibilidade de acesso à parcela, ou ao bloco devido à condições meteorológicas; falha no equipamento (muito comum na utilização de sensores). 15.5.2 OBTENÇÃO DA ANÁLISE DE VARIÂNCIA Todo o delineamento experimental é estruturado para que haja um perfeito balanceamento, e a perda de parcelas causa um quebra neste balanceamento, acarretando modificações no método de análise estatística. Para ilustrar o procedimento de análise de um experimento inteiramente casualizado, não balanceado, vamos utilizar o exemplo seguinte, referente a dados adaptados do trabalho de CARDOSO FILHO (1974) e se referem a produção de matéria seca de cultivares de sorgo, em t/ha. Experimento de produção de matéria seca de cultivares de sorgo, em t/ha. Para os cultivares, H = Híbrido e V = Variedade. ## Warning: package &#39;knitr&#39; was built under R version 4.1.1 Cultivares Rep.1 Rep.2 Rep.3 Rep.4 Rep.5 Rep.6 Totais NK300H 10.27 11.55 11.68 11.38 11.20 11.24 67.32 Sordan67H 9.77 9.96 11.94 10.18 10.43 10.49 62.77 Pionner988H 9.86 9.59 10.43 9.99   39.87 Pionner93H 21.22 20.62 22.33 19.89 21.00  105.06 SartV 20.20 20.55 22.12 20.78 20.90 20.92 125.47 Total 400.49 Dados originais:DOWNLOAD As hipóteses testadas são: \\[ \\begin{cases} H_0: As\\;cultivares\\; não\\; diferem\\; entre\\; si\\; quanto\\; à\\; produção\\; de\\; matéria\\; seca.\\\\ H_1: As\\;cultivares\\; diferem\\; entre\\; si\\; quanto\\; à\\; produção\\; de\\; matéria\\; seca. \\end{cases} \\] a) Cálculo das Somas de Quadrados: a.1) Cálculo da Soma de Quadrados Total: \\[ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jx_{ij}^2 - C \\\\ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jx_{ij}^2 - \\frac{G^2}{N} \\\\ SQ_{Total} = (10,27^2+11,55^2+...+20,92^2) - \\frac{400,49^2}{27}\\\\ SQ_{Total} = 710,1902 \\] a.2) Cálculo da Soma de Quadrados de Tratamentos: Como os tratamento não são igualmente repetidos, devemos calcular a \\(SQ_{trat}\\) por: \\[ SQ_{Trat} =\\left[ {\\frac{T_1^2}{r_1}+\\frac{T_2^2}{r_2}+...+\\frac{T_I^2}{r_I}} \\right] - \\frac{G^2}{N}\\\\ onde,\\; N=\\sum_{i=1}^Ir_i\\\\ \\] Assim, no nosso exemplo, temos: \\[ SQ_{Trat} =\\left[ {\\frac{62,32^2}{6}+\\frac{62,77^2}{6}+\\frac{39,87^2}{4}+\\frac{105,06^2}{5}+\\frac{125,47^2}{6}} \\right] - \\frac{400,49^2}{27}\\\\ SQ_{Trat}= 700,2677 \\] a.2) Cálculo da Soma de Quadrados do Resíduo: \\[ SQ_{Res} = SQ_{Total} - SQ_{Trat} \\\\ SQ_{Res} = 710,1902 - 700,2677 \\\\ SQ_{Res} = 9,9225 \\] Então, podemos montar o seguinte quadro de análise de variância: Causas.de.Variação GL SQ QM F Tratamento 4 700,2677 175,0669 388,17** Resíduo 22 9,9225 0,4510  Total 26 710,1902   Valores de \\(F\\) da tabela para (4 x 22 GL):\\(\\begin{cases} 5\\%=2,82 \\\\ 1\\%=4,31 \\end{cases}\\) Conclusão: O teste F foi significativo ao nível de 1% de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que as cultivares de sorgo possuem efeitos diferentes sobre a produção de matéria seca, com um grau de confiança de \\(99\\%\\) de probabilidade. Para tirar conclusões mais específicas sobre o comportamento das cultivares, devemos utilizar um teste de compração de médias. 15.5.3 CÁLCULO DAS MÉDIAS E ERROS PADRÕES DAS MÉDIAS: A média do Tratamento \\(i\\) é dada por: \\[ \\hat{m_i} = \\frac{T_i}{r_i}, \\] e seu erro padrão, por: \\[ s(\\hat{m_i})=\\frac{s}{\\sqrt{r_i}} = \\sqrt{\\frac{QM_{Resíduo}}{r_i}}. \\] Então, no nosso exemplo, temos: \\[ \\hat{m_1} = \\frac{T_1}{r_1}=\\frac{67,32}{6}=11,22\\;t/ha\\\\ \\hat{m_2} = \\frac{T_2}{r_2}=\\frac{62,77}{6}=10,46\\;t/ha\\\\ \\hat{m_3} = \\frac{T_3}{r_3}=\\frac{39,87}{4}=9,97\\;t/ha\\\\ \\hat{m_4} = \\frac{T_4}{r_4}=\\frac{105,06}{5}=21,01\\;t/ha\\\\ \\hat{m_5} = \\frac{T_5}{r_5}=\\frac{125,47}{6}=20,91\\;t/ha\\\\ \\] E os erros padrões dessas médias serão: \\[ s(\\hat{m_1})=s(\\hat{m_2})=s(\\hat{m_5})=\\sqrt{\\frac{0,4510}{6}} = 0,27\\;t/ha.\\\\ s(\\hat{m_3})=\\sqrt{\\frac{0,4510}{4}} = 0,34\\;t/ha.\\\\ s(\\hat{m_4})=\\sqrt{\\frac{0,4510}{5}} = 0,30\\;t/ha. \\] 15.5.4 TESTE DE TUKEY PARA COMPARAÇÃO DAS MÉDIAS Como os tratamentos não possuem o mesmo número de repetições, vamos ter diversos casos a considerar: Para a aplicação do teste de Tukey, devemos colocar as médias em ordem decresecente, acompanhadas do número de repetições com que foram calculadas, ou seja \\[ \\hat{m_4}=21,01 \\:\\:\\:\\:\\:\\: r_4=5\\\\ \\hat{m_5}=20,91 \\:\\:\\:\\:\\:\\: r_5=6\\\\ \\hat{m_1}=11,22 \\:\\:\\:\\:\\:\\: r_1=6\\\\ \\hat{m_2}=10,46 \\:\\:\\:\\:\\:\\: r_2=6\\\\ \\hat{m_3}=9,97 \\:\\:\\:\\:\\:\\:\\:\\: r_3=4\\\\ \\] a. Comparação entre médias com 6 repetições entre si: Neste caso, como as médias são igualmente repetidas, a dms é dada por: \\[ q_{(5\\times 22\\;GLres)} = 4,20,\\;então \\\\ \\Delta=q\\cdot\\sqrt{\\frac{0,4510}{6}}=1,15\\;t/ha \\] Os contrastes que envolvem comparações são: \\[ \\hat{Y_1} = \\hat{m_5} - \\hat{m_1} = 9,69^*\\;t/ha \\\\ \\hat{Y_2} = \\hat{m_5} - \\hat{m_2} = 10,45^*\\;t/ha \\\\ \\hat{Y_3} = \\hat{m_1} - \\hat{m_2} = 0,76^{ns}\\;t/ha \\] b. Comparação entre médias com 6 repetições com média com 5 repetições: Neste caso, a dms será calculada por: \\[ \\Delta = q\\cdot\\sqrt{\\frac{1}{2}\\hat{V(\\hat{Y})}}, onde\\;\\\\ \\hat{V(\\hat{Y})}= \\left( \\frac{1}{6}+\\frac{1}{5} \\right)\\cdot0,4510 = 0,1654 \\] Portanto: \\[ \\Delta = 4.20\\cdot\\sqrt{\\frac{1}{2}\\cdot0,1654} = 1,21\\;t/ha \\] Os contrastes que envolvem comparações são: \\[ \\hat{Y_4} = \\hat{m_4} - \\hat{m_1} = 9,79^*\\;t/ha \\\\ \\hat{Y_5} = \\hat{m_4} - \\hat{m_2} = 10,55^*\\;t/ha \\\\ \\hat{Y_6} = \\hat{m_4} - \\hat{m_5} = 0,10^{ns}\\;t/ha \\] c. Comparação entre médias com 6 repetições com média com 4 repetições: Neste caso, a dms será calculada por: \\[ \\Delta = q\\cdot\\sqrt{\\frac{1}{2}\\hat{V(\\hat{Y})}}, onde\\;\\\\ \\hat{V(\\hat{Y})}= \\left( \\frac{1}{6}+\\frac{1}{4} \\right)\\cdot0,4510 = 0,1879 \\] Portanto: \\[ \\Delta = 4.20\\cdot\\sqrt{\\frac{1}{2}\\cdot0,1879} = 1,29\\;t/ha \\] Os contrastes que envolve comparações são: \\[ \\hat{Y_7} = \\hat{m_1} - \\hat{m_3} = 1,25^{ns}\\;t/ha \\\\ \\hat{Y_8} = \\hat{m_2} - \\hat{m_3} = 0,49^{ns}\\;t/ha \\\\ \\hat{Y_9} = \\hat{m_5} - \\hat{m_3} = 10,94^{*}\\;t/ha \\] d. Comparação entre médias com 5 repetições com média com 4 repetições: Neste caso, a dms será calculada por: \\[ \\Delta = q\\cdot\\sqrt{\\frac{1}{2}\\hat{V(\\hat{Y})}}, onde \\\\ \\hat{V(\\hat{Y})}= \\left( \\frac{1}{5}+\\frac{1}{4} \\right)\\cdot0,4510 = 0,2030 \\] Portanto: \\[ \\Delta = 4.20\\cdot\\sqrt{\\frac{1}{2}\\cdot0,2030} = 1,34\\;t/ha \\] O contraste que envolvem comparação é: \\[ \\hat{Y_{10}} = \\hat{m_4} - \\hat{m_3} = 11,04^{*}\\;t/ha \\] Podemos agora montar um quadro com todos os contrastes: \\(\\hat{m_5}\\) \\(\\hat{m_1}\\) \\(\\hat{m_2}\\) \\(\\hat{m_3}\\) \\(\\hat{m_4}\\) \\(0,10^{ns}\\) \\(9,79^{*}\\) \\(10,55^{*}\\) \\(11,04^{*}\\) \\(\\hat{m_5}\\) \\(9,69^{*}\\) \\(10,45^{*}\\) \\(10,94^{*}\\) \\(\\hat{m_1}\\) \\(0,76^{ns}\\) \\(1,25^{ns}\\) \\(\\hat{m_2}\\) \\(0,49^{ns}\\) \\[ \\hat{m_4}=21,01 \\:\\:\\:\\:\\:\\: a\\\\ \\hat{m_5}=20,91 \\:\\:\\:\\:\\:\\: a\\\\ \\hat{m_1}=11,22 \\:\\:\\:\\:\\:\\: b\\\\ \\hat{m_2}=10,46 \\:\\:\\:\\:\\:\\: b\\\\ \\hat{m_3}=9,97 \\:\\:\\:\\:\\:\\:\\:\\:b \\] Resumo dos resultados obtidos no Teste de Tukey: Cultivares Médias Tukey Erros padrões NK300H 11.22 b 0.27 Sordan67H 10.46 b 0.27 Pionner988H 9.97 b 0.34 Pionner93H 21.01 a 0.30 SartV 20.91 a 0.27 15.5.5 CÁLCULO DO COEFICIENTE DE VARIAÇÃO DO EXPERIMENTO \\[ \\hat{m} = \\frac{G}{N} = \\frac{400,49}{27} = 14,82\\;t/ha \\\\ s = \\sqrt{QM_{Res}}=\\sqrt{0,4510}=0,6716\\;t/ha\\\\ \\] Portanto: \\[ CV=100\\cdot \\frac{s}{\\hat{m}} = 100\\cdot \\frac{0,6716}{14,83}=4,53\\% \\] Utilizando o R para obtermos o quadro da análise de variância, os dados estão disponíveis online em: sorgo. # Carregando o pacote para a análise library(ExpDes.pt) # Caminho dos dados caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/sorgo.txt&quot; # Lendo o arquivo de dados dados&lt;-read.table(caminho,h=T,sep=&quot;\\t&quot;) # verificando os 6 primeiros registros head(dados) ## Trat Y ## 1 NK300H 10.27 ## 2 Sordan67H 9.77 ## 3 Pionner988H 9.86 ## 4 Pionner93H 21.22 ## 5 SartV 20.20 ## 6 NK300H 11.55 # Análise de variância e teste de Tukey com a função dic trat &lt;- dados$Trat[!is.na(dados$Y)] # Criando o vetor de tratamentos sem os valores perdidos massa_seca &lt;- dados$Y[!is.na(dados$Y)] # Criando o vetor com a variável resposta semos valores perdidos # Utilizando a função dic(trat,massa_seca,mcomp = &quot;tukey&quot;, sigT = .01) ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Tratamento 4 700.27 175.067 388.15 4.6914e-20 ## Residuo 22 9.92 0.451 ## Total 26 710.19 ## ------------------------------------------------------------------------ ## CV = 4.53 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos ( Shapiro-Wilk ) ## Valor-p: 0.05636895 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## Teste de homogeneidade de variancia ## valor-p: 0.9943713 ## De acordo com o teste de bartlett a 5% de significancia, as variancias podem ser consideradas homogeneas. ## ------------------------------------------------------------------------ ## ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a Pionner93H 21.012 ## a SartV 20.91167 ## b NK300H 11.22 ## b Sordan67H 10.46167 ## b Pionner988H 9.9675 ## ------------------------------------------------------------------------ 15.5.6 APLICAÇÃO DO TESTE t DE STUDENT A UM CONTRASTE Para exemplificar, vamos estudar o seguinte contraste: \\[ Y=m_1+m_2+m_3+m_4-4m_5 \\] que permite testar se a média dos híbridos difere da média da variedade. Neste caso, as hipóteses a serem testada serão: \\[ \\begin{cases} H_0:\\frac{m_1+m_2+m_3+m_4}{4}=m_5 \\\\ H_1: \\frac{m_1+m_2+m_3+m_4}{4} \\neq m_5 \\end{cases} \\] A estimativa do contraste será: \\[ \\hat{Y} = \\hat{m_1}+\\hat{m_2}+\\hat{m_3}+\\hat{m_4}-4\\hat{m_5} \\\\ \\hat{Y} = 11,22+10,46+9,97+21,01-4\\cdot20,91 \\\\ \\hat{Y} = -30,98 \\] A estimativa da variância do contraste será: \\[ \\hat{V}(\\hat{Y}) = \\left(\\frac{c_1^2}{r_1}+\\frac{c_2^2}{r_2}+...+\\frac{c_I^2}{r_I} \\right)s^2 \\\\ \\hat{V}(\\hat{Y}) = \\left(\\frac{1^2}{6}+\\frac{1^2}{6}+\\frac{1^2}{4}+\\frac{1^2}{5}+\\frac{(-4)^2}{6} \\right)0,4510 \\\\ \\hat{V}(\\hat{Y}) = 1,5560 \\] Assim, o erro padrão do contraste será: \\[ s(\\hat{Y})=\\sqrt{\\hat{V}(\\hat{Y})} = \\sqrt{1,5560}=1,25 \\] e o valor da estatística do teste t será: \\[ t_{obs} = \\frac{\\hat{Y}}{s(\\hat{Y})}=\\frac{-30,98}{1.25}=-24,78^{**} \\\\ t_{tab (22 GLres)} \\begin{cases}5\\%=2,07\\\\1\\%=2,82 \\end{cases} \\] Conclusão: Como \\(t_{obs} &gt; t_{tab}\\), rejeitamos \\(H_0\\) e concluímos que a média do grupo de híbridos diferem da média da variedade SART, sendo que, em médias, a variedade possui maior produção de matéria seca que os híbridos. require(agricolae) ## Carregando pacotes exigidos: agricolae ## Warning: package &#39;agricolae&#39; was built under R version 4.1.2 ## ## Attaching package: &#39;agricolae&#39; ## The following objects are masked from &#39;package:ExpDes.pt&#39;: ## ## lastC, order.group, tapply.stat m &lt;- tapply(massa_seca,trat,mean,na.rm=TRUE) r &lt;- tapply(massa_seca,trat, .nac&lt;-function(x) {x&lt;-na.omit(x);length(x)}) QMRes &lt;- 0.4510 glRes &lt;- 22 #Híbridos vs Variedades m ## NK300H Pionner93H Pionner988H SartV Sordan67H ## 11.22000 21.01200 9.96750 20.91167 10.46167 c1&lt;-c(1,1,1,-4,1) # atribua os coeficientes de acordo com a ordem das médias (Y1=sum(m*c1)) ## [1] -30.9855 (tobs=Y1/sqrt(sum(c1^2/r)*QMRes)) ## [1] -24.84052 (tc5=qt(1-0.05/2,glRes)) ## [1] 2.073873 (tc1=qt(1-0.01/2,glRes)) ## [1] 2.818756 # Construção do Gráfico para interpretação curve(dt(x,glRes),-30,9,xlab=&quot;t&quot;,ylab=&quot;D(t)&quot;,las=1) abline(v=0) abline(v=c(tc5,-tc5,tc1,-tc1),col=2,lty=2) abline(v=tobs,col=4,lty=1) text(2.8,.1,expression(paste(alpha,&quot;=2.5%&quot;))) text(4,.02,expression(paste(alpha,&quot;=0.5%&quot;))) text(-2.8,.1,expression(paste(alpha,&quot;=2.5%&quot;))) text(-4,.02,expression(paste(alpha,&quot;=0.5%&quot;))) text(tobs,.05,&quot;tobs&quot;) 15.5.7 TESTE DE DUNCAN PARA A COMPARAÇÃO DAS MÉDIAS Para a aplicação do teste de Duncan, devemos colocar as médias em ordem decresecente, acompanhadas do número de repetições com que foram calculadas, ou seja \\[ \\hat{m_4}=21,01 \\:\\:\\:\\:\\:\\: r_4=5\\\\ \\hat{m_5}=20,91 \\:\\:\\:\\:\\:\\: r_5=6\\\\ \\hat{m_1}=11,22 \\:\\:\\:\\:\\:\\: r_1=6\\\\ \\hat{m_2}=10,46 \\:\\:\\:\\:\\:\\: r_2=6\\\\ \\hat{m_3}=9,97 \\:\\:\\:\\:\\:\\:\\:\\: r_3=4\\\\ \\] Amplitude total estudentizada (z), encontradas na tabela para uso no teste de Duncan, ao nível de 5% de significância. n n (22) 2 2.93 3 3.08 4 3.17 5 3.24 a) Contrastes que abrangem 5 médias: \\[ \\hat{Y_1}=\\hat{m_4}-\\hat{m_3} = 21,01-9,97 = 11,04\\;t/ha \\\\ \\hat{V}(\\hat{Y_1})= \\left( \\frac{1}{r_4} + \\frac{1}{r_3}\\right)s^2 = \\left( \\frac{1}{5} + \\frac{1}{4}\\right)0,4510 = 0,2030\\\\ D_5 = z_\\alpha \\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y})}= 3,24\\sqrt{\\frac{1}{2}\\cdot0,2030} = 1,03\\; t/ha \\] Como \\(\\hat{Y_1} &gt; D_5&#39;\\), rejeita-se \\(H_0\\), o contraste é significativo, indicando que \\(\\hat{m_4}\\) difere de \\(\\hat{m_3}\\). b) Contrastes que abrangem 4 médias: \\[ \\begin{cases} \\hat{Y_2}=\\hat{m_4}-\\hat{m_2} = 21,01-10,46 = 10,55\\;t/ha \\\\ \\hat{Y_3}=\\hat{m_5}-\\hat{m_3} = 20,91-9,97 = 10,94\\;t/ha \\end{cases} \\\\ \\hat{V}(\\hat{Y_2})= \\left( \\frac{1}{r_4} + \\frac{1}{r_2}\\right)s^2 = \\left( \\frac{1}{5} + \\frac{1}{6}\\right)0,4510 = 0,1654\\\\ D_4&#39; = z_\\alpha \\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y_2})}= 3,17\\sqrt{\\frac{1}{2}\\cdot0,1654} = 0,91\\; t/ha \\\\ \\] Como \\(\\hat{Y_2} &gt; D_4&#39;\\), rejeita-se \\(H_0\\), o contraste é significativo, indicando que \\(\\hat{m_4}\\) difere de \\(\\hat{m_2}\\). \\[ \\hat{V}(\\hat{Y_3})= \\left( \\frac{1}{r_5} + \\frac{1}{r_3}\\right)s^2 = \\left( \\frac{1}{6} + \\frac{1}{4}\\right)0,4510 = 0,1879\\\\ D&#39;_4 = z_\\alpha \\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y_3})}= 3,17\\sqrt{\\frac{1}{2}\\cdot0,1879} = 0,97\\; t/ha \\\\ \\] Como \\(\\hat{Y_3} &gt; D&#39;&#39;_4\\), rejeita-se \\(H_0\\), o contraste é significativo, indicando que \\(\\hat{m_5}\\) difere de \\(\\hat{m_3}\\). c) Contrastes que abrangem 3 médias: \\[ \\begin{cases} \\hat{Y_4}=\\hat{m_4}-\\hat{m_1} = 21,01-11,22 = 9,79\\;t/ha \\\\ \\hat{Y_5}=\\hat{m_5}-\\hat{m_2} = 20,91-10,46 = 10,45\\;t/ha \\\\ \\hat{Y_6}=\\hat{m_4}-\\hat{m_1} = 11,22-9,97 = 1,25\\;t/ha \\end{cases} \\\\ \\hat{V}(\\hat{Y_4})= \\left( \\frac{1}{r_4} + \\frac{1}{r_1}\\right)s^2 = \\left( \\frac{1}{5} + \\frac{1}{6}\\right)0,4510 = 0,1654\\\\ D&#39;_3 = z_\\alpha \\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y_4})}= 3,08\\sqrt{\\frac{1}{2}\\cdot0,1654} = 0,89\\; t/ha \\\\ \\] Como \\(\\hat{Y_4} &gt; D&#39;_3\\), rejeita-se \\(H_0\\), o contraste é significativo, indicando que \\(\\hat{m_4}\\) difere de \\(\\hat{m_1}\\). \\[ D_3 = z_\\alpha \\sqrt{\\frac{QM_{Res}}{r}} = 3,08 \\sqrt{\\frac{0,4510}{6}} = 0,84\\; t/ha \\] Como \\(\\hat{Y_5} &gt; D_3\\), rejeita-se \\(H_0\\), o contraste é significativo, indicando que \\(\\hat{m_5}\\) difere de \\(\\hat{m_2}\\). \\[ \\hat{V}(\\hat{Y_6})= \\left( \\frac{1}{r_1} + \\frac{1}{r_3}\\right)s^2 = \\left( \\frac{1}{6} + \\frac{1}{4}\\right)0,4510 = 0,1879\\\\ D&#39;&#39;_3 = z_\\alpha \\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y_C})}= 3,08\\sqrt{\\frac{1}{2}\\cdot0,1879} = 0,94\\; t/ha \\\\ \\] Como \\(\\hat{Y_6} &gt; D&#39;&#39;_3\\), rejeita-se \\(H_0\\), o contraste é significativo, indicando que \\(\\hat{m_1}\\) difere de \\(\\hat{m_3}\\). d) Contrastes que abrangem 2 médias: \\[ \\begin{cases} \\hat{Y_7}=\\hat{m_4}-\\hat{m_5} = 21,01-20,91 = 0,10\\;t/ha \\\\ \\hat{Y_8}=\\hat{m_5}-\\hat{m_1} = 20,91-11,22 = 9,69\\;t/ha \\\\ \\hat{Y_9}=\\hat{m_1}-\\hat{m_2} = 11,22-10,46 = 0,76\\;t/ha \\\\ \\hat{Y_{10}}=\\hat{m_2}-\\hat{m_3} = 10,46-9,97 = 0,49\\;t/ha \\end{cases} \\\\ \\hat{V}(\\hat{Y_7})= \\left( \\frac{1}{r_4} + \\frac{1}{r_5}\\right)s^2 = \\left( \\frac{1}{5} + \\frac{1}{6}\\right)0,4510 = 0,1654\\\\ D_2&#39; = z_\\alpha \\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y_7})}= 2,93\\sqrt{\\frac{1}{2}\\cdot0,1654} = 0,84\\; t/ha \\\\ \\] Como \\(\\hat{Y_7} &lt; D&#39;_2\\), não rejeita-se \\(H_0\\), o contraste não é significativo, indicando que \\(\\hat{m_4}\\) não difere de \\(\\hat{m_5}\\). As médias devem ser unidas por uma barra. \\[ D_2 = z_\\alpha \\sqrt{\\frac{QM_{Res}}{r}} = 2,93 \\sqrt{\\frac{0,4510}{6}} = 0,80\\; t/ha \\] Como \\(\\hat{Y_8} &gt; D_2\\), rejeita-se \\(H_0\\), o contraste é significativo, indicando que \\(\\hat{m_5}\\) difere de \\(\\hat{m_1}\\). Como \\(\\hat{Y_9} &lt; D_2\\), não rejeita-se \\(H_0\\), o contraste não é significativo, indicando que \\(\\hat{m_1}\\) não difere de \\(\\hat{m_2}\\). As médias devem ser unidas por uma barra. \\[ \\hat{V}(\\hat{Y_{10}})= \\left( \\frac{1}{r_2} + \\frac{1}{r_3}\\right)s^2 = \\left( \\frac{1}{6} + \\frac{1}{4}\\right)0,4510 = 0,1879\\\\ D&#39;&#39;_2 = z_\\alpha \\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y_{10}})}= 2,93\\sqrt{\\frac{1}{2}\\cdot0,1879} = 0,90\\; t/ha \\\\ \\] Como \\(\\hat{Y_{10}} &lt; D&#39;&#39;_2\\), não rejeita-se \\(H_0\\), o contraste é não significativo, indicando que \\(\\hat{m_2}\\) não difere de \\(\\hat{m_3}\\). As médias devem ser unidas por uma barra. Cultivares Médias Pionner93H 21.01 a SartV 20.91 a NK300H 11.22 b Sordan67H 10.46 bc Pionner988H 9.97 c 15.5.8 Utilizando o R # Carregando o pacote para a análise library(ExpDes.pt) # Caminho dos dados caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/sorgo.txt&quot; # Lendo o arquivo de dados dados&lt;-read.table(caminho,h=T,sep=&quot;\\t&quot;) # verificando os 6 primeiros registros head(dados) ## Trat Y ## 1 NK300H 10.27 ## 2 Sordan67H 9.77 ## 3 Pionner988H 9.86 ## 4 Pionner93H 21.22 ## 5 SartV 20.20 ## 6 NK300H 11.55 # Análise de variância e teste de Tukey com a função dic trat &lt;- dados$Trat[!is.na(dados$Y)] # Criando o vetor de tratamentos sem os valores perdidos massa_seca &lt;- dados$Y[!is.na(dados$Y)] # Criando o vetor com a variável resposta semos valores perdidos # Utilizando a função dic(trat,massa_seca,mcomp = &quot;duncan&quot;, sigT = .01) ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Tratamento 4 700.27 175.067 388.15 4.6914e-20 ## Residuo 22 9.92 0.451 ## Total 26 710.19 ## ------------------------------------------------------------------------ ## CV = 4.53 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos ( Shapiro-Wilk ) ## Valor-p: 0.05636895 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## Teste de homogeneidade de variancia ## valor-p: 0.9943713 ## De acordo com o teste de bartlett a 5% de significancia, as variancias podem ser consideradas homogeneas. ## ------------------------------------------------------------------------ ## ## Teste de Duncan ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a Pionner93H 21.012 ## a SartV 20.91167 ## b NK300H 11.22 ## bc Sordan67H 10.46167 ## c Pionner988H 9.9675 ## ------------------------------------------------------------------------ "],["desdobramento-de-graus-de-liberdade-de-tratamento.html", "16 Desdobramento de graus de liberdade de tratamento 16.1 Introdução 16.2 Obtenção da análise de variância 16.3 Método dos contrastes de totais de tratamentos 16.4 Método dos totais de tratamentos (sem utilizar contrastes) 16.5 Análise no R", " 16 Desdobramento de graus de liberdade de tratamento 16.1 Introdução Quando planejamos um experimento, normalmente, o fazemos de tal forma que o mesmo nos forneça respostas a uma série de perguntas. Geralmente, com um único experimento, o pesquisador deseja testar várias hipóteses relacionadas com os efeitos dos tratamentos utilizados. Isso pode ser feito, decompondo-se os graus de liberdade de tratamentos, isolando-se o efeito devido a cada um dos graus de liberdade, ou então, a cada grupo de graus de liberdade. Conforme vimos, quando efetuamos a análise de variância pelo teste \\(F\\), trabalhando com todos os tratamentos, obtemos informações muito mais gerais, relacionadas com o comportamento dos tratamentos como um todo. Por meio do desdobramento ou decomposição dos graus de liberdade de tratamentos, podemos obter informações muito mais específicas, relacionadas com o comportamento de cada um dos componentes do desdobramento. Além disso, após a decomposição dos graus de liberdade de tratamentos, podemos aplicar o teste \\(F\\) a cada um dos componentes do desdobramento. Essa técnica baseia-se na utilização de contrastes, sendo necessário que os contrastes correspondentes aos componentes do desdobramento sejam ORTOGONAIS entre si. Isto significa que os componentes do desdobramento devem ser independentes. Uma vez que desejamos aplicar o teste \\(F\\) a cada um dos componentes do desdobramento, cada componente deve possuir uma estimativa de variância, e portanto, devemos decompor a soma dos quadrados de tratamentos em partes atribuídas a cada um dos componentes do desdobramento. Então, podemos dizer que o desdobramento dos graus de liberdade de tratamentos é uma técnica semelhante à da análise de variância, que nos permite fazer partições dos graus de liberdade e da soma de quadrados de tratamentos em partes atribuídas a vários componentes independentes, sendo que cada um dos componentes deverá nos proporcionar uma estimativa de variância. Existem dois métodos que nos permitem calcular as somas de quadrados correspondentes a cada um dos componentes do desdobramento: Método dos contrastes de totais de tratamentos. Métodos dos totais de tratamentos, sem utilizar contraste. O procedimento utilizado para o desdobramento dos graus de liberdade de tratamentos será apresentado por meio de um exemplo numérico. 16.2 Obtenção da análise de variância Vamos considerar os dados do trabalho Ação de microrganismos do solo sobre o desenvolvimento de mudas de Eucalyptus grandis Hill Ex Maiden em tubetes (Korari, 1990), realizado no delineamento inteiramente casualizado, com 5 tratamentos e 5 repetições. Os tratamentos utilizados foram: ST - Solo rizosférico de Paspalum notatum (grama batatais) AC - Cultura de Azotobacter chroococcum AP - Cultura de Azotobacter paspali PM - Cultura de Azotobacter paspali morta T - Testemunha Os resultados obtido para a altura média das mudas de Eucalyptus grandis (cm), \\(105\\) dias após a inoculação foram os seguintes. eucalyptus. Tratamentos Rep.1 Rep.2 Rep.3 Rep.4 Rep.5 Total 1 - ST 25.6 25.48 25.06 20.58 19.02 115.74 2 - AC 27.2 21.57 23.61 24.89 20.27 117.54 3 - AP 25.36 31.07 27.3 18.93 18.09 120.75 4 - PM 17.97 19.07 15.94 16.46 16.44 85.88 5 - T 18.92 16.07 14.5 20.17 22.27 91.93 Total 531.84 Dados originais:DOWNLOAD Inicialmente, devemos fazer a análise de variância, denominada de análise de variância preliminar, de acordo com o delineamento inteiramente casualizado. Assim, temos: Cálculo das Somas de Quadrados: a)Soma de quadrados Total: \\[ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jy_{ij}^2 - C \\] \\[ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jy_{ij}^2 - \\frac{G^2}{IJ} \\] \\[ SQ_{Total} = (25,60^2+25,48^2+...+22,27^2) - \\frac{531,84^2}{5\\cdot5} \\] \\[ SQ_{Total} = 447,1366 \\] b) Soma de quadrados de Tratamentos: \\[ SQ_{Trat} = \\frac{\\sum_{i=1}^IT_i^2}{J} - C \\] \\[ SQ_{Trat} = \\frac{T_1^2+T_2^2+...+T_I^2}{J} - \\frac{G^2}{IJ} \\] \\[ SQ_{Trat} = \\frac{115,74^2+117,54^2+...+91,93^2}{5} - \\frac{531,84^2}{5\\cdot5} \\] \\[ SQ_{Trat} = 209,5408 \\] c) Soma de Quadrado de Resíduo: \\[ SQ_{Res} = SQ_{Total} - SQ_{Trat} \\] \\[ SQ_{Res} = 447,1366- 209,5408 = 237,5958 \\] Então, podemos montar o seguinte quadro de análise de variância: Causas.de.Variação GL SQ QM F Tratamento 4 209,5408 52,3852 4,41* Resíduo 20 237,5958 11,8798  Total 24 447,1366   Valores de F da tabela para \\(4 \\times 20\\) GL:\\(\\begin{cases} 5\\%=2,87 \\\\ 1\\%=4,43 \\end{cases}\\) Conclusão: O teste \\(F\\) foi significativo ao nível de 5% de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que os tratamentos possuem efeitos diferentes sobre a altura média de mudas de Eucalypto, com um grau de confiança de \\(95\\%\\) de probabilidade. Podemos notar que esta é uma conclusão muito geral, relacionada com o efeito dos tratamentos como um todo, nada nos dizendo com relação à comparação entre os tratamentos. Para a obtenção de informações mais específicas, podemos proceder à decomposição dos graus de liberdade de tratamentos. Para desdobrar os graus de liberdade de tratamentos, devemos, inicialmente, estabelecer os componentes do desdobramento, de tal forma que as comparações sejam de interesse prático, e os contrastes correspondentes aos componentes do desdobramento sejam ortogonais entre si. Assim, no nosso exemplo, podemos proceder ao seguinte desdobramento: 1 - (ST+AC+AP) vs. (PM+T).1 GL 2 - ST vs. (AC+AP)1 GL 3 - AC vs. AP..1 GL 4 - PM vs. T.1 GL Sendo a técnica do desdobramento de graus de liberdade de tratamentos, baseada em contrastes ortogonais, devemos estabelecer os contrastes de totais de tratamentos correspondentes à cada componente do desdobramento: \\[ Y_1 = 2T_1+2T_2+2T_3-3T_4-3T5 \\] \\[ Y_2 = 2T_1-T_2-T_3+0T_4+0T5 \\] \\[ Y_3 = 0T_1+T_2-T_3+0T_4+0T5 \\] \\[ Y_4 = 0T_1+0T_2+0T_3+T_4-T5 \\] Note que os contrastes são ortogonais entre si, uma vez que são ortogonais dois a dois. Para o cálculo das somas de quadrados correspondentes a cada componente do desdobramento, podemos utilizar um dos dois métodos seguintes 16.3 Método dos contrastes de totais de tratamentos Para aplicação deste método, devemos, inicialmente, estabelecer os contrastes correspondentes a cada um dos componentes do desdobramento, tomando-se o cuidado de trabalhar com contrastes ortogonais entre si. Esses contrastes diferem dos que utilizamos normalmente, por serem funções lineares de totais de tratamentos e não de médias de tratamentos. Vejamos então, como procedemos para calcular a soma de quadrados correspondente a um determinado componente do desdobramento, conhecendo-se o seu contraste. Se tivermos um contraste de totais de tratamento, da forma genérica: \\[ Y=c_1T_1+c_2T_2+...+c_IT_I \\] onde, \\(\\sum_{i=1}^Ic_i=0\\), e cuja estimativa é dada por: \\[ \\hat{Y}=c_1\\hat{T_1}+c_2\\hat{T_2}+...+c_I\\hat{T_I} \\] a soma de quadrados correspondente é obtida por: \\[ SQY=\\frac{\\hat{Y^2}}{r\\sum_{i=1}^Ic_i^2} \\] em que: \\(\\hat{Y}\\) é a estimativa do contraste, \\(r\\) é o número de repetições com que foram obtidos os totais de tratamentos. \\(c_i\\) é o coeficiente do total do tratamento \\(i\\) no contraste. No nosso exemplo, os contrastes de totais de tratamentos já foram estabelecidos, de modo que podemos passar ao cálculo das somas de quadrados: a) Cálculo da soma de quadrados do componente (ST+AC+AP) vs. (PM+T): \\[ \\hat{Y_1}=2\\hat{T_1}+2\\hat{T_2}+2\\hat{T_3}-3\\hat{T_4}-3\\hat{T_5} \\] \\[ \\hat{Y_1}=2\\cdot115,74+2\\cdot117,54+2\\cdot120,75-3\\cdot85,88-3\\cdot91,95 \\] \\[ \\hat{Y_1}=174,63\\;cm \\] \\[ \\sum_{i=1}^Ic_i^2=2^2+2^2+2^2+(-3)^2+(-3)^2=30\\:\\:\\:\\:\\:e\\:\\:\\:\\:r=5 \\] Então, temos: \\[ SQY_1=\\frac{\\hat{Y_1^2}}{r\\sum_{i=1}^Ic_i^2}=\\frac{174,63^2}{5\\cdot30}=203,3042 \\] b) Cálculo da soma de quadrados do componente ST vs. (AC+AP): \\[ \\hat{Y_2}=2\\hat{T_1}-\\hat{T_2}-\\hat{T_3} \\] \\[ \\hat{Y_2}=2\\cdot115,74-117,54-120,75 \\] \\[ \\hat{Y_2}=-6,81\\;cm \\] \\[ \\sum_{i=1}^Ic_i^2=2^2+(-1)^2+(-1)^2=6\\:\\:\\:\\:\\:e\\:\\:\\:\\:r=5 \\] Então, temos: \\[ SQY_2=\\frac{\\hat{Y_2^2}}{r\\sum_{i=1}^Ic_i^2}=\\frac{(-6,81)^2}{5\\cdot6}=1,5459 \\] c) Cálculo da soma de quadrados do componente AC vs. AP: \\[ \\hat{Y_3}=\\hat{T_2}-\\hat{T_3} \\] \\[ \\hat{Y_3}=117,54-120,75 \\] \\[ \\hat{Y_3}=-3,21\\;cm \\] \\[ \\sum_{i=1}^Ic_i^2=1^2+(-1)^2=2\\:\\:\\:\\:\\:e\\:\\:\\:\\:r=5 \\] Então, temos: \\[ SQY_3=\\frac{\\hat{Y_3^2}}{r\\sum_{i=1}^Ic_i^2}=\\frac{(-3,21)^2}{5\\cdot2}=1,0304 \\] d) Cálculo da soma de quadrados do componente PM vs. T: \\[ \\hat{Y_4}=\\hat{T_4}-\\hat{T_5} \\] \\[ \\hat{Y_4}=85,88-91,95 \\] \\[ \\hat{Y_4}=-6,05\\;cm \\] \\[ \\sum_{i=1}^Ic_i^2=1^2+(-1)^2=2\\:\\:\\:\\:\\:e\\:\\:\\:\\:r=5 \\] Então, temos: \\[ SQY_4=\\frac{\\hat{Y_4^2}}{r\\sum_{i=1}^Ic_i^2}=\\frac{(-6,05)^2}{5\\cdot2}=3,6603 \\] VERIFICAÇÃO: \\[ SQY_1+SQY_2+SQY_3+SQY_4=SQ_{Trat} \\] \\[ 203,3042+1,5459+1,0304+3,6603=209,5808 \\] 16.4 Método dos totais de tratamentos (sem utilizar contrastes) Embora a técnica do desdobramento seja baseada em contrastes ortogonais, para o cálculo das somas de quadrados dos componentes do desdobramento por este método, necessitamos apenas conhecer os totais de tratamentos e o número de parcelas somadas para obter cada total de tratamento, não nos preocupando em utilizar os contrastes. Assim, para uma comparação qualquer, por exemplo: Grupo A vs. Grupo B, necessitamos apenas conhecer os totais de cada grupo (\\(T_A\\) e \\(T_B\\)) e o número de parcelas somadas para obter estes totais (\\(n_A\\) e \\(n_B\\)). então temos: \\[ \\frac{\\begin{cases} Total\\;do\\;grupo\\;A=T_A\\;(n_a\\;parcelas) \\\\ Total\\;do\\;grupo\\;B=T_B\\;(n_B\\;parcelas) \\end{cases}} {Total(A+B)=(T_A+T_B)\\;(n_A+n_b \\;parcelas)} \\] a soma de quadrados correspondente será calculada por: \\[ SQ_{A\\;vs.\\;B}=\\frac{T_A^2}{n_A}+\\frac{T_B^2}{n_B}-\\frac{(T_A+T_B)^2}{(n_A+n_B)} \\] No nosso Exemplo, temos: a) Cálculo da soma de quadrados do componente (ST+AC+AP) vs. (PM+T): \\[ \\frac{\\begin{cases} Total\\;do\\;grupo\\;(ST+AC+AP)=354,03\\;(15\\;parcelas) \\\\ Total\\;do\\;grupo\\;(PM+T)=177,81\\;(10\\;parcelas) \\end{cases}} {Total=531,84\\;(25\\;parcelas)} \\] Então, temos: \\[ SQ_{(ST+AC+AP)\\;vs.\\;(PM+T)}=\\frac{354,03^2}{15}+\\frac{177,81^2}{10}-\\frac{531,84^2}{25}=203,3042 \\] b) Cálculo da soma de quadrados do componente ST vs. (AC+AP): \\[ \\frac{\\begin{cases} Total\\;do\\;grupo\\;ST=115,74\\;(5\\;parcelas) \\\\ Total\\;do\\;grupo\\;(AC+AP)=238,29\\;(10\\;parcelas) \\end{cases}} {Total=354,03\\;(15\\;parcelas)} \\] Então, temos: \\[ SQ_{ST\\;vs.\\;(AC+AP)}=\\frac{115,74^2}{5}+\\frac{238,29^2}{10}-\\frac{354,03^2}{15}=1,5459 \\] c) Cálculo da soma de quadrados do componente AC vs. AP: \\[ \\frac{\\begin{cases} Total\\;do\\;grupo\\;AC=117,54\\;(5\\;parcelas) \\\\ Total\\;do\\;grupo\\;AP=120,75\\;(5\\;parcelas) \\end{cases}} {Total=238,29\\;(10\\;parcelas)} \\] Então, temos: \\[ SQ_{AC\\;vs.\\;AP}=\\frac{117,54^2}{5}+\\frac{120,75^2}{5}-\\frac{238,29^2}{10}=1,0304 \\] d) Cálculo da soma de quadrados do componente PM vs. T: \\[ \\frac{\\begin{cases} Total\\;do\\;grupo\\;PM=85,88\\;(5\\;parcelas) \\\\ Total\\;do\\;grupo\\;T=91,93\\;(5\\;parcelas) \\end{cases}} {Total=177,81\\;(10\\;parcelas)} \\] Então, temos: \\[ SQ_{PM\\;vs.\\;T}=\\frac{85,88^2}{5}+\\frac{91,93^2}{5}-\\frac{177,81^2}{10}=3,6603 \\] 16.5 Análise no R Uma vez obtidas as somas de quadrados, para cada componente do desdobramento, por qualquer dos dois métodos apresentados anteriormente, podemos montar o quadro de análise de variância com o desdobramento: Causas.de.Variação GL SQ QM F (ST+AC+AP) vs. (PM+T) 1 203,3042 203,3042 17,11** ST vs. (AC+AP) 1 1,5459 1,5459 0,13 AC vs AP 1 1,0304 1,0304 0,09 PM vs. T 1 3,6603 3,6603 0,31 (Tratamento) (209,5408)   Resíduo 20 237,5958 11,8798  Total 24 447,1366   Valores de F da tabela para (1 x 20 GL):\\(\\begin{cases} 5\\%=4.35 \\\\ 1\\%=8.10 \\end{cases}\\) Verificação: \\(\\frac{17,11+0,13+0,09+0,31}{4}=4,41=F_{Trat}\\) Conclusões 1. Para componente (ST+AC+AP) vs. (PM+T): O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que o grupo de tratamentos (ST+AC+AP) difere do grupo (PM+T) em relação à altura média de mudas de Eucalyptus, com grau de confiança superior a \\(99\\%\\) de probabilidade. 2. Para componente ST vs. (AC+AP): O teste F não foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\) e concluir que o grupo de tratamentos (ST) não difere do grupo (AC+AP) em relação à altura média de mudas de Eucalyptus. 3. Para componente AC vs. AP: O teste F não foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\) e concluir que o grupo de tratamentos (AC) não difere do grupo (AP) em relação à altura média de mudas de Eucalyptus. 4. Para componente PM vs. T: O teste F não foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\) e concluir que o grupo de tratamentos (PM) não difere do grupo (T) em relação à altura média de mudas de Eucalyptus. Cálculo do Coeficiente de Variação do experimento: \\[ \\hat{m} = \\frac{G}{IJ} = \\frac{531,84}{5\\cdot5} = 21,27\\;cm \\\\ s = \\sqrt{QM_{Res}}=\\sqrt{11,8798}=3,45\\;cm\\\\ \\] Portanto: \\[ CV=100\\cdot \\frac{s}{\\hat{m}} = 100\\cdot \\frac{3,45}{21,27}=16,22\\% \\] Resolvendo no R # Caminho dos dados caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/eucalyptus.txt&quot; # Carregando os dados no programa dados&lt;-read.table(caminho,h=T,sep=&quot;\\t&quot;) #Extraindo o fator tratamento e a variável resposta trat&lt;-as.factor(dados$TRAT) y&lt;-dados$Y # definindo o modelo para a análise de variância preliminar modelo&lt;-aov(y~trat) #Quadro da análise de variância preliminar anova(modelo) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trat 4 209.54 52.385 4.4096 0.01021 * ## Residuals 20 237.60 11.880 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Os coeficientes do contraste devem ser atribuídos de acordo com a ordem dos tratamentos definidos por: levels(trat) ## [1] &quot;1_ST&quot; &quot;2_AC&quot; &quot;3_AP&quot; &quot;4_PM&quot; &quot;5_T&quot; #Então os contrastes serão: contrastes&lt;-cbind( c(2,2,2,-3,-3), c(2,-1,-1,0,0), c(0,1,-1,0,0), c(0,0,0,1,-1) ) #Visualização dos contrastes contrastes ## [,1] [,2] [,3] [,4] ## [1,] 2 2 0 0 ## [2,] 2 -1 1 0 ## [3,] 2 -1 -1 0 ## [4,] -3 0 0 1 ## [5,] -3 0 0 -1 #Atribuição dos contrastes ao fator trat contrasts(trat)&lt;-contrastes #Verificação dos contrastes contrasts(trat) ## [,1] [,2] [,3] [,4] ## 1_ST 2 2 0 0 ## 2_AC 2 -1 1 0 ## 3_AP 2 -1 -1 0 ## 4_PM -3 0 0 1 ## 5_T -3 0 0 -1 #Definição do novo modelo para o desdobramento dos graus de liberdade modelo.contrastes&lt;-aov(y~trat) #Desdobramento dos graus de Liberdade summary(modelo.contrastes, split= list(trat= list(&quot;(ST+AC+AP) vs. (PM+T)&quot;= 1, &quot;ST vs. (AC+AP)&quot;= 2, &quot;AC vs. AP&quot;= 3, &quot;PM vs T&quot;= 4))) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trat 4 209.54 52.39 4.410 0.010205 * ## trat: (ST+AC+AP) vs. (PM+T) 1 203.30 203.30 17.113 0.000511 *** ## trat: ST vs. (AC+AP) 1 1.55 1.55 0.130 0.722083 ## trat: AC vs. AP 1 1.03 1.03 0.087 0.771403 ## trat: PM vs T 1 3.66 3.66 0.308 0.584999 ## Residuals 20 237.60 11.88 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Coeficiente de variação do ensaio media&lt;-mean(y) QMres&lt;-deviance(modelo)/df.residual(modelo) CV&lt;-100*sqrt(QMres)/media paste(round(CV,2),&quot;%&quot;,sep=&quot;&quot;) ## [1] &quot;16.2%&quot; 16.5.1 COMPONENTES COM MAIS DE UM GRAU DE LIBERDADE Em alguns casos, o experimentador pode estar interessado em realizar um desdobramento incompleto, como o seguinte: 1 - Inoculados (ST+AC+AP) vs. Testemunhas (PM+T).1 GL 2 - Entre inoculados (ST, AC, AP)2 GL 3 - Entre Testemunhas (PM vs. T)1 GL Nesse caso, a soma de quadrados correspondente ao componente com mais do que 1 grau de liberdade pode ser calculada pelo método dos totais de tratamentos, da seguinte maneira: \\[ \\frac{\\begin{cases} Total\\;de\\;ST=115,74\\;(5\\;parcelas) \\\\ Total\\;de\\;AC=117,54\\;(5\\;parcelas) \\\\ Total\\;de\\;AP=120,75\\;(5\\;parcelas) \\\\ \\end{cases}} {Total=354.03\\;(15\\;parcelas)} \\] Então, temos: \\[ SQ_{(Entre\\;Inoculantes)}=\\frac{115,74^2}{5}+\\frac{117,54^2}{5}+\\frac{120,75^2}{5}-\\frac{354,03^2}{15}=2,5763 \\] Assim, podemos montar o seguinte quadro de análise de variância: Causas.de.Variação GL SQ QM F (ST+AC+AP) vs. (PM+T) 1 203,3042 203,3042 17,11** Entre Inoculantes (ST, AC, AP) 2 2,5763 1,2882 0,11 PM vs. T 1 3,6603 3,6603 0,31 (Tratamento) 4 (209,5408)   Resíduo 20 237,5958 11,8798  Total 24 447,1366   # Criando, novamente o fator tratamento. trat&lt;-as.factor(dados$TRAT) contrasts(trat) ## 2_AC 3_AP 4_PM 5_T ## 1_ST 0 0 0 0 ## 2_AC 1 0 0 0 ## 3_AP 0 1 0 0 ## 4_PM 0 0 1 0 ## 5_T 0 0 0 1 #Então os contrastes serão: contrastes&lt;-cbind( c(2,2,2,-3,-3), c(0,1,0,0,0), c(0,0,1,0,0), c(0,0,0,1,-1) ) #Atribuição dos contrastes ao fator trat contrasts(trat)&lt;-contrastes #Verificação dos contrastes contrasts(trat) ## [,1] [,2] [,3] [,4] ## 1_ST 2 0 0 0 ## 2_AC 2 1 0 0 ## 3_AP 2 0 1 0 ## 4_PM -3 0 0 1 ## 5_T -3 0 0 -1 #Definição do novo modelo para o desdobramento dos graus de liberdade modelo.contrastes&lt;-aov(y~trat) #Desdobramento dos graus de Liberdade summary(modelo.contrastes, split= list(trat= list(&quot;(ST+AC+AP) vs. (PM+T)&quot;= 1, &quot;Entre Inoculantes (AC, AP, ST)&quot;= c(2,3), &quot;PM vs T&quot;= 4))) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trat 4 209.54 52.39 4.410 0.010205 * ## trat: (ST+AC+AP) vs. (PM+T) 1 203.30 203.30 17.113 0.000511 *** ## trat: Entre Inoculantes (AC, AP, ST) 2 2.58 1.29 0.108 0.897764 ## trat: PM vs T 1 3.66 3.66 0.308 0.584999 ## Residuals 20 237.60 11.88 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["delineamento-em-blocos-casualizados.html", "17 Delineamento em Blocos Casualizados 17.1 Caracterização 17.2 Modelo matemático 17.3 Hipóteses básicas 17.4 Obtenção da análise de variância 17.5 Exemplo de aplicação 17.6 Delineamento em blocos casualizados com uma parcela perdida 17.7 Aplicação no R.", " 17 Delineamento em Blocos Casualizados 17.1 Caracterização O delineamento em blocos casualizados (DBC) é o mais utilizado dos delineamentos experimentais, e os experimentos instalados de acordo com esse delineamento são denominados de experimentos em blocos casualizados ou experimentos em blocos ao acaso. Além dos princípios da repetição e da casualização, leva em conta também o princípio do controle local. Sempre que houver dúvidas a respeito da homogeneidade das condições experimentais é conveniente usar o princípio do controle local, estabelecendo blocos com parcelas homogêneas. As principais características deste delineamento são: As parcelas são distribuídas em grupos ou blocos (princípio do controle local) de tal forma que elas sejam o mais uniforme possível dentro de cada bloco. O número de parcelas por bloco deve ser igual ao número de tratamentos (blocos completos casualizados). Os tratamentos são designados às parcelas de forma casual, sendo essa casualização feita dentro de cada bloco. Esse delineamento é mais eficiente que o delineamento inteiramente casualizado, e essa eficiência depende da uniformidade das parcelas dentro de cada bloco, podendo inclusive, haver diferenças acentuadas de um bloco para outro. Então, por exemplo, se temos um experimento em blocos casualizados em que desejamos estudar o efeito de \\(4\\) variedades (V1, V2, V3 e V4), sendo cada uma delas repetidas \\(5\\) vezes, teremos o seguinte plano experimental: Note que dentro de cada bloco temos as variedades sorteadas ao acaso. Caso este mesmo ensaio fosse montado no delineamento inteiramente casualizados (DIC), o sorteio seria feito em todas as parcelas do experimento, e os tratamentos não seriam agrupados em blocos. Por exemplo, no delineamento inteiramente casualizado teríamos: E eficiência do delineamento em blocos casualizados depende de uniformidade das parcelas dentro de cada bloco. De um bloco para outro, pode haver variação. A forma e a distribuição dos blocos no campo depende apenas da uniformidade das condições experimentais. Os blocos podem ser retangulares, quadrados ou irregulares: Os blocos podem ser espalhados em toda a área de estudo, ou então agrupados um ao lado do outro. Geralmente se coloca um bloco ao lado do outro para facilitar a condução do experimento. Os blocos são utilizados para controlar pequenas diferenças que ocorrem entre as parcelas experimentais, tais como, diferenças de fertilidade do solo, diferenças de disponibilidade de água, diferenças de luminosidade (no caso de casas de vegetação), etc. As principais vantagens desse delineamento são: Controla as diferenças que ocorrem nas condições experimentais de um bloco para outro. Permite, dentro de certos limites, utilizar qualquer número de tratamentos e blocos. Nos conduz a uma estimativa mais exata para a variância residual. A análise de variância é relativamente simples, sendo apenas um pouco mais demorada que a do delineamento inteiramente casualizado, pois possui uma causa de variação a mais (Blocos). As principais desvantagens desse delineamento são: Pela utilização do princípio do controle local há uma diminuição no número de graus de liberdade do resíduo. A exigência de homogeneidade dentro do bloco limita o número de tratamentos, que não pode ser muito grande. 17.2 Modelo matemático Para podermos analisar um experimento em qualquer delineamento, necessitamos conhecer o modelo matemático do mesmo, e aceitar algumas hipóteses básicas necessárias para a validade da análise de variância. O modelo matemático do delineamento em blocos casualizados é o seguinte: \\[ x_{ij} = \\mu +\\tau_i + \\beta_j+\\epsilon_{ij} \\] onde, \\(x_{ij}\\) representa o valor esperado na parcela que recebeu o tratamento \\(i\\) e que se encontra no bloco \\(j\\). \\(\\mu\\) é a média geral do experimento. \\(\\tau_i\\) é o efeito devido ao tratamento i que foi aplicado à parcela. \\(\\beta_j\\) é o efeito devido ao bloco j em que se encontra a parcela. \\(\\epsilon_{ij}\\) é o efeito dos fatores não controlados ou acaso na parcela que recebeu o tratamento i e que se encontra no bloco j. 17.3 Hipóteses básicas As hipóteses básicas para a validade da análise de variância são as mesmas já vistas para o delineamento inteiramente casualizado, ou seja: Aditividade: Os efeitos dos fatores do modelo são aditivos Independência: Os erros (desvios) \\(\\epsilon_{ij}\\), devido aos efeitos de fatores não controlados ou acaso devem ser independentes Homocedasticidade (homogeneidade de variâncias): Os erros (desvios) \\(\\epsilon_{ij}\\), devido ao acaso devem possuir variância comum \\(\\sigma^2\\) Normalidade dos desvios: Os erros ou desvios \\(\\epsilon_{ij}\\) devido ao acaso devem possuir distribuição normal de probabilidades. Uma forma resumida de apresentar estas quatro pressuposições é apresentada a seguir: \\[ \\epsilon_{ij} \\overset{iid}{\\tilde{}} N(0,\\sigma^2) \\\\ \\] Os erros ou desvios \\(\\epsilon_{ij}\\) são independentes e identicamente distribuídos, de acordo com uma distribuição normal, com média zero e variância comum \\(\\sigma^2\\). 17.4 Obtenção da análise de variância Para a obtenção da análise de variância, vamos considerar um experimento genérico em blocos casualizados com \\(I\\) tratamentos e \\(J\\) blocos. Os dados obtidos podem ser dispostos num quadro como o seguinte: Tratamentos 1 2  \\(j\\)  \\(J\\) Total 1 \\(x_{11}\\) \\(x_{12}\\)  \\(x_{1j}\\)  \\(x_{1J}\\) \\(T_1=\\sum_{j=1}^Jx_{1j}\\) 2 \\(x_{21}\\) \\(x_{22}\\)  \\(x_{2j}\\)  \\(x_{2J}\\) \\(T_2=\\sum_{j=1}^Jx_{2j}\\)    \\(i\\) \\(x_{i1}\\) \\(x_{i2}\\)  \\(x_{ij}\\)  \\(x_{iJ}\\) \\(T_i=\\sum_{j=1}^Jx_{ij}\\)    \\(I\\) \\(x_{I1}\\) \\(x_{I2}\\)  \\(x_{Ij}\\)  \\(x_{IJ}\\) \\(T_I=\\sum_{j=1}^Jx_{Ij}\\) Total \\(B_1=\\sum_{i=1}^Ix_{i1}\\) \\(B_2=\\sum_{i=1}^Ix_{i2}\\)  \\(B_j=\\sum_{i=1}^Ix_{ij}\\)  \\(B_J=\\sum_{i=1}^Ix_{iJ}\\) \\(G=\\sum_{i=1}^I\\sum_{j=1}^Jx_{ij}\\) Pelo método dos mínimos quadrados, devemos obter as estimativas de \\(\\mu\\), \\(\\tau_i\\) e \\(\\beta_j\\) que minimizam a soma de quadrados dos erros do modelo: \\[ x_{ij} = \\mu +\\tau_i + \\beta_j+\\epsilon_{ij} \\] Então, temos: \\[ \\hat{m} = \\frac{\\sum_{i=1}^I\\sum_{j=1}^Jx_{ij}}{IJ}=\\frac{G}{IJ} \\\\ \\hat{t_i}=\\frac{T_i}{J}-\\hat{m}=\\hat{m_i}-\\hat{m} \\\\ \\hat{b_j}=\\frac{B_j}{I}-\\hat{m}=\\hat{m_j}-\\hat{m} \\] Podemos agora obter as somas de quadrados para cada um desses efeitos. As somas de quadrados total e devido ao efeito de tratamentos são obtidas da mesma maneira que no delineamento inteiramente casualizado: \\[ SQ_{Total}=\\sum_{i=1}^{I}\\sum_{j=1}^Jx_{ij}-C\\\\ SQ_{Trat}=\\frac{1}{J}\\sum_{i=1}^IT_i^2-C \\\\ onde,\\; C=\\frac{G^2}{IJ} \\] A soma de quadrados devido ao efeito de blocos é obtida por: \\[ SQ_{Blocos} = I\\hat{b_1^2}+I\\hat{b_2^2}+...+I\\hat{b_J^2} \\\\ SQ_{Blocos} = I(\\hat{b_1^2}+\\hat{b_2^2}+...+\\hat{b_J^2})\\\\ SQ_{Blocos} = I\\left[ \\left(\\frac{B_1}{I}-\\hat{m} \\right)^2+\\left( \\frac{B_2}{I}-\\hat{m} \\right)^2+...+\\left( \\frac{B_J}{I}-\\hat{m} \\right)^2 \\right]\\\\ SQ_{Blocos} = I\\left[ \\left(\\frac{B_1^2}{I^2}-2\\hat{m}\\frac{B_1}{I}+\\hat{m}^2 \\right)+\\left(\\frac{B_2^2}{I^2}-2\\hat{m}\\frac{B_2}{I}+\\hat{m}^2 \\right)+...+\\left( \\frac{B_J^2}{I^2}-2\\hat{m}\\frac{B_J}{I}+\\hat{m^2}\\right) \\right] \\\\ SQ_{Blocos} =\\left[\\left(\\frac{B_1^2}{I}+\\frac{B_2^2}{I}+...+\\frac{B_J^2}{I}\\right)-2\\hat{m}(B_1+B_2+...+B_J)+ IJ\\hat{m}^2 \\right] \\\\ SQ_{Blocos} = \\frac{1}{I}(B_1^2+B_2^2+...+B_J^2)-2\\hat{m}G+IJ\\frac{G^2}{(IJ)^2}\\\\ SQ_{Blocos} =\\frac{1}{I}\\sum_{j=1}^JB_j^2-C \\] A soma de quadrados devido ao efeito dos fatores não controlados ou acaso (\\(SQ_{Resíduo}\\)), é obtida por diferença: \\[ SQ_{Resíduo} = SQ_{Total} - SQ_{Trat} - SQ_{Blocos} \\] Causas de Variação GL SQ QM F Tratamentos \\(I-1\\) \\(SQ_{Trat}\\) \\(SQ_{Trat}\\)/(I-1) \\(QM_{Trat}\\)/\\(QM_{Res}\\) Blocos \\(J-1\\) \\(SQ_{Blocos}\\) \\(SQ_{Blocos}\\)/(J-1) \\(QM_{Blocos}\\)/\\(QM_{Res}\\) Resíduo \\((I-1)(J-1)\\) \\(SQ_{Res}\\) \\(SQ_{Res}\\)/[(I-1)(J-1)] Total \\(IJ-1\\) \\(SQ_{Total}\\) HIPÓTESES TESTADAS: Para Tratamentos: \\[ \\begin{cases} H_0: t_i=0, i=1,2, ...,I. \\\\ H_1:pelo\\;menos\\;um \\;valor\\;t_i \\neq 0 \\end{cases} \\] Para Blocos: \\[ \\begin{cases} H_0: \\beta_j=0, j=1,2, ...,J. \\\\ H_1:pelo\\;menos\\;um \\;valor\\;\\beta_j \\neq 0 \\end{cases} \\] CRITÉRIO DO TESTE: Para Tratamentos: Comparamos o valor \\(F\\) calculado para tratamentos com o valor de \\(F\\) tabelado em função do número de GL de Tratamentos e GL do resíduo, ao nível \\(\\alpha\\) de significância. Se \\(F_{Trat} &gt; F_{Tab}\\), concluímos que o teste é significativo, portanto, rejeitamos \\(H_0\\) e devemos concluir que existe diferença significativa entre os efeitos dos tratamentos testados em relação à variáveis (característica) em estudo. Para Blocos: Pra Blocos, a comparação é feita entre o valor de \\(F\\) calculado com o \\(F\\) tabelado em função do número de GL de Blocos e GL do resíduo, ao nível \\(\\alpha\\) de significância. Se \\(F_{Blocos} &gt; F_{Tab}\\), concluímos que o teste é significativo, portanto, rejeitamos \\(H_0\\) e devemos concluir que os blocos possuem efeitos diferentes em relação à característica em estudo, ou seja, os blocos foram eficientes no controle da heterogeneidade local. 17.5 Exemplo de aplicação No trabalho Influência do genótipo e da adubação sobre algumas características fenotípicas de Zea mays L (Milho), realizado por BARBOSA (1976), foram utilizadas 4 cultivares de milho: \\(C_1\\) = OPACO 2 \\(C_2\\) = PIRANÃO \\(C_3\\) = COMPOSTO FLINT \\(C_4\\) = AGROCERES AG-152 O ensaio foi montado de acordo com o delineamento em blocos casualizados, sendo utilizados \\(5\\) blocos para controlar as diferenças de fertilidade do solo entre terraços. Os resultados obtidos para a produção em kg/ha, foram os seguintes e podem ser encontrados online em: milho. Tratamentos Bloco1 Bloco2 Bloco3 Bloco4 Bloco5 Total OPACO2 2812 2296 3501 3301 3691 15601 PIRANAO 3728 3588 4418 4544 5084 21362 COMP.FLINT 5359 5106 7477 8007 7956 33905 AG152 4482 4510 5236 5930 5025 25183 Total 16381 15500 20632 21782 21756 96051 Dados originais:DOWNLOAD As hipóteses que desejamos testar são as seguintes: \\[ \\begin{cases} H_0: As\\;cultivares\\;testadas\\;não\\;diferem\\;entre\\;si\\;em\\;relação\\;à\\;produção\\;da\\;cultura\\;do\\;milho. \\\\ H_1: As\\;cultivares\\;testadas\\;diferem\\;entre\\;si\\;em\\;relação\\;à\\;produção\\;da\\;cultura\\;do\\;milho. \\end{cases} \\] Para obtenção da análise de variância, devemos, inicialmente, obter as Somas de Quadrados: a) Soma de quadrados total: \\[ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jx_{ij}^2 - C \\\\ SQ_{Total} = \\sum_{i=1}^I\\sum_{j=i}^Jx_{ij}^2 - \\frac{G^2}{IJ} \\\\ SQ_{Total} = (2812^2+2296^2+...+3501^2) - \\frac{96051^2}{4\\cdot5}\\\\ SQ_{Total} = 47817032,90 \\] b) Soma de quadrados devido ao efeito de tratamentos: \\[ SQ_{Trat} = \\frac{\\sum_{i=1}^IT_i^2}{J} - C \\\\ SQ_{Trat} = \\frac{T_1^2+T_2^2+...+T_I^2}{J} - \\frac{G^2}{IJ}\\\\ SQ_{Trat} = \\frac{15601^2+21362^2+33905^2+25183^2}{5} - \\frac{96051^2}{4\\cdot5}\\\\ SQ_{Trat}= 35402021,70 \\] c) Soma de quadrados devido ao efeito de blocos: \\[ SQ_{Blocos} = \\frac{\\sum_{j=1}^JB_j^2}{I} - C \\\\ SQ_{Blocos} = \\frac{B_1^2+B_2^2+...+B_J^2}{I} - \\frac{G^2}{IJ}\\\\ SQ_{Blocos} = \\frac{16381^2+15500^2+20632^2+21782^2+21756^2}{4} - \\frac{96051^2}{4\\cdot5}\\\\ SQ_{Blocos}= 9221681,20 \\] d) Soma de quadrados do resíduo: \\[ SQ_{Res} = SQ_{Total} - SQ_{Trat} - SQ_{Blocos}\\\\ SQ_{Res} = 47817032,9-35402021,7-9221681,2=3193330,00 \\] Então podemos montar o seguinte quadro de análise de variância: Causas.de.Variação GL SQ QM F Tratamento 3 35402021,7 11800673,9 44,34** Blocos 4 9221681,2 2305420,3 8,66** Resíduo 12 3193330,00 266110,83  Total 19 47817032,9   Valores de F da tabela para tratamentos (3 x 12 GL):\\(\\begin{cases} 5\\%=3,49 \\\\ 1\\%=5,95 \\end{cases}\\) Valores de F da tabela para blocos (4 x 12 GL):\\(\\begin{cases} 5\\%=3,26 \\\\ 1\\%=5,41 \\end{cases}\\) Conclusões: a) Para Tratamentos: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que as cultivares diferem entre si em relação à produção da cultura do milho, com um grau de confiança superior a \\(99\\%\\) de probabilidade. b) Para Blocos: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que as diferenças de fertilidade do solo entre terraços apresentaram efeitos diferentes sobre a produção do milho, com um grau de confiança superior a \\(99\\%\\) de probabilidade. Para tirar conclusões mais específicas sobre o comportamento das cultivares, devemos utilizar um teste de comparação de médias. a) Cálculo das médias de cada tratamento e erros padrões das médias: Então, no nosso exemplo, temos: \\[ \\hat{m_1} = \\frac{T_1}{J}=\\frac{15601}{5}=3120,20\\;kg/ha\\\\ \\hat{m_2} = \\frac{T_2}{J}=\\frac{21362}{5}=4272,40\\;kg/ha\\\\ \\hat{m_3} = \\frac{T_3}{J}=\\frac{33905}{5}=6781,00\\;kg/ha\\\\ \\hat{m_4} = \\frac{T_4}{J}=\\frac{25183}{5}=5036,60\\;kg/ha\\\\ \\] Erro padrão dessas médias será: \\[ s(\\hat{m}) = \\frac{s}{\\sqrt{J}}=\\sqrt{\\frac{QM_{Res}}{J}} =\\sqrt{\\frac{266110,83}{5}}=230,70\\;kg/ha \\] b) Aplicação do teste de Tukey : b.1. Cálculo do valor de \\(\\Delta\\): \\[ q_{(4\\;trat\\times12\\;GLres)}(5\\%) = 4,20\\\\ s=\\sqrt{QM_{Res}}=515,8593,\\;com\\; r=5 \\\\ DMS=q\\cdot\\sqrt{\\frac{QM_{Res}}{r}}=4,20\\cdot\\sqrt{\\frac{266110,83}{5}}=968,94\\;kg/ha \\] b.2. Cálculo das estimativas dos contrastes entre duas médias: \\(\\hat{m_3}\\) \\(\\hat{m_4}\\) \\(\\hat{m_2}\\) \\(\\hat{m_1}\\) \\(\\hat{m_3}\\) - 1744,4* 2508,60* 3660,80* \\(\\hat{m_4}\\) - - 764,20 1916,40* \\(\\hat{m_2}\\) - - - 1152,20* b.3. Conclusão: Médias seguidas de pela mesma letra não diferem entre si pelo teste de Tukey, ao nível de 5% de probabilidade. \\[ \\hat{m_3} = 6781,00\\;kg/ha\\;\\;\\;\\;-a\\\\ \\hat{m_4} = 5036,60\\;kg/ha\\;\\;\\;\\;-b\\\\ \\hat{m_2} = 4272,40\\;kg/ha\\;\\;\\;\\;-b\\\\ \\hat{m_1} = 3120,20\\;kg/ha\\;\\;\\;\\;-c\\\\ \\] Portanto, a melhor cultivar foi o Composto Flint, pois diferiu das demais pelo teste de Tukey e apresentou maior produtividade. c) Cálculo do coeficiente de variação do experimento: \\[ \\hat{m}=\\frac{G}{IJ}=\\frac{96051}{4\\cdot5}=4802,55\\;kg/ha\\\\ s=\\sqrt{QM_{Res}}=\\sqrt{226110,83}=515,8593\\;kg/ha\\\\ CV=\\frac{100\\cdot s}{\\hat{m}}=\\frac{100\\cdot515,8593}{4802,55}=10,74\\% \\] Aplicação no R caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/milho.txt&quot; dados&lt;-read.table(caminho,h=T,sep=&quot;\\t&quot;) head(dados) ## trat bloco y ## 1 OPACO2 1 2812 ## 2 OPACO2 2 2296 ## 3 OPACO2 3 3501 ## 4 OPACO2 4 3301 ## 5 OPACO2 5 3691 ## 6 PIRANAO 1 3728 # Extraindo os fatores e a variável resposta trat&lt;-as.factor(dados$trat) bloco&lt;-as.factor(dados$bloco) y&lt;-dados$y # Definindo o modelo matemático modelo&lt;-aov(y~trat+bloco) anova(modelo) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trat 3 35402022 11800674 44.3450 9.068e-07 *** ## bloco 4 9221681 2305420 8.6634 0.00158 ** ## Residuals 12 3193330 266111 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #Comparação de Médias pelo teste de Tukey require(&quot;agricolae&quot;) glRes&lt;-df.residual(modelo) QMres&lt;-deviance(modelo)/glRes tukey &lt;- HSD.test(modelo,&quot;trat&quot;, group=TRUE,console=TRUE) ## ## Study: modelo ~ &quot;trat&quot; ## ## HSD Test for y ## ## Mean Square Error: 266110.8 ## ## trat, means ## ## y std r Min Max ## AG152 5036.6 596.4368 5 4482 5930 ## COMP.FLINT 6781.0 1431.4177 5 5106 8007 ## OPACO2 3120.2 565.1997 5 2296 3691 ## PIRANAO 4272.4 616.1240 5 3588 5084 ## ## Alpha: 0.05 ; DF Error: 12 ## Critical Value of Studentized Range: 4.19866 ## ## Minimun Significant Difference: 968.628 ## ## Treatments with the same letter are not significantly different. ## ## y groups ## COMP.FLINT 6781.0 a ## AG152 5036.6 b ## PIRANAO 4272.4 b ## OPACO2 3120.2 c bar.group(tukey$groups, las=1, ylim=c(0,max(y)*1.10), xlab=&quot;Cultivares&quot;, ylab=&quot;Produção (kg por ha)&quot;, main=&quot;Teste de Tukey (5%)&quot;);box() # Cálculo do CV cv&lt;-100*sqrt(QMres)/mean(y) paste(round(cv,2),&quot;%&quot;,sep=&quot;&quot;) ## [1] &quot;10.74%&quot; Utilizando o pacote {ExpDes.pt} # Carregando o pacote para a análise library(ExpDes.pt) # verificando os 6 primeiros registros head(dados) ## trat bloco y ## 1 OPACO2 1 2812 ## 2 OPACO2 2 2296 ## 3 OPACO2 3 3501 ## 4 OPACO2 4 3301 ## 5 OPACO2 5 3691 ## 6 PIRANAO 1 3728 # Análise de variância e teste de Tukey com a função dbc trat &lt;- dados$trat # Criando o vetor de tratamentos bloco &lt;- dados$bloco # Criando o vetor dos blocos y &lt;- dados$y # Criando o vetor com a variável resposta # Utilizando a função dbc(trat,bloco,y,mcomp = &quot;tukey&quot;) ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Tratamento 3 35402022 11800674 44.345 0.00000091 ## Bloco 4 9221681 2305420 8.663 0.00158020 ## Residuo 12 3193330 266111 ## Total 19 47817033 ## ------------------------------------------------------------------------ ## CV = 10.74 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos ## valor-p: 0.1786606 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## Teste de homogeneidade de variancia ## valor-p: 0.03461775 ## ATENCAO: a 5% de significancia, as variancias nao podem ser consideradas homogeneas! ## ------------------------------------------------------------------------ ## ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a COMP.FLINT 6781 ## b AG152 5036.6 ## b PIRANAO 4272.4 ## c OPACO2 3120.2 ## ------------------------------------------------------------------------ 17.6 Delineamento em blocos casualizados com uma parcela perdida 17.6.1 INTRODUÇÃO O Delineamento em blocos casualizados é balanceado de tal forma que todos os blocos possuem todos os tratamentos. Assim, se ocorrer uma perda de parcela, há uma quebra neste balanceamento, pois se, por exemplo, perdermos a parcela \\(x_{ij}\\) o tratamento \\(i\\) não conterá o efeito do bloco \\(j\\). Essa quebra de balaceamento leva a sérias complicações no método de análise de variância. Um método de facilitar a análise consiste em obter uma estimativa da parcela perdida. Esta estimativa não representa o valor que seria obtido, pois ningém pode saber qua seria este valor, mas é um artifício de cálculo bastante simple que conduz ao mesmo resultado que se chegaria, por processos mais complexos, considerando apenas os dados realmente obtidos (blocos incompletos). 17.6.2 ESTIMATIVA DA PARCELA PERDIDA Blocos Tratamentos 1 2  \\(j\\)  \\(J\\) Total 1 \\(x_{11}\\) \\(x_{12}\\)  \\(x_{1j}\\)  \\(x_{1J}\\) \\(T_1\\) 2 \\(x_{21}\\) \\(x_{22}\\)  \\(x_{2j}\\)  \\(x_{2J}\\) \\(T_2\\)    \\(i\\) \\(x_{i1}\\) \\(x_{i2}\\)  \\(x_{ij} = NA\\)  \\(x_{iJ}\\) \\(T+x_{ij}\\)    \\(I\\) \\(x_{I1}\\) \\(x_{I2}\\)  \\(x_{Ij}\\)  \\(x_{IJ}\\) \\(T_I\\) Total \\(B_1\\) \\(B_2\\)  \\(B+x_{ij}\\)  \\(B_J\\) \\(G&#39;+x_{ij}\\) NA = é a parcela perdida De acordo com o modelo matemático, temos: \\[ x_{ij}= \\mu+\\tau_{i}+\\beta_{j}+\\epsilon_{ij} \\] A estimativa da parcela perdida é obtida tornando mínimo o efeito dos fatores não contralados ou acaso na parcela. Então, utilizando o método dos quadrados mínimos, obtém-se: \\[ x_{ij} = \\frac{IT+JB-G&#39;}{(I-1)(J-1)} \\] onde \\(I\\) é o número de tratamentos do experimento. \\(J\\) é o número de blocos do experimento. \\(T\\) é a soma das parcelas existentes no tratamento que perdeu a parcela. \\(B\\) é a soma das parcelas existentes no bloco que perdeu a parcela. \\(G&#39;\\) é a soma das parcelas existentes no experimento. 17.6.3 OBTENÇÃO DA ANÁLISE DE VARIÂNCIA Uma vez obtida a estimativa da parcela perdida, substituímos o seu valor no quadro de dados, e calculamos as somas de quadrados da maneira usual. Então, com o valor estimado da parcela perdidada obtemos: \\(T_{i}=T+x_{ij}\\) \\(B_{j}=B+x_{ij}\\) \\(G=G&#39;+x_{ij}\\) \\(C=\\frac{G^2}{IJ}\\) \\(SQ_{Total}=\\sum_{i=1}^I\\sum_{j=1}^Jx_{ij}^2-C\\) \\(SQ_{Trat}=\\frac{1}{J}\\sum_{i=1}^IT_{i}^2-C\\) \\(SQ_{Blocos}=\\frac{1}{I}\\sum_{j=1}^JB_{j}^2-C\\) \\(SQ_{Res}=SQ_{Total}-SQ_{Trat}-SQ_{Blocos}\\) O método dos mínimos quadrados torna mínima a soma de quadrados do resíduo, a qual fica corretamente estimada, porém, causa uma superestimação na soma de quadrados de tratamentos e de blocos. Então, a soma de quadrados de tratamentos deve ser ajustada. Para isso, devemos calcular o fator de correção, representado por \\(U\\), e dado por: \\[ U_{T}=F.C. =\\frac{I-1}{I}\\left(x_{ij}-\\frac{B}{I-1}\\right)^2 \\] Onde: \\(x_{ij}\\) é a estimativa da parcela perdida. \\(I\\) é o número de tratamentos do experimento. \\(B\\) é a soma dos parcelas existentes no bloco que perdeu a parcela. E a soma de quadrados de tratamentos ajustada será: \\(SQ_{Trat(Aj.)}=SQ_{Trat}-U\\) Embora, na prática não seja necessário, caso haja interesse, pode-se também, fazer a correção da soma de quadrado de blocos por meio da expressão: \\(SQ_{Blocos(Aj.)}=SQ_{Blocos}-U_B\\) Onde: \\[ U_B=\\frac{J-1}{J}\\left(x_{ij}-\\frac{T}{J-1}\\right)^2 \\] A seguir montamos o quadro de análise de variância, lembrando que há uma perda de um grau de liberdade para o total e para o resíduo, devido à estimativa da parcela perdida. Então, o quadro de análise de variância será: Causas de Variação GL SQ QM F Tratamentos (Ajust.) \\(I-1\\) \\(SQ_{Trat}-U\\) \\(SQ_{Trat(Aj.)}\\)/(I-1) \\(QM_{Trat}\\)/\\(QM_{Res}\\) Blocos \\(J-1\\) \\(SQ_{Blocos}-U_B\\) \\(SQ_{Blocos}\\)/(J-1) \\(QM_{Blocos}\\)/\\(QM_{Res}\\) Resíduo \\((I-1)(J-1)-1\\) \\(SQ_{Res}\\) \\(SQ_{Res}\\)/[(I-1)(J-1)-1] Total \\(IJ-2\\)  Os valores de F calculados são comparados com os valores de F tabelados para se verificar se o teste é significativo ou não. 17.6.4 MÉDIAS DE TRATAMENTOS As médias de tratamentos são obtidas de maneiral usual, ou seja: \\(\\hat{m}_k=\\frac{T_k}{J}\\) para os tratamentos que não perderam parcela. \\(\\hat{m}_i=\\frac{T+x_{ij}}{J}\\) para o tratamento que perdeu a parcela. 17.6.5 ERROS PADRÕES DAS MÉDIAS DE TRATAMENTOS a) Para as médias dos tratamentos que não perderam parcela. \\[ s(\\hat{m})=\\frac{s}{\\sqrt{J}}=\\sqrt{\\frac{QM_{Res}}{J}} \\] b) Para a média do tratamento que perdeu parcela. \\[ s(\\hat{m})= \\sqrt{\\hat{V}(\\hat{m})},\\; onde \\\\ \\hat{V}(\\hat{m}) = \\left[ \\frac{1}{J}+\\frac{I}{J(J-1)(I-1)} \\right]s^2 \\] 17.6.6 COMPARAÇÃO DAS MÉDIAS PARA O TESTE DE TUKEY Temos dois casos a considerar; a) Comparação entre médias dos tratamentos sem parcela perdida: \\[ \\hat{Y} = \\hat{m}_k-\\hat{m}_{k&#39;} \\\\ \\hat{V}(\\hat{Y})=\\frac{2}{J}s^2 \\] Então, para a aplicação do teste de Tukey, temos: \\[ \\Delta = dms=q\\sqrt{\\frac{1}{2}\\cdot\\frac{2}{J}s^2}=q\\frac{s}{\\sqrt{J}} \\] b) Comparações entre as médias dos tratamentos sem parcela perdida \\((k)\\) com média do tratamento que perdeu parcela \\((i)\\): \\[ \\hat{Y} = \\hat{m}_k-\\hat{m}_{i} \\\\ \\hat{V}(\\hat{Y})=\\left[\\frac{2}{J}+\\frac{I}{J(I-1)(J-1)} \\right]s^2 \\] então, para a aplicação do teste de Tukey, temos: \\[ \\Delta = dms = q\\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y})} \\] 17.6.7 EXEMPLO DE APLICAÇÃO Para exemplificar a análise de variância de um experimento em blocos casualizados com parcela perdida, vamos analisar os dados obtidos pelo trabalho intitulado Estudos dos Efeitos do Promalin sobre frutos de macieiras (Malus spp) cultivares Brasil e Rainha, realizado por MESTRINER (1980). Foram utiizados 5 tratamentos: \\(T_1\\) - 12.5 ppm de promalin em plena floração \\(T_2\\) - 25.0 ppm de promalin em plena floração \\(T_3\\) - 50.0 ppm de promalin em plena floração \\(T_4\\) - 12.5 ppm de promalin em plena floração + 12.5 ppm de promalin no início da frutificação \\(T_5\\) - Testemunha Os resultados obtidos para peso médio de frutos da macieira, em gramas, foram os seguintes, e podem ser encontrados online em macieira. Tratamentos Bloco1 Bloco2 Bloco3 Bloco4 Total T1 142.36 144.78 145.19 138.88 571.21 T2 139.28 137.77 144.44 130.61 552.10 T3 140.73 134.06 136.07 144.11 554.97 T4 150.88 135.83 136.97 136.36 560.04 T5 153.49 NA 151.75 150.22 455.46 Total 726.74 552.44 714.42 700.18 2693.78 Em vermelho descamos o tratamento e o bloco que perderam a parcela. Dados originais:DOWNLOAD As hipóteses que desejamos testar são: \\[ \\begin{cases} H_0:\\;As\\;doses\\;de\\;Promalin\\;não\\;influenciam\\;no\\;peso\\;médio\\;de\\;frutos\\;de\\;macieira\\\\ H_1: \\;As\\;doses\\;de\\;Promalin\\;possuem\\;efeitos \\;diferentes\\;sobre\\;o\\;peso\\;médio\\;de\\;frutos\\;de\\;macieira \\end{cases} \\] a) estimativa da parcela perdida \\[ x_{ij} = \\frac{IT+JB-G&#39;}{(I-1)(J-1)} \\\\ x_{52} = \\frac{5\\cdot455,46+4\\cdot552,44-2693,78}{(5-1)(4-1)} = 149,44\\;g \\] A seguir, colocamos esse valor no quado de dados, e recalculamos os totais: \\(T_{5}=T+x_{52}=455,46+149,44=604,90\\) \\(B_{2}=B+x_{52}=552,44+149,44=701,88\\) \\(G=G&#39;+x_{52}=2693,78+149,44=2843,22\\) Tratamentos Bloco1 Bloco2 Bloco3 Bloco4 Total T1 142.36 144.78 145.19 138.88 571.21 T2 139.28 137.77 144.44 130.61 552.10 T3 140.73 134.06 136.07 144.11 554.97 T4 150.88 135.83 136.97 136.36 560.04 T5 153.49 149.44 151.75 150.22 604.90 Total 726.74 701.88 714.42 700.18 2843.22 b) Cálculo das somas de Quadrados b.1) Soma de Quadrados Total: \\[ SQ_{Total}=\\sum_{i=1}^I \\sum_{j=1}^J x_{ij}^2 - C \\\\ SQ_{Total}=[142,36^2+\\cdots+149,44^2+\\cdots+150,22^2]-\\frac{2843,22^2}{5\\cdot4} \\\\ SQ_{Total}=816,5390 \\] b.2) Soma de Quadrados de Tratamentos: \\[ SQ_{Trat}=\\frac{1}{J}\\sum_{i=1}^IT_{i}^2-C \\\\ SQ_{Trat}=\\frac{1}{4}[571,21^2+\\cdots+604,9^2]-\\frac{2843,22^2}{5\\cdot4}=463,9483 \\] b.3) Soma de Quadrados de Blocos: \\[ SQ_{Blocos}=\\frac{1}{I}\\sum_{j=1}^JB_{j}^2-C \\\\ SQ_{Blocos}=\\frac{1}{5}[726,74^2+701,88^2+714,42^2+700,18^2]-\\frac{2843,22^2}{5\\cdot4}=91,9078 \\] b.4) Soma de Quadrados de Resíduos: \\[ SQ_{Res}=SQ_{total}-SQ_{Trat}-SQ_{Blocos} \\\\ SQ_{Res}=816,5390-463,9483-91,9078=260,6829 \\] c) Correção da soma de quadrados de tratamentos: \\[ U_{T}=\\frac{I-1}{I}\\left(x_{ij}-\\frac{B}{I-1}\\right)^2 \\\\ U_{T}=\\frac{5-1}{5}\\left(149,44-\\frac{455,46}{4-1}\\right)^2=102,6951 \\] Portanto, temos: \\(SQ_{Trat(Aj.)}=SQ_{Trat}-U_T=463,9483-102,6951=361,2532\\) d) Correção da soma de quadrados de blocos: \\[ U_B=\\frac{J-1}{J}\\left(x_{ij}-\\frac{T}{J-1}\\right)^2 \\\\ U_{T}=\\frac{4-1}{4}\\left(149,44-\\frac{552,44}{5-1}\\right)^2=4,2483 \\] Portanto, temos: \\(SQ_{Bloco(Aj.)}=SQ_{Bloco}-U_T=91,9078-4,2483=87,6594\\) Então podemos montar o seguinte quadro de análise de variância: Causas de Variação GL SQ QM F Tratamento 4 361,2532 90,3133 3,81* Blocos 3 87,6594 29,21981 1,23ns Resíduo 11 260,6829 23,6984  Total 18    Valores de F da tabela para tratamentos (4 x 11 GL):\\(\\begin{cases} 5\\%=3,36 \\\\ 1\\%=5,67 \\end{cases}\\) Valores de F da tabela para blocos (3 x 11 GL):\\(\\begin{cases} 5\\%=3,59 \\\\ 1\\%=6,22 \\end{cases}\\) Conclusões: O teste F foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que as doses de Promalin possuem efeitos diferentes sobre o peso médio dos frutos de maçã, com um grau de confiança superior a \\(95\\%\\) de probabilidade. Para tirar conclusões mais específicas sobre o comportamento dos inseticidas, podemos aplicar um teste de comparação de médias. a) Cálculo das médias de tratamentos: \\[ \\hat{m_1} = \\frac{T_1}{J}=\\frac{571,21}{4}=142,80\\;g\\\\ \\hat{m_2} = \\frac{T_2}{J}=\\frac{552,10}{4}=138,03\\;g\\\\ \\hat{m_3} = \\frac{T_3}{J}=\\frac{554,97}{4}=138,74\\;g\\\\ \\hat{m_4} = \\frac{T_4}{J}=\\frac{560,04}{4}=140,01\\;g\\\\ \\hat{m_5} = \\frac{T_5}{J}=\\frac{604,90}{4}=151,23\\;g\\\\ \\] b) Cálculo dos erros padrões das médias de tratamentos: Para as médias dos tratamentos que não perderam parcela, temos: \\[ s(\\hat{m})=\\sqrt{\\frac{QM_{Res}}{J}}=\\sqrt{\\frac{23,6984}{4}}=2,43\\;g,\\;k=(1,2,3,4) \\] Para média do tratamento que perdeu a parcela, o erro padrão será: \\[ s(\\hat{m}_5)=\\sqrt{\\left(\\frac{1}{J}+\\frac{I}{J(J-1)(I-1)}\\right)s^2}\\\\ s(\\hat{m}_5)=\\sqrt{\\left( \\frac{1}{4}+\\frac{5}{4(4-1)(5-1)}\\right)23,6984}=2,90\\;g \\] c) Aplicação do teste de Tukey para comparação das médias de tratamentos: c.1) Cálculo do valor de \\(\\Delta\\) para comparações entre médias de tratamentos que não perderam parcela: \\(q_{(5\\times11GLRes)} = 4,57, s=\\sqrt{QM_{Res}}=4,8681\\;\\;\\;\\;\\;\\;\\;\\ r=4\\) Então, temos: \\[ \\Delta = dms = q\\sqrt{\\frac{QM_{Res}}{r}}=4,57\\sqrt{\\frac{23,6984}{4}}=11,12\\;g \\] c.2) Cálculo do \\(\\Delta\\) para comparações entre médias dos tratamentos que não perderam parcela e a média do tratamento que perdeu a parcela \\(\\hat{m}_5\\): \\[ \\hat{Y} = \\hat{m}_k-\\hat{m}_{i} \\\\ \\hat{V}(\\hat{Y})=\\left[\\frac{2}{J}+\\frac{I}{J(I-1)(J-1)} \\right]s^2 \\] Então, temos: \\[ \\Delta = dms =q\\sqrt{\\frac{1}{2}\\hat{V}(\\hat{Y})} \\\\ \\Delta = dms =4,57\\sqrt{\\frac{1}{2}\\left[\\frac{2}{4}+\\frac{5}{4(5-1)(4-1)} \\right]23,6984}=12,23\\;g \\] c.3) Cálculo das estimativas dos contrastes entre duas médias: \\(\\hat{m_1}\\) \\(\\hat{m_4}\\) \\(\\hat{m_3}\\) \\(\\hat{m_2}\\) \\(\\hat{m_5}\\) 8,43 11,22 12,49* 13,2* \\(\\hat{m_1}\\) - 2,79 4,06 4,77 \\(\\hat{m_4}\\) - - 1,27 1,98 \\(\\hat{m_3}\\) - - - 0,71 Conclusão: Médias seguidas de pelo menos uma letra em comum não diferem entre si, pelo teste de Tukey, ao nível de 5% de probabilidade. \\[ \\hat{m_5} = 151,23\\;g\\;\\;\\;\\;-a\\\\ \\hat{m_1} = 142,80\\;g\\;\\;\\;\\;-ab\\\\ \\hat{m_4} = 140,01\\;g\\;\\;\\;\\;-ab\\\\ \\hat{m_3} = 138,74\\;g\\;\\;\\;\\;-b\\\\ \\hat{m_2} = 138,03\\;g\\;\\;\\;\\;-b \\] Portanto, a melhor dose 12,5 ppm de promalin aplicada em plena floração (Traramento 1) ou em plena floração e no início da frutificação (Tratamento 4) não afeta o peso médio dos frutos, pois não difere da testemunha. d) Cálculo do coeficiente de variação do experimento: \\[ \\hat{m}=\\frac{G}{IJ}=\\frac{2843,22}{5\\cdot4}=142,16\\;g\\\\ s=\\sqrt{QM_{Res}}=\\sqrt{23,6984}=4,8681\\;g\\\\ CV=\\frac{100\\cdot s}{\\hat{m}}=\\frac{100\\cdot4,8681}{142,16}=3,42\\%. \\] 17.7 Aplicação no R. caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/macieira.txt&quot; dados&lt;-read.table(caminho,h=TRUE) # Extraindo os fatores e a variável resposta trat&lt;-as.factor(dados$trat) bloco&lt;-as.factor(dados$bloco) y&lt;-dados$y B&lt;-tapply(y,bloco,sum,na.rm=TRUE)[is.na(B&lt;-tapply(y,bloco,sum))] # Definindo o modelo para análise preliminar com a parcela perdida modelo&lt;-aov(y~trat+bloco) anova(modelo) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trat 4 412.42 103.106 4.3507 0.02369 * ## bloco 3 87.66 29.220 1.2330 0.34419 ## Residuals 11 260.68 23.698 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Estimando a parcela perdida xij&lt;-predict(modelo,newdata = dados$bloco&lt;-as.factor(bloco))[is.na(y)] # valor estimado xij ## 18 ## 149.44 # Substituindo a parcela perdida pelo valor estimado y[is.na(y)]&lt;-xij # Definindo o novo modelo modeloC&lt;-aov(y~trat+bloco) anava&lt;-anova(modeloC) # Calculando o fator de correção para o ajuste das soma de quadrados dos tratamentos I&lt;-length(levels(trat)) QMRes&lt;-anava$`Mean Sq`[3] (U&lt;-(I-1)/(I)*(xij-B/(I-1))^2) ## 18 ## 102.6951 anava$Df[3]&lt;-anava$Df[3]-1 anava$`Sum Sq`[1]&lt;-anava$`Sum Sq`[1]-U anava$`Mean Sq`&lt;-anava$`Sum Sq`/anava$Df anava$`F value`[1]&lt;-anava$`Mean Sq`[1]/anava$`Mean Sq`[3] anava$`Pr(&gt;F)`[1]&lt;-pf(anava$`Pr(&gt;F)`[1],anava$Df[1],anava$Df[3]) anava$`F value`[2]&lt;-anava$`Mean Sq`[2]/anava$`Mean Sq`[3] anava$`Pr(&gt;F)`[2]&lt;-pf(anava$`Pr(&gt;F)`[2],anava$Df[2],anava$Df[3]) anava ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trat 4 361.25 90.313 3.8109 0.0002554 *** ## bloco 3 91.91 30.636 1.2927 0.1667326 ## Residuals 11 260.68 23.698 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Comparações múltiplas para as médias Tukey require(&quot;agricolae&quot;) tukey &lt;- HSD.test(modelo,&quot;trat&quot;, group=TRUE,console=TRUE) ## ## Study: modelo ~ &quot;trat&quot; ## ## HSD Test for y ## ## Mean Square Error: 23.69846 ## ## trat, means ## ## y std r Min Max ## T1 142.8025 2.897843 4 138.88 145.19 ## T2 138.0250 5.708897 4 130.61 144.44 ## T3 138.7425 4.539760 4 134.06 144.11 ## T4 140.0100 7.261621 4 135.83 150.88 ## T5 151.8200 1.636123 3 150.22 153.49 ## ## Alpha: 0.05 ; DF Error: 11 ## Critical Value of Studentized Range: 4.573596 ## ## Groups according to probability of means differences and alpha level( 0.05 ) ## ## Treatments with the same letter are not significantly different. ## ## y groups ## T5 151.8200 a ## T1 142.8025 ab ## T4 140.0100 ab ## T3 138.7425 b ## T2 138.0250 b bar.group(tukey$groups, las=1, ylim=c(0,max(y)*1.10), xlab=&quot;Cultivares&quot;, ylab=&quot;Produção (kg por ha)&quot;, main=&quot;Teste de Tukey (5%)&quot;);box() # Cálculo do CV QMRes&lt;-anava$`Mean Sq`[3] cv&lt;-100*sqrt(QMRes)/mean(y) round(cv,2) ## [1] 3.42 "],["experimentos-fatoriais.html", "18 Experimentos Fatoriais 18.1 Introdução 18.2 Efeito simples de um fator 18.3 Efeito principal de um fator 18.4 Efeito da interação entre fatores 18.5 Vantagens dos experimentos fatoriais 18.6 Desvantagens dos experimentos fatoriais 18.7 Classificação dos experimentos fatoriais 18.8 Casualização dos tratamentos 18.9 Análise de variância de um experimento fatorial com \\(2\\) fatores com interação não significativa 18.10 Obtenção da análise de variância 18.11 Cálculo das médias de Tratamento do Solo e erros padrões das médias 18.12 Cálculo das médias de Fungicidas e erros padrões das médias 18.13 Cálculo do coeficiente de variação do experimento", " 18 Experimentos Fatoriais 18.1 Introdução Os experimentos simples, realizados de acordo com o delineamento inteiramente casualizado (DIC) ou em blocos casualizados (DBC), são utilizados para testar os efeitos de apenas um tipo de tratamento, ou fator, sendo os demais mantidos constantes. Assim, por exemplo, num experimento de comparação de inseticidas em relação ao controle de uma determinada praga, devemos manter constante a dosagem, o método de aplicação, os tratos culturais, etc. Porém, há casos em que necessitamos testar simultaneamente os efeitos de dois ou mais tipos de tratamentos (fatores) para obtermos resultados de interesse prático. Por exemplo, supondo que desejamos testar \\(3\\) inseticidas, \\(2\\) métodos de aplicação e \\(4\\) dosagens, teremos então um experimento fatorial de \\(3\\times2\\times4\\). Os experimentos fatoriais são aqueles que nos permitem estudar, simultaneamente, os efeitos de dois ou mais tipos de fatores. Assim, eles devem ser instalados em um dos delineamentos já estudados (DIC, DBC, etc.). Estes experimentos são utilizados em quase todos os campos de investigação e são bastante úteis em pesquisas iniciais, nas quais pouco se conhece a respeito de uma série de fatores. O número de tratamentos nos experimentos fatoriais consiste de todas as combinações possíveis dos níveis dos fatores. Por exemplo, se estamos interessados em testar o efeito de \\(3\\) inseticidas, cada um dos quais em \\(4\\) doses, teremos os \\(12\\) tratamentos seguintes. \\[ \\begin{array}{} I_1 D_1 &amp; I_2D_1 &amp; I_3D_1 \\\\ I_1D_2 &amp; I_2D_2 &amp; I_3D_2 \\\\ I_1D_3 &amp; I_2D_3 &amp; I_3D_3 \\\\ I_1D_4 &amp; I_2D_4 &amp; I_3D_4 \\end{array} \\] Neste caso, representamos o esquema fatorial como: \\(\\text{Fatorial }3 \\times 4\\) com \\(3\\) inseticidas e \\(4\\) dosagens. As subdivisões de um fator são denominados NÍVEIS desse fator. Então, no exemplo anterior, o fator Inseticida ocorrem em 3 níveis, e o fator Dosagem ocorre em 4 níveis. Assim, no ensaio acima, podemos obter conclusões sobre qual o melhor inseticida, qual a melhor dosagem e qual a melhor dosagem para cada inseticida. Vamos então apresentar algumas definições necessárias ao bom entendimento dos ensaios fatoriais. Supondo que temos \\(2\\) fatores A e B cada um dos quais em dois níveis \\((0 \\text{ e } 1)\\), presença e ausência, respectivamente, e que os resultados obtidos foram os seguintes: \\(B_0\\) \\(B_1\\) Total \\(A_0\\) \\(3\\) \\(7\\) \\(10\\) \\(A_1\\) \\(9\\) \\(18\\) \\(27\\) Total \\(12\\) \\(25\\) \\(37\\) DOWNLOAD 18.2 Efeito simples de um fator É uma medida da variação que ocorre com a variável resposta (característica em estudo) correspondente a variação nos níveis desse fator, em cada um dos níveis do outro fator. Efeito de \\(A\\) dentro de \\(B_0 = A_1B_0-A_0B_0 = 9 -3 = 6\\) Efeito de \\(A\\) dentro de \\(B_1 = A_1B_1-A_0B_1 = 18-7 = 11\\) Efeito de \\(B\\) dentro de \\(A_0 = A_0B_1-A_0B_0=7-3=4\\) Efeito de \\(B\\) dentro de \\(A_1 = A_1B_1-A_1B_0 = 18-9=9\\) 18.3 Efeito principal de um fator É uma medida da variação que ocorre com a variável resposta correspondente às variações nos níveis em média de todos os níveis do outro fator. Efeito Principal de A \\[ A = \\frac{\\text{Ef. de A d. }B_0+\\text{Ef. de A d. }B_1}{2}=\\frac{6+11}{2}=8,5 \\] ou ainda: \\(A = \\frac{A_1B_0+A_1B_1}{2}-\\frac{A_0B_0+A_0B_1}{2} = \\frac{9+18}{2}-\\frac{3+7}{2} = 13,5-5,0=8,5\\) Efeito Principal de B \\[ B = \\frac{\\text{Ef. de B d. }A_0+\\text{Ef. de B d. }A_1}{2}=\\frac{4+9}{2}=6,5 \\] ou ainda: \\(B = \\frac{A_0B_1+A_1B_1 }{2}-\\frac{A_0B_0+A _1B_0}{2} = \\frac{7+18}{2}-\\frac{3+9}{2} = 12,5-6,0=6,5\\) Portanto, o efeito principal de um fator é a média dos efeitos simples desse fator 18.4 Efeito da interação entre fatores É uma medida da variação que ocorre com o efeito simples de um fator ao passar de um nível a outro do outro fator. Efeito da Interação \\(A \\times B\\) \\[ A \\times B = \\frac{A\\;d.\\;B_1-A\\;d.\\;B_0}{2} = \\frac{(A_1B_1-A_0B_1)-(A_1B_0-A_0B_0)}{2}=\\frac{11-6}{2}=2,5 \\] Efeito da Interação \\(B \\times A\\) \\(B \\times A = \\frac{B\\;d.\\;A_1-B\\;d.\\;A_0}{2} = \\frac{(A_1B_1-A_1B_0)-(A_0B_1-A_0B_0)}{2}=\\frac{9-4}{2} = 2,5\\) Note que o efeito da Interação \\(A \\times B = B \\times A\\). 18.5 Vantagens dos experimentos fatoriais Permite estudar os efeito simples, o efeito principal e o efeito da interação entre fatores. Todas as parcelas experimentais entram no cálculo dos efeitos principais e interações, razão pela qual as médias dos níveis dos fatores são calculadas com um maior número de repetições. 18.6 Desvantagens dos experimentos fatoriais A análise estatística é trabalhosa, e a interpretação dos resultados torna-se mais difícil à medida que aumentam o número de fatores e de níveis. Como os tratamentos devem conter todas as combinações possíveis dos fatores em seus diversos níveis, o número de tratamentos aumenta rapidamente, e às vezes devido à exigência de homogeneidade dentro dos blocos, não podemos utilizar o delinemamento em blocos completos casualizados. Sendo os tratamentos constituídos de todas as combinações possíveis dos fatores em seus diversos níveis, para que haja um balanceamento estatístico, às vezes algumas das combinações não tem interesse prático, mas devem ser mantidas para não quebrar o balanceamento. 18.7 Classificação dos experimentos fatoriais Fatoriais de série \\(2^N\\) Nesta série são enquadrados os experimentos fatoriais em que são estudados os efeitos de \\(N\\) fatores cada um em \\(2\\) níveis. BASE = Nº de Níveis EXPOENTE = Nº de Fatores Exemplos: \\[ 2^2 \\Rightarrow \\text{2 Fatores em 2 Níveis} \\\\ 2^3 \\Rightarrow \\text{3 Fatores em 2 Níveis} \\\\ 2^4 \\Rightarrow \\text{4 Fatores em 2 Níveis} \\\\ \\] etc. Fatoriais de série \\(3^N\\) Nesta série são enquadrados os experimentos fatoriais em que são estudados os efeitos de \\(N\\) fatores cada um em \\(3\\) níveis. Exemplos: \\[ 3^2 \\Rightarrow \\text{2 Fatores em 3 Níveis} \\\\ 3^3 \\Rightarrow \\text{3 Fatores em 3 Níveis} \\\\ 3^4 \\Rightarrow \\text{4 Fatores em 3 Níveis} \\\\ \\] etc. Fatoriais de série mista Nesta série são enquadrado os fatoriais em que os fatores ocorrem em número diferente de níveis: Exemplo: \\[ 4\\times 3\\times 2 \\Rightarrow \\begin{cases} \\text{1º Fator em 4 Níveis }\\\\ \\text{2º Fator em 3 Níveis } \\\\ \\text{3º Fator em 2 Níveis } \\end{cases} \\] 18.8 Casualização dos tratamentos Para exemplificar a casualização dos tratamentos, vamos supor um experimento fatorial \\(3 \\times 2\\), com \\(3\\) variedades de milho (\\(V_1,V_2,V_3\\)) e \\(2\\) níveis de adubação com \\(P_2O_5\\) (\\(P_1 e\\; P_2\\)). Se o experimento fosse instalado de acordo com o delineamento em blocos casualizados, com \\(4\\) repetições, teríamos: O esquema da análise de variância preliminar deste ensaio será o seguinte: Causas de Variação GL Tratamentos 5 Blocos 3 Resíduos 15 Total 23 Após a análise de variância preliminar, os graus de liberdade de tratamentos devem ser desdobrados de acordo com o esquema fatorial \\(3 \\times 2\\), da seguinte maneira: Causas de Variação GL Variedades (V) 2 Adubação (P) 1 Interação (VxP) 2 (Tratamentos) 5 Blocos 3 Resíduos 15 Total 23 18.9 Análise de variância de um experimento fatorial com \\(2\\) fatores com interação não significativa Para a obtenção da análise de variância, vamos utlizar os dados adaptados do trabalho Ensaios em condições de casa-de-vegetação para controle químico do damping-off em Eucalyptus saligna Sm. realizado por KRUGNER; CARVALHO (1971) e publicado em IPEF, n 2/3 p. 97-113. O ensaio foi realizado no delineamento inteiramente casualizado, com \\(3\\) repetições e foram estudados os efeitos sobre a altura média das mudas de Eucalytus saligna, dos fatores: Tratamento do solo (S), sendo: \\(S_1=\\text{Vapam}\\) \\(S_2=\\text{Brometo de metila}\\) \\(S_3=\\text{PCNB}\\) \\(S_4=\\text{Testemunha}\\) Pulverização com fungicida em pós emergência, sendo: \\(F_0 = \\text{Sem fungicida}\\) \\(F_1 = \\text{Com fungicida}\\) Os dados podem ser encontrados online em solofungi.txt As alturas médias de mudas (cm) \\(28\\) dias após a semeadura foram: Tratamentos Rep.1 Rep.2 Rep.3 Total \\(S_1F_0\\) 4,65 5,18 5,52 15,35 \\(S_1F_1\\) 4,86 4,81 4,51 14,18 \\(S_2F_0\\) 4,55 5,16 6,00 15,71 \\(S_2F_1\\) 4,73 5,51 5,09 15,33 \\(S_3F_0\\) 2,68 2,65 2,56 7,89 \\(S_3F_1\\) 2,90 2,71 2,93 8,54 \\(S_4F_0\\) 3,48 2,75 3,06 9,29 \\(S_4F_1\\) 2,65 2,47 2,83 7,95 Total 94,24 Dados originais:DOWNLOAD 18.10 Obtenção da análise de variância O ensaio foi montado de acordo com o delineamento inteiramente casualizado, e, portanto, a análise de variância preliminar, obtida de maneira usual, foi a seguinte: \\[ \\begin{aligned} SQ_{Total} &amp;= (4,65^2+5,18^2+ \\dots +2,83^2) - \\frac{94,24^2}{8 \\cdot 3} \\\\ &amp; = 403,2566 - 370,0491 \\\\ &amp;= 33,2075 \\\\ \\\\ \\\\ SQ_{Trat} &amp;= \\frac{1}{3}(15,35^2+14,18^2+ \\dots +7,95^2) - \\frac{94,24^2}{8 \\cdot 3} \\\\ &amp; = 401,0061 - 370,0491 \\\\ &amp;= 31,0170 \\\\ \\\\ \\\\ SQ_{Res} &amp;= SQ_{Total} - SQ_{Trat} \\\\ &amp; = 33,2075 - 31,0170 \\\\ &amp;= 2,1905 \\end{aligned} \\] Quadro da análise de variância preliminar: Causas de Variação GL SQ QM F Tratamentos 7 31,0170 4,4310 32,37** Resíduos 16 2,1905 0,1369 Total 23 33,20175 Conclusão: O teste \\(F\\) foi significativo ao nível de \\(1\\%\\) de probabilidade, logo, rejeitamos a hipótese da nulidade (\\(H_0\\)), e concluímos que os tratamentos possuem efeitos diferentes sobre a altura das mudas de Eucalyptus saligna, com um grau de confiança superior a \\(99\\%\\) de probabilidade. Devemos agora desdobrar a soma de quadrados e os graus de liberdade de tratamentos para estudar os efeitos principais e a interação entre os fatores. Para facilitar os cálculos, utilizamos um quadro auxiliar como o seguinte: \\((3)\\) \\(S_1\\) \\(S_1\\) \\(S_1\\) \\(S_1\\) Total \\(F_0\\) 15,35 15,71 7,89 9,29 48,24 \\(F_1\\) 14,18 15,33 8,54 7,95 46,00 Total 29,53 31,04 16,43 17,24 94,24 Então, as somas de quadrados são obtidas da seguinte maneira: 1. Soma de quadrados devido ao efeito de Tratamento do Solo: \\[ \\begin{aligned} SQ_{Ef.Trat.Solo} &amp;= \\frac{1}{r_s}(T_{S_1}^2+T_{S_2}^2+T_{S_3}^2+T_{S_4}^2) - C \\\\ &amp;= \\frac{1}{6}(29,53^2+31,04^2+16,43^2+17,24^2) - \\frac{94,24^2}{24} \\\\ &amp;= 30,3951 \\end{aligned} \\] 2. Soma de quadrados devido ao efeito de Fungicidas: \\[ \\begin{aligned} SQ_{Ef.Fung.} &amp;= \\frac{1}{r_F}(T_{F_0}^2+T_{F_1}^2) - C \\\\ &amp;= \\frac{1}{12}(48,24^2+46,00^2) - \\frac{94,24^2}{24} \\\\ &amp;= 0,2091 \\end{aligned} \\] 3. Soma de quadrados devido ao efeito da Interação Tratamento do Solo x Fungicida: \\[ \\begin{aligned} SQ_{Interação\\;S\\times F} &amp;= SQ_{S,F}-SQ_{Ef.Trat.Solo}-SQ_{Ef.Fungicida} \\\\ \\\\ &amp; \\text{assim, calculamos } SQ_{S,F}: \\\\ SQ_{S,F} &amp;= \\frac{1}{r_{SF}}(T_{S_1F_0}^2+T_{S_1F_1}^2+\\cdots +T_{S_4F_1}^2) - C \\\\ &amp;= \\frac{1}{3}(15,35^2+14,18^2+\\cdots + 7,95^2) - \\frac{94,24^2}{24} \\\\ &amp;= 31,0170 \\end{aligned} \\] Portanto, \\[ \\begin{aligned} SQ_{Interação\\;S\\times F} &amp;= SQ_{S,F}-SQ_{S}-SQ_{F} \\\\ SQ_{Interação\\;S\\times F} &amp;= 31,0170-30,3951-0,2091 \\\\ &amp;= 0,4128 \\end{aligned} \\] Portanto, temos o seguinte quadro de análise de variância: Causas de Variação GL SQ QM F Tratamento de solo (S) 3 30,3951 10,1317 74,00** Fungicida (F) 1 0,2091 0,2091 1,53ns Interação (SxF) 3 0,4128 0,1376 1,01ns (Tratamentos) 7 31,0170 Resíduos 16 2,1905 0,1369 Total 23 33,2075 Valores de \\(F\\) da tabela para Trat. do solo (\\(3 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,24 \\\\ 1\\%=5,29 \\end{cases}\\) Valores de \\(F\\) da tabela para Fungicida (\\(1 \\times 16 GL\\)): \\(\\begin{cases}5\\%=4,49 \\\\ 1\\%=8,53 \\end{cases}\\) Valores de \\(F\\) da tabela para Interação \\(S\\times F\\) (\\(3 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,24 \\\\ 1\\%=5,29 \\end{cases}\\) Conclusões Para efeito de Tratamento do solo: O teste foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que os tratamentos do solo possuem efeitos diferentes sobre a altura de mudas de eucalipto. Para efeito de Fungicida: O teste não foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\) e concluir que a aplicação ou não de fungicida em pós-emergência não possui efeito sobre a altura de mudas de eucalipto. Para efeito da Interação (S \\(\\times\\) F): O teste não foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\) e concluir que os tratamentos de solo agem de maneira independente do fungicida sobre a altura das mudas de eucalipto. Portanto, como a Interação S\\(\\times\\)F foi não significativa, podemos estudar os efeitos principais dos fatores independentemente um do outro. Para complementar a análise de variância, e obter conclusões mais específicas sobre o efeito de cada um dos fatores, podemos utilizar os testes de comparação de médias. 18.11 Cálculo das médias de Tratamento do Solo e erros padrões das médias \\[ \\hat{m_{S_1}} = \\frac{T_{S_1}}{r_{S_1}}=\\frac{29,53}{6}=4,92\\;cm \\\\ \\hat{m_{S_2}} = \\frac{T_{S_2}}{r_{S_2}}=\\frac{31,04}{6}=5,17\\;cm \\\\ \\hat{m_{S_3}} = \\frac{T_{S_3}}{r_{S_3}}=\\frac{16,43}{6}=2,74\\;cm \\\\ \\hat{m_{S_4}} = \\frac{T_{S_4}}{r_{S_4}}=\\frac{17,24}{6}=2,87\\;cm \\] O erro padrão destas médias será: \\[ s(\\hat{m})=\\sqrt{\\frac{QM_{Res}}{r_S}}=\\sqrt{\\frac{0,1369}{6}} = 0,15\\;cm \\] Teste de Tukey para comparar as médias de tratamento do solo \\[ DMS=q_{(4\\times16)}\\cdot s(\\hat{m})=4,05 \\cdot 0,15 = 0,61\\; cm \\] - \\(\\hat{m_{S_1}}\\) \\(\\hat{m_{S_4}}\\) \\(\\hat{m_{S_3}}\\) \\(\\hat{m_{S_2}}\\) \\(0,25\\) \\(2,30^*\\) \\(2,43^*\\) \\(\\hat{m_{S_1}}\\) - \\(2,05^*\\) \\(2,18^*\\) \\(\\hat{m_{S_4}}\\) - - \\(0,13\\) ou ainda: \\[ \\hat{m_{S_2}} = 5,17\\;cm -a\\\\ \\hat{m_{S_1}} = 4,92\\;cm -a\\\\ \\hat{m_{S_4}} = 2,87\\;cm -b\\\\ \\hat{m_{S_3}} = 2,74\\;cm -b\\\\ \\] Conclusão: Médias seguidas pela mesma letra não diferem entre sim pelo teste de Tukey ao nível de \\(5\\%\\) de probabilidade. 18.12 Cálculo das médias de Fungicidas e erros padrões das médias \\[ \\hat{m_{F_0}} = \\frac{T_{F_0}}{r_{F_0}}=\\frac{48,24}{12}=4,02\\;cm \\\\ \\hat{m_{F_1}} = \\frac{T_{F_1}}{r_{F_1}}=\\frac{46,00}{12}=3,83\\;cm \\] O erro padrão destas médias será: \\[ s(\\hat{m})=\\sqrt{\\frac{QM_{Res}}{r_F}}=\\sqrt{\\frac{0,1369}{12}} = 0,11\\;cm \\] Teste de Tukey para comparar as médias de Fungicidas Note que não houve diferença significativa entre os tratamentos com e sem fungicidas e, portanto, não há necessidade de se comparar essas médias. Porém, caso quiséssemos compara-lás pelo teste de Tukey, teríamos: \\[ DMS=q_{(2\\times1 6)}\\cdot s(\\hat{m})=3.00 \\cdot 0,11 = 0,32\\; cm \\\\ \\hat{Y}=\\hat{m}_{F_0}-\\hat{m}_{F_1}=4,02-3,83=0,19 \\; cm \\] Portanto, como \\(\\hat{Y}&lt; DMS \\Rightarrow \\hat{m}_{F_0}\\) não difere de \\(\\hat{m}_{F_1}\\) 18.13 Cálculo do coeficiente de variação do experimento \\[ CV=100\\cdot \\frac{\\sqrt{QM_{res}}}{\\hat{m}}=100\\cdot \\frac{0,3700}{3,9267}=9,42\\% \\] Aplicação no R Utilizando as funções básicas e o pacote agricolae # Carregando o pacote para análise de variância library(agricolae) library(tidyverse) # Definindo o caminho do banco de dados caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/solofungi.txt&quot; # Entrada da dados dados&lt;-read.table(caminho,h=TRUE) #Guardando os fatores (tratamentos de solo e fungicidas) e a variável resposta (y) solos&lt;-as.factor(dados$S) fungicida&lt;-as.factor(dados$F) y&lt;-dados$y # Gráfico da interação dados %&gt;% group_by(S,F) %&gt;% summarise(Y = mean(y)) %&gt;% ggplot(aes(x=S, y=Y,col=as.factor(F)))+ geom_line()+ labs(x=&quot;Tratamentos do solo&quot;,y=&quot;Altura de plantas (cm)&quot;,col=&quot;Fungicida&quot;) ## `summarise()` has grouped output by &#39;S&#39;. You can override using the `.groups` argument. dados %&gt;% group_by(S,F) %&gt;% summarise(Y = mean(y)) %&gt;% ggplot(aes(x=F, y=Y,col=as.factor(S)))+ geom_line()+ labs(x=&quot;Fungicida&quot;,y=&quot;Altura de plantas (cm)&quot;,col=&quot;Tratamento do solo&quot;) ## `summarise()` has grouped output by &#39;S&#39;. You can override using the `.groups` argument. Utilizando ao pacrote ExpDes.pt, mais prático # Carregando o pacote para análise de variância library(ExpDes.pt) # Definindo o caminho do banco de dados caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/solofungi.txt&quot; # Entrada da dados dados&lt;-read.table(caminho,h=TRUE) #Guardando os fatores (tratamentos de solo e fungicidas) e a variável resposta (y) solos&lt;-dados$S fungicida&lt;-dados$F y&lt;-dados$y # Utilizando a função fat2.dic do pacote ExpDes.pt fat2.dic(solos,fungicida,y,fac.names = c(&quot;Trat_Solo&quot;, &quot;Fungicida&quot;)) ## ------------------------------------------------------------------------ ## Legenda: ## FATOR 1: Trat_Solo ## FATOR 2: Fungicida ## ------------------------------------------------------------------------ ## ## ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Trat_Solo 3 30.395 10.1317 74.004 0.00000 ## Fungicida 1 0.209 0.2091 1.527 0.23439 ## Trat_Solo*Fungicida 3 0.413 0.1376 1.005 0.41607 ## Residuo 16 2.191 0.1369 ## Total 23 33.208 ## ------------------------------------------------------------------------ ## CV = 9.42 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos (Shapiro-Wilk) ## valor-p: 0.6260575 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## Interacao nao significativa: analisando os efeitos simples ## ------------------------------------------------------------------------ ## Trat_Solo ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 2 5.173333 ## a 1 4.921667 ## b 4 2.873333 ## b 3 2.738333 ## ------------------------------------------------------------------------ ## ## Fungicida ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 0 4.020000 ## 2 1 3.833333 ## ------------------------------------------------------------------------ "],["experimento-fatorial-com-2-fatores-e-interação-significativa.html", "19 Experimento fatorial com 2 fatores e interação significativa 19.1 Obtenção da análise de variância 19.2 Teste de Tukey para a comparação de médias 19.3 Resultado do teste de Tukey 19.4 Estudo do Fatorial \\(3^2\\)", " 19 Experimento fatorial com 2 fatores e interação significativa Para obtenção da análise de variância, vamos supor o seguinte ensaio em que foram estudados os efeitos de \\(4\\) inseticidas em \\(2\\) doses diferentes sobre a produção da cultura do milho em kg/parcela. Dados estão disponíveis online em inseticidas.txt Tratamento Rep.1 Rep.2 Rep.3 Total \\(I_1D_1\\) 58 45 47 150 \\(I_1D_2\\) 61 65 47 173 \\(I_2D_1\\) 31 35 29 95 \\(I_2D_2\\) 43 51 49 143 \\(I_3D_1\\) 45 55 79 179 \\(I_3D_2\\) 31 37 37 105 \\(I_4D_1\\) 78 83 62 223 \\(I_4D_2\\) 36 34 34 104 Total 383 405 384 1172 Dados originais:DOWNLOAD 19.1 Obtenção da análise de variância O ensaio foi montado de acordo com o delineamento inteiramente casualizado, e portanto, a análise de variância preliminar, obtida da maneira usual, foi a seguinte: \\[ \\begin{aligned} SQ_{Total} &amp;= (58^2+45^2+\\cdots +34^2)-\\frac{1175^2}{8 \\cdot 3} \\\\ &amp;=5813,33 \\end{aligned} \\] \\[ \\begin{aligned} SQ_{Trat} &amp;= \\frac{1}{3} (150^2+173^2+\\cdots +104^2)-\\frac{1175^2}{8 \\cdot 3} \\\\ &amp;=46 \\end{aligned} \\] \\[ \\begin{aligned} SQ_{Res} &amp;= SQ_{Total} - SQ_{Trat} &amp;=5813,33 - 4605,33 &amp;=1208,00 \\end{aligned} \\] Quadro de Análise de Variância Preliminar: Causas de Variação GL SQ QM F Trat. 7 4605,33 657,90 8,71* Res 16 1208,0 75.50 Total 23 5813,13 Conclusão: O teste é siginificativo ao nível de \\(1\\%\\) de probabilidade, logo, rejeitamos a hipótese da nulidade (\\(H_0\\)), e concluímos que os efeitos dos tratamento diferem entre sim em relação à característica analisada, com um grau de confiança superior a \\(99\\%\\) de probabilidade. Devemos agora, desdobrar a soma de quadrado e os graus de liberdade de tratamentos para estudar os efeitos principais e a interação entre os fatores. Para facilitar os cálculos, utilizamos um quadro auxiliar como o seguinte: Quadro de totais (3) \\(I_1\\) \\(I_2\\) \\(I_3\\) \\(I_4\\) Total \\(D_1\\) 150 95 179 223 647 \\(D_2\\) 173 143 105 104 525 Total 323 238 284 327 1172 Então, as somas de quadrados são obtidas da seguinte maneira: 1. Soma de quadrados devido ao efeito de Inseticidas: \\[ \\begin{aligned} SQ_{Ef.Inseticida} &amp;= \\frac{1}{r_I}[T_{I_1}^2+T_{I_2}^2+T_{I_3}^2+T_{I_4}^2] - \\frac{G^2}{I\\cdot J} \\\\ &amp;= \\frac{1}{6}[323^2+238^2+284^2+327^2] - \\frac{1172^2}{24} \\\\ &amp;= 860,33 \\end{aligned} \\] 2. Soma de quadrados devido ao efeito de Doses: \\[ \\begin{aligned} SQ_{Ef.Dose} &amp;= \\frac{1}{r_D}[T_{D_1}^2+T_{D_2}] - \\frac{G^2}{I\\cdot J} \\\\ &amp;= \\frac{1}{12}[647^2+525^2] - \\frac{1172^2}{24} \\\\ &amp;= 620,16 \\end{aligned} \\] 3. Soma de quadrados devido ao efeito da Interação Inseticida x Doses: \\[ \\begin{aligned} SQ_{Interação\\;I\\times D} &amp;= SQ_{S,F}-SQ_{Ef.Ins.}-SQ_{Ef.Dos.} \\\\ SQ_{I,D} &amp;= \\frac{1}{r_{SF}}(T_{I_1D_1}^2+T_{I_1D_2}^2+\\cdots +T_{I_4D_2}^2) - C \\\\ &amp;= \\frac{1}{3}(150^2+173^2+\\cdots + 104^2) - \\frac{1172^2}{24} \\\\ &amp;= 4605.33 \\end{aligned} \\] assim, \\[ SQ_{Interação\\;I\\times D} = SQ_{S,F}-SQ_{Ef.Ins.}-SQ_{Ef.Dos.} \\\\ SQ_{Interação\\;I\\times D}=4605,33-860,33-620,16 \\\\ SQ_{Interação\\;I\\times D}=3124,84 \\] Portanto, temos o seguinte quadro de análise de variância: Causas de Variação GL SQ QM F Efeito de Inseticida (I) 3 860,33 286,78 3,80* Efeito de Doses (D) 1 620,16 620,16 8,21* Ef. da Interação (IxD) 3 3124,84 1041,61 13,80** (Tratamentos) 7 4605,33   Resíduos 16 1208,00 75,50  Total 23 5813,33   Valores de F da tabela para Inseticida (\\(3 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,24 \\\\ 1\\%=5,29 \\end{cases}\\) Valores de F da tabela para Fungicida (\\(1 \\times 16 GL\\)): \\(\\begin{cases}5\\%=4,49 \\\\ 1\\%=8,53 \\end{cases}\\) Valores de F da tabela para Interação \\(S\\times F\\) (\\(3 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,24 \\\\ 1\\%=5,29 \\end{cases}\\) Conclusões Para efeito de Inseticida: O teste foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que os inseticidas possuem efeitos diferentes sobre a produção da cultura do milho. Para efeito de Dose: O teste foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que as dosagens possuem efeitos diferentes sobre a produção da cultura do milho. Para efeito da Interação (I \\(\\times\\) D): O teste foi significativo ao nível de \\(1\\%\\) de probabilidae, indicando que devemos rejeitar \\(H_0\\) e concluir que os inseticidas e dosagens agem conjuntamente sobre a produção da cultura do milho, ou seja, inseticidas e dosagens não agem de maneira independente. Devemos portanto, desdobrar o efeito da interação para estudar os efeitos de cada um dos fatores dentro dos níveis do outro fator. 19.1.1 Desdobrando a interação \\(I\\times D\\), para estudar os efeitos do fator DOSES em cada nível do fator INSETICIDA (D d. I): \\[ SQ_{Dd.I_1} = \\frac{1}{3}(150^2+173^2) - \\frac{323^2}{6}=88,16 \\\\ SQ_{Dd.I_2} = \\frac{1}{3}(95^2+143^2) - \\frac{238^2}{6}=384,00 \\\\ SQ_{Dd.I_3} = \\frac{1}{3}(179^2+105^2) - \\frac{284^2}{6}=912,66 \\\\ SQ_{Dd.I_4} = \\frac{1}{3}(223^2+104^2) - \\frac{327^2}{6}=2360,17 \\] Verificação: \\(SQ_{Dd.I_1}+SQ_{Dd.I_2}+SQ_{Dd.I_3}+SQ_{Dd.I_4}=SQ_{D}+SQ_{D\\times I}\\) Então, o quadro de análise de variância com desdobramento da interação IxD, estudando-se o efeito de doses dentro de cada inseticida será o seguinte: Causas de Variação GL SQ QM F Efeito de Inseticida (I) 3 860,33 286,78 3,80* Doses d.I1 1 88,16 88,16 1,17 Doses d.I2 1 384,00 384,00 5,09* Doses d.I3 1 912,66 912,66 12,09** Doses d.I4 1 2360,17 2360,17 31,26** (Tratamentos) 7 4605,33   Resíduos 16 1208,00 75,50  Total 23    Valores de F da tabela para Inseticidas (\\(3 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,24 \\\\ 1\\%=5,29 \\end{cases}\\) Valores de F da tabela para Doses d. Inseticidas (\\(1 \\times 16 GL\\)): \\(\\begin{cases}5\\%=4,49 \\\\ 1\\%=8,53 \\end{cases}\\) 19.1.2 Desdobrando a interação \\(I \\times D\\), para estudar os efeitos do fator INSETICIDAS dentro de cada nível do fator DOSE (D d. I): \\[ SQ_{Id.D_1} = \\frac{1}{3}(150^2+95^2+179^2+223^2) - \\frac{647^2}{12}=2880,92 \\\\ SQ_{Id.D_2} = \\frac{1}{3}(173^2+143^2+105^2+104^2) - \\frac{525^2}{12}=1104,25 \\\\ \\] Verificação: \\(SQ_{Id.D_1}+SQ_{Id.D_2}=SQ_{I}+SQ_{I \\times D}\\) Causas de Variação GL SQ QM F Efeito de Inseticida (I) 1 620,16 620,16 8,21* Inseticida d.D1 3 2880,92 960,31 12,72** Inseticida d.D2 3 1104,25 368,08 4,88* (Tratamentos) 7 4605,33   Resíduos 16 1208,00 75,50  Total 23 5813,33   Valores de F da tabela para Doses (\\(1 \\times 16 GL\\)): \\(\\begin{cases}5\\%=4.49 \\\\ 1\\%=8.53 \\end{cases}\\) Valores de F da tabela para Inseticidas d. Doses (\\(3 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3.24 \\\\ 1\\%=5.29 \\end{cases}\\) Para completar a análise de variância, e obter conclusões mais específicas sobre o efeito dos inseticidas em cada dosagem, podemos aplicar um teste de comparação de médias. 19.2 Teste de Tukey para a comparação de médias Construção do Quadro de Médias a partir do Quadro de totais: Quadro auxiliar de totais: (3) \\(I_1\\) \\(I_2\\) \\(I_3\\) \\(I_4\\) Total \\(D_1\\) \\(150\\div 3\\) \\(95\\div 3\\) \\(179\\div 3\\) \\(223\\div 3\\) \\(647\\div 12\\) \\(D_2\\) \\(173\\div 3\\) \\(143\\div 3\\) \\(105\\div 3\\) \\(104\\div 3\\) \\(525\\div 12\\) Total \\(323\\div 6\\) \\(238\\div 6\\) \\(284\\div 6\\) \\(327\\div 6\\) \\(1172\\div 24\\) Quadro de médias: \\(I_1\\) \\(I_2\\) \\(I_3\\) \\(I_4\\) Média Doses \\(D_1\\) 50,00 31,67 59,67 74,32 53,92 \\(D_2\\) 53,67 47,67 35,00 34,67 43,75 Média Inseticidas 53,83 39,67 47,33 54,5 48,83 19.2.1 1. Para comparar médias de Inseticidas na Dose 1 (I d. \\(D_1\\)) \\[ DMS=q_{(4 \\times 16 GL)} \\cdot s(m) = q_{(4 \\times 16 GL)}\\cdot \\sqrt{\\frac{QM_{Res}}{r}}=4,05\\cdot \\sqrt{\\frac{75,50}{3}}=20,34\\;kg/parcela \\] Inseticidas d. \\(D_1\\) \\(\\hat{m_{I_4}}\\) \\(\\hat{m_{I_3}}\\) \\(\\hat{m_{I_1}}\\) \\(\\hat{m_{I_2}}\\) \\(\\hat{m_{I_4}}\\) 14,66 24,33* 42,66* \\(\\hat{m_{I_3}}\\)   9,67 28,00* \\(\\hat{m_{I_1}}\\)    18,37 19.2.2 2. Para comparar médias de Inseticidas na Dose 2 (I d. \\(D_2\\)) \\[ DMS=q \\cdot s(m) = q_{(4 \\times 16 GL)}\\cdot \\sqrt{\\frac{QM_{Res}}{r}}=4,05\\cdot \\sqrt{\\frac{75,50}{3}}=20,34\\;kg/parcela \\] Inseticidas d. \\(D_1\\) \\(\\hat{m_{I_1}}\\) \\(\\hat{m_{I_2}}\\) \\(\\hat{m_{I_3}}\\) \\(\\hat{m_{I_4}}\\) \\(\\hat{m_{I_1}}\\) 10,00 22,67* 23,00* \\(\\hat{m_{I_2}}\\)   12,67 13,00 \\(\\hat{m_{I_3}}\\)    0,33 19.3 Resultado do teste de Tukey Médias seguidas pela mesma letra, minúsculas nas linhas e maiúsculas nas colunas, não diferem entre si pelo teste de Tukey ao nível de \\(5\\%\\) de probabilidade. \\(I_1\\) \\(I_2\\) \\(I_3\\) \\(I_4\\) Média Doses \\(D_1\\) 50,00 Abc 31,67 Bc 59,67 Aab 74,32 Aa 53,92 \\(D_2\\) 53,67 Aa 47,67 Aab 35,00 Bb 34,67 Bb 43,75 Média Inseticidas 53,83 39,67 47,33 54,5 48,83 19.3.1 Cálculo do coeficiente de variação do experimento \\[ CV=100\\cdot \\frac{\\sqrt{QM_{res}}}{\\hat{m}}=100\\cdot \\frac{8,69}{48,83}=17,80\\% \\] ## Aplicação no R # Carregando o pacote par análise de variância library(ExpDes.pt) caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/inseticidas.txt&quot; d&lt;-read.table(caminho,h=TRUE) Inseticidas&lt;-factor(d$Ins) Doses&lt;-factor(d$Dos) y&lt;-d$Y interaction.plot(Doses,Inseticidas,y,mean) interaction.plot(Inseticidas,Doses,y,mean) fat2.dic(Inseticidas,Doses,y,fac.names = c(&quot;Inseticidas&quot;, &quot;Doses&quot;)) ## ------------------------------------------------------------------------ ## Legenda: ## FATOR 1: Inseticidas ## FATOR 2: Doses ## ------------------------------------------------------------------------ ## ## ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Inseticidas 3 860.3 286.78 3.7984 0.0312885 ## Doses 1 620.2 620.17 8.2141 0.0112029 ## Inseticidas*Doses 3 3124.8 1041.61 13.7962 0.0001057 ## Residuo 16 1208.0 75.50 ## Total 23 5813.3 ## ------------------------------------------------------------------------ ## CV = 17.79 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos (Shapiro-Wilk) ## valor-p: 0.419147 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ## ## Interacao significativa: desdobrando a interacao ## ------------------------------------------------------------------------ ## ## Desdobrando Inseticidas dentro de cada nivel de Doses ## ------------------------------------------------------------------------ ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr.Fc ## Doses 1 620.1667 620.1667 8.2141 0.0112 ## Inseticidas:Doses 1 3 2880.9167 960.3056 12.7193 0.0002 ## Inseticidas:Doses 2 3 1104.2500 368.0833 4.8753 0.0135 ## Residuo 16 1208.0000 75.5000 NA NA ## Total 23 5813.3333 252.7536 NA NA ## ------------------------------------------------------------------------ ## ## ## ## Inseticidas dentro do nivel 1 de Doses ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 4 74.33333 ## ab 3 59.66667 ## bc 1 50 ## c 2 31.66667 ## ------------------------------------------------------------------------ ## ## ## Inseticidas dentro do nivel 2 de Doses ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 1 57.66667 ## ab 2 47.66667 ## b 3 35 ## b 4 34.66667 ## ------------------------------------------------------------------------ ## ## ## ## Desdobrando Doses dentro de cada nivel de Inseticidas ## ------------------------------------------------------------------------ ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr.Fc ## Inseticidas 3 860.33333 286.77778 3.7984 0.0313 ## Doses:Inseticidas 1 1 88.16667 88.16667 1.1678 0.2959 ## Doses:Inseticidas 2 1 384.00000 384.00000 5.0861 0.0385 ## Doses:Inseticidas 3 1 912.66667 912.66667 12.0883 0.0031 ## Doses:Inseticidas 4 1 2360.16667 2360.16667 31.2605 0.0000 ## Residuo 16 1208.00000 75.50000 NA NA ## Total 23 5813.33333 252.75362 NA NA ## ------------------------------------------------------------------------ ## ## ## ## Doses dentro do nivel 1 de Inseticidas ## ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 50.00000 ## 2 2 57.66667 ## ------------------------------------------------------------------------ ## ## ## Doses dentro do nivel 2 de Inseticidas ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 2 47.66667 ## b 1 31.66667 ## ------------------------------------------------------------------------ ## ## ## Doses dentro do nivel 3 de Inseticidas ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 1 59.66667 ## b 2 35 ## ------------------------------------------------------------------------ ## ## ## Doses dentro do nivel 4 de Inseticidas ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 1 74.33333 ## b 2 34.66667 ## ------------------------------------------------------------------------ 19.4 Estudo do Fatorial \\(3^2\\) Nos experimentos fatoriais \\(3^2\\) ou \\(3 \\times 3\\), temos \\(2\\) fatores, cada um dos quais ocorre em \\(3\\) níveis. Os tratamentos são formados pelas combinações dos \\(3\\) níveis dos \\(2\\) fatores, resultando em \\(9\\) tratamentos. Como exemplo de um ensaio fatorial \\(3^2\\), vamos utilizar os dados obtidos do trabalho de graduação intitulado Efeitos do espaçamento e da densidade de semeadura na produção de massa verde e matéria seca em diferentes épocas e, na produção de sementes da cultura Crotalaria juncea L. realizado por LAMERS (1981). Neste trabalho, foram utilizado 3 espaçamentos entre linhas (25 cm, 50 cm e 75 cm) e 3 densidade de plantas por metro linear (15, 30 e 45 plantas por metro linear). O delineamento foi instalado em blocos casualizados com 3 repetições, e os dados obtidos para produção de massa verde (t/ha), 139 dias após a semeadura, foram os seguintes: Os dados pode ser acessados no link crotalária.txt. Espaçamento Densidade Bloco 1 Bloco 2 Bloco 3 Totais 25 15 46,82 30,705 59,77 137,295 25 30 31,04 28,41 25,1 84,55 25 45 47,325 50,445 29,01 126,78 50 15 26,3875 15,61 15,12 57,1175 50 30 32,765 33,615 32,115 98,495 50 45 37,455 21,4125 21,21 80,0775 75 15 12,6116 10,4015 26,2095 49,2226 75 30 23,4776 24,1842 18,1548 65,8166 75 45 26,3297 24,0652 33,8482 84,2431 Totais 284,2114 238,8484 260,5375 783,5973 Dados originais:DOWNLOAD 19.4.1 Obtenção da análise de variância A análise de variância preliminar é feita de acordo com o delineamento em blocos causalizado, com 9 tratamentos e 3 blocos: \\[ \\begin{aligned} SQ_{Total} &amp;= (46,8200^2+30,7050^2+\\cdots +33,8482^2)-\\frac{783,5973^2}{9 \\cdot 3} \\\\ &amp;=3544,9079 \\end{aligned} \\] \\[ \\begin{aligned} SQ_{Trat} &amp;= \\frac{1}{3} (137,2950^2+84,5500^2+\\cdots +84,2431^2)-\\frac{783,5973^2}{9 \\cdot 3} \\\\ &amp;=2358,1623 \\end{aligned} \\] \\[ \\begin{aligned} SQ_{Blocos} &amp;= \\frac{1}{9} (284,2114^2+238,8484^2+260,5375^2)-\\frac{783,5973^2}{9 \\cdot 3} \\\\ &amp;=114,3953 \\end{aligned} \\] \\[ \\begin{aligned} SQ_{Res} &amp;= SQ_{Total} - SQ_{Trat}- SQ_{Blocos} &amp;=3544,9079-2358,1623-114,3953=1072,3503 \\end{aligned} \\] Então, podemos montar o seguinte quadro de análise de variância: Causas de Variação GL SQ QM F Trat. 8 2358,1623 294,7703 4,40* Blocos 2 114,3953 57,1976 0,85 Res 16 1072,3503 67,0219 Total 26 3544,9079 Conclusão: O teste é siginificativo ao nível de \\(1\\%\\) de probabilidade, logo, rejeitamos a hipótese da nulidade (\\(H_0\\)), e concluímos que os efeitos dos tratamentos diferem entre sim em relação à característica analisada, com um grau de confiança superior a \\(99\\%\\) de probabilidade. Devemos agora, desdobrar a soma de quadrado e os graus de liberdade de tratamentos para estudar os efeitos principais e a interação entre os fatores. Para facilitar os cálculos, utilizamos um quadro auxiliar como o seguinte: Quadro de totais (3) \\(D_{15}\\) \\(D_{30}\\) \\(D_{45}\\) Total \\(E_{25}\\) 137.2950 84.5500 126.7800 348.6250 \\(E_{50}\\) 57.1175 98.4950 80.0775 235.6900 \\(E_{75}\\) 49.2226 65.8166 84.2431 199.2823 Total 243.6351 248.8616 291.1006 783.5973 Então, as somas de quadrados são obtidas da seguinte maneira: 1. Soma de quadrados devido ao efeito de Espaçamento: \\[ SQ_{Esp.} = \\frac{1}{r_E}[T_{E25}^2+T_{E50}^2+T_{E75}^2] - \\frac{G^2}{I\\cdot J} \\\\ SQ_{Esp.}= \\frac{1}{9}[348,6250^2+235,6900^2+199,2823^2] - \\frac{783,5973^2}{9 \\cdot 3} \\\\ SQ_{Esp.}= 1347,5214 \\] 2. Soma de quadrados devido ao efeito de Densidade: \\[ SQ_{Dens.} = \\frac{1}{r_D}[T_{D15}^2+T_{D30}^2+T_{D45}^2] - \\frac{G^2}{I\\cdot J} \\\\ SQ_{Dens.}= \\frac{1}{9}[243,635^2+248,8616^2+291,1006^2] - \\frac{783,5973^2}{27} \\\\ SQ_{Dens.}= 150,5342 \\] 3. Soma de quadrados devido ao efeito da Interação Espaçamento \\(\\times\\) Densidade: \\[ SQ_{Interação\\;E\\times D} = SQ_{E,D}-SQ_{E}-SQ_{D} \\\\ SQ_{I,D} = \\frac{1}{r_{ED}}(T_{E25D15}^2+T_{E25D30}^2+\\cdots +T_{E75D45}^2) - C \\\\ SQ_{I,D}= \\frac{1}{3}(137,2950^2+84,5500^2+\\cdots + 84,2431^2) - \\frac{783,5973^2}{27} \\\\ SQ_{I,D} = 2358,1623 \\\\ \\] assim, \\[ SQ_{Interação\\;E\\times D} = SQ_{E,D}-SQ_{E}-SQ_{D} \\\\ SQ_{Interação\\;E\\times D}=2358,1623-1347,5214-150,5342 \\\\ SQ_{Interação\\;E\\times D}=860,1068 \\] Portanto, temos o seguinte quadro de análise de variância: Causas de Variação GL SQ QM F Espaçamento (E) 2 1347.5214 673.7607 10.05** Densidade (D) 2 150.5342 75.2671 1.12 Interação (ExD) 4 860.1068 215.0267 3.21* (Tratamentos) 8 2358.1623   Bloco 2 114.3953 57.1976 0.85 Resíduos 16 1072.3503 67.0219  Total 26 3544.9079   Valores de F da tabela para Espaçamento e Densidade (\\(2 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,63 \\\\ 1\\%=6,23 \\end{cases}\\) Valores de F da tabela para Interação (\\(4 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,01 \\\\ 1\\%=4,77 \\end{cases}\\) Conclusões Para efeito de Espaçamento: O teste foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que existe diferença entre os espaçamentos em relação à produção de massa verde de Crotalaria juncea, com um grau de confiança superior a \\(99\\%\\) de probabilidade. Para efeito de Densidade: O teste não foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\) e concluir que as densidades de semeadura não possuem efeitos diferentes sobre a produção de massa verde de Crotalaria juncea. Para efeito da Interação (E \\(\\times\\) D): O teste foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\) e concluir que os fatores espaçamento e densidade de semeadura agem conjuntamente sobre a produção de massa verde de Crotalaria juncea. Como a interação \\(E \\times D\\) foi significativa, devemos portanto, desdobrar os graus de liberdade da interação para estudar os efeitos de cada um dos fatores dentro dos níveis do outro fator. 19.4.2 Desdobrando a interação \\(E \\times D\\), para estudar os efeitos do fator Espaçamento em cada nível do fator Densidade (E d. D): \\[ SQ_{Ed.D15} = \\frac{1}{3}(137,2950^2+57,1175^2+49,2226^2) - \\frac{243,6351^2}{9}=1583,0565 \\\\ SQ_{Ed.D30} = \\frac{1}{3}(84,5500^2+98,4950^2+65,8166^2) - \\frac{248,8616^2}{9}=179,2535 \\\\ SQ_{Ed.D45} = \\frac{1}{3}(126,7800^2+80,0775^2+84,2431^2) - \\frac{291,1006^2}{9}=455,3182 \\] Verificação: \\(SQ_{Ed.D15}+SQ_{Ed.D30}+SQ_{Ed.D45}=SQ_{E}+SQ_{E\\times D}\\) Então, o quadro de análise de variância com desdobramento da interação ExD, estudando-se o efeito de Espaçamento dentro de cada Densidade será o seguinte: Causas de Variação GL SQ QM F Espaçamento d. D15 2 1583.0565 791.5283 11.81* Espaçamento d. D30 2 179.2535 89.6267 1.34 Espaçamento d. D45 2 445.3182 222.6591 3.32 Densidades (D) 2 150.5342 75.2671 1.12 (Tratamentos) 8 2358.1623   Bloco 2 114.3953 57.1976 0.85 Resíduos 16 1072.3503 67.0219  Total 26 3544.9079   F da tabela (\\(2 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,63 \\\\ 1\\%=6,23 \\end{cases}\\) Conclusões Para Espaçamento dentro de Densidade 15: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que existe diferença entre os espaçamentos em relação à produção de massa verde de Crotalaria juncea, com um grau de confiança superior a \\(99\\%\\) de probabilidade. Para Espaçamento dentro de Densidade 30 e 45: Os valores do teste F foram não significativos, indicando que os espaçamentos não diferem entre si em relação à produção de massa verde de Crotalaria juncea. 19.4.3 Desdobrando a interação \\(E\\times D\\), para estudar os efeitos do fator Densidade em cada nível do fator Espaçamentos (D d. E): \\[ SQ_{Dd.E25} = \\frac{1}{3}(137.2950^2+84.55^2+126.78^2) - \\frac{348.6250^2}{9}=519.5526 \\\\ SQ_{Dd.E50} = \\frac{1}{3}(57.1175^2+98.4950^2+80.0775^2) - \\frac{235.6900^2}{9}=286.4959 \\\\ SQ_{Dd.E75} = \\frac{1}{3}(49.2226^2+65.8166^2+84.2431^2) - \\frac{199.2823^2}{9}=204.5925 \\] Verificação: \\(SQ_{Dd.E25}+SQ_{Dd.D50}+SQ_{Dd.D75}=SQ_{D}+SQ_{E\\times D}\\) Então, o quadro de análise de variância com desdobramento da interação ExD, estudando-se o efeito de Densidade dentro de cada Espaçamento será o seguinte: Causas de Variação GL SQ QM F Densidade d. E25 2 519.5526 259.5526 3.88* Densidade d. E50 2 286.4959 143.2480 2.14 Densidade d. E75 2 204.5925 102.2962 1.53 Espaçamento (E) 2 1347.5214 673.7607 10.05** (Tratamentos) 8 2358.1623   Bloco 2 114.3953 57.1976 0.85 Resíduos 16 1072.3503 67.0219  Total 26 3544.9079   F da tabela (\\(2 \\times 16 GL\\)): \\(\\begin{cases}5\\%=3,63 \\\\ 1\\%=6,23 \\end{cases}\\) Conclusões Para Densidade dentro de Espaçamento 25: O teste F foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que existe diferença entre densidades, dentro do espaçamento de 25 cm entre linhas, em realação à produção de massa verde de Crotalaria juncea, com um grau de confiança superior a \\(95\\%\\) de probabilidade. Para Densidade dentro de Espaçamento 50 e 75: Os valores do teste F foram não significativos, indicando que as densidades não diferem entre si, dentro destes espaçamentos, em relação à produção de massa verde de Crotalaria juncea. Para completar a análise de variância, e obter conclusões mais específicas sobre o efeito dos espaçamentos em cada densidade, podemos aplicar um teste de comparação de médias. 19.4.4 Teste de Tukey para a comparação de médias Construção do Quadro de Médias a partir do Quadro de totais: Quadro auxiliar de totais: (3) \\(D_{15}\\) \\(D_{30}\\) \\(D_{45}\\) Total \\(E_{25}\\) 137,2950/3 84,5500/3 126,7800/3 348,6250/9 \\(E_{50}\\) 57,1175/3 98,4950/3 80,0775/3 235,6900/9 \\(E_{75}\\) 49,2226/3 65,8166/3 84,2431/3 199,2823/9 Total 243,6351/9 248,8616/9 291,100/9 783,5973/27 Quadro de médias: \\(D_{15}\\) \\(D_{30}\\) \\(D_{45}\\) Média Espaçamento \\(E_{25}\\) 45,7650 28,1833 42,2600 38,7361 \\(E_{50}\\) 19,0392 32,8317 26,6925 26,1878 \\(E_{75}\\) 16,4075 21,9389 28,0810 22,1425 Média Densidade 27,0706 27,6513 32,3445 29,0221 19.4.4.1 1. Para comparar médias de Espaçamento na Densidade 15 (E d. \\(D_15\\)) \\[ DMS=q \\cdot s(m) = q_{(3 \\times 16 GL)}\\cdot \\sqrt{\\frac{QM_{Res}}{r}}=3,65\\cdot \\sqrt{\\frac{67,0219}{3}}=17,2520\\;t/ha \\] 19.4.4.2 2. Para comparar médias de densidades no Espaçamento 25 (D d. \\(E_25\\)) \\[ DMS=q \\cdot s(m) = q_{(3 \\times 16 GL)}\\cdot \\sqrt{\\frac{QM_{Res}}{r}}=3,65\\cdot \\sqrt{\\frac{67,0219}{3}}=17,2520\\;t/ha \\] 19.4.4.3 Resultado do teste de Tukey Médias seguidas pela mesma letra, minúsculas nas linhas e maiúsculas nas colunas, não diferem entre si pelo teste de Tukey ao nível de \\(5\\%\\) de probabilidade. \\(D_{15}\\) \\(D_{30}\\) \\(D_{45}\\) Média Espaçamento \\(E_{25}\\) 45,7650 Aa 28,1833 Ab 42,2600 Aab 38,7361 \\(E_{50}\\) 19,0392 Ba 32,8317 Aa 26,6925 Aa 26,1878 \\(E_{75}\\) 16,4075 Ba 21,9389 Aa 28,0810 Aa 22,1425 Média Densidade 27,0706 27,6513 32,3445 29,0221 19.4.4.4 Cálculo do coeficiente de variação do experimento \\[ CV=100\\cdot \\frac{\\sqrt{QM_{res}}}{\\hat{m}}=100\\cdot \\frac{8,19}{29,0221}=28,21\\% \\] ### Aplicação no R # Carregando o pacote para análise de variância library(ExpDes.pt) caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/crotalaria.txt&quot; d&lt;-read.table(caminho,h=TRUE) esp&lt;-factor(d$Espaçamento) den&lt;-factor(d$Densidade) bloco&lt;-factor(d$Bloco) y&lt;-d$y interaction.plot(esp,den,y,mean) interaction.plot(den,esp,y,mean) fat2.dbc(esp,den,bloco,y,fac.names = c(&quot;Espaçamento&quot;, &quot;Densidade&quot;)) ## ------------------------------------------------------------------------ ## Legenda: ## FATOR 1: Espaçamento ## FATOR 2: Densidade ## ------------------------------------------------------------------------ ## ## ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Bloco 2 114.4 57.20 0.8535 0.44444 ## Espaçamento 2 1347.5 673.75 10.0527 0.00149 ## Densidade 2 150.5 75.26 1.1230 0.34964 ## Espaçamento*Densidade 4 860.1 215.02 3.2083 0.04101 ## Residuo 16 1072.3 67.02 ## Total 26 3544.9 ## ------------------------------------------------------------------------ ## CV = 28.21 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos (Shapiro-Wilk) ## valor-p: 0.541538 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ## ## Interacao significativa: desdobrando a interacao ## ------------------------------------------------------------------------ ## ## Desdobrando Espaçamento dentro de cada nivel de Densidade ## ------------------------------------------------------------------------ ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr.Fc ## Bloco 2 114.4004 57.20020 0.8535 0.4444 ## Densidade 2 150.5283 75.26417 1.123 0.3496 ## Espaçamento:Densidade 15 2 1583.0186 791.50931 11.8098 0.0007 ## Espaçamento:Densidade 30 2 179.2489 89.62444 1.3372 0.2904 ## Espaçamento:Densidade 45 2 445.3134 222.65669 3.3222 0.0621 ## Residuo 16 1072.3451 67.02157 NA ## Total 26 3544.8547 136.34057 NA ## ------------------------------------------------------------------------ ## ## ## ## Espaçamento dentro do nivel 15 de Densidade ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 1 45.765 ## b 2 19.03933 ## b 3 16.408 ## ------------------------------------------------------------------------ ## ## ## Espaçamento dentro do nivel 30 de Densidade ## ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 28.18333 ## 2 2 32.83167 ## 3 3 21.93900 ## ------------------------------------------------------------------------ ## ## ## Espaçamento dentro do nivel 45 de Densidade ## ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 42.26000 ## 2 2 26.69267 ## 3 3 28.08100 ## ------------------------------------------------------------------------ ## ## ## ## Desdobrando Densidade dentro de cada nivel de Espaçamento ## ------------------------------------------------------------------------ ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr.Fc ## Bloco 2 114.4004 57.20020 0.8535 0.4444 ## Espaçamento 2 1347.4923 673.74615 10.0527 0.0015 ## Densidade:Espaçamento 25 2 519.5526 259.77629 3.876 0.0424 ## Densidade:Espaçamento 50 2 286.4893 143.24465 2.1373 0.1504 ## Densidade:Espaçamento 75 2 204.5751 102.28753 1.5262 0.2474 ## Residuo 16 1072.3451 67.02157 NA ## Total 26 3544.8547 136.34057 NA ## ------------------------------------------------------------------------ ## ## ## ## Densidade dentro do nivel 25 de Espaçamento ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 1 45.765 ## ab 3 42.26 ## b 2 28.18333 ## ------------------------------------------------------------------------ ## ## ## Densidade dentro do nivel 50 de Espaçamento ## ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 19.03933 ## 2 2 32.83167 ## 3 3 26.69267 ## ------------------------------------------------------------------------ ## ## ## Densidade dentro do nivel 75 de Espaçamento ## ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 16.408 ## 2 2 21.939 ## 3 3 28.081 ## ------------------------------------------------------------------------ "],["estudo-do-fatorial-23.html", "20 Estudo do Fatorial \\(2^3\\) 20.1 Análise de variância de um experimento fatorial \\(2^3\\) 20.2 Cálculo das somas de quadrados pelo método dos contrastes de totais de tratamentos. 20.3 Desdobramento da Interação \\(N \\times K\\)", " 20 Estudo do Fatorial \\(2^3\\) No fatorial \\(2^3\\), podemos estudar os efeitos de \\(3\\) fatores, cada um dos quais em \\(2\\) níveis. Assim, por exemplo, num ensaio de adubação NPK, podemos ter: Fatores: N - Nitrogênio P - Fósforo K - Potássio Níveis: 0 - Ausência do nutriente 1 - Presença do nutriente Neste caso, teremos os seguintes tratamentos: \\(N_0 P_0 K_0\\) - 000 - Testemunha \\(N_1 P_0 K_0\\) - 100 - N \\(N_0 P_1 K_0\\) - 010 - P \\(N_0 P_0 K_1\\) - 001 - K \\(N_1 P_1 K_0\\) - 110 - NP \\(N_1 P_0 K_1\\) - 101 - NK \\(N_0 P_1 K_1\\) - 011 - PK \\(N_1 P_1 K_1\\) - 111 - NPK Estes 8 tratamentos devem ser ditribuídos de acordo com um delineamento experimental qualquer, como por exemplo, DIC, DBC, etc, e após a análise de variância preliminar, realizada de acordo com o delineamento adotado, devemos desdobrar os graus 7 graus de liberdade de tratamentos da seguinte forma: Efeito de \\(N\\)..1 g.l. Efeito de \\(P\\)..1 g.l. Efeito de \\(K\\)..1 g.l. Efeito da Interação \\(N \\times P\\)..1 g.l. Efeito da Interação \\(N \\times K\\).1 g.l. Efeito da Interação \\(P \\times K\\)..1 g.l. Efeito da Interação \\(N \\times P \\times K\\)1 g.l. _____________________________________ (Tratamentos) ..(7 g.l.) 20.1 Análise de variância de um experimento fatorial \\(2^3\\) Para obtenção da análise de variância, vamos supor o seguinte ensaio em que se estudou o efeito da adubação NPK na cultura do cafeeiro. As produções de café coco, em kg por parcela de 105 \\(m^2\\) (12 covas no espaçamento de \\(3,5 \\times 2,5\\;m\\)) foram: Os dados podem ser encontrados em cafeeito3fatores.txt Tratamentos Bloco 1 Bloco 2 Bloco 3 Bloco 4 Bloco 5 Bloco 6 Total \\(N_0P_0K_0\\) 31,8 40,5 25,7 25,7 37,2 45,3 206,2 \\(N_1P_0K_0\\) 35,3 39,0 36,0 33,5 28,2 42,4 214,4 \\(N_0P_1K_0\\) 36,2 37,8 40,9 44,8 32,4 38,4 230,5 \\(N_0P_0K_1\\) 25,6 32,4 39,6 48,9 20,6 33,7 200,8 \\(N_1P_1K_0\\) 43,8 32,7 43,3 41,8 31,9 37,7 231,2 \\(N_1P_0K_1\\) 51,5 66,1 51,7 52,0 56,5 58,2 336,0 \\(N_0P_1K_1\\) 37,1 53,0 36,4 43,0 19,7 30,4 219,6 \\(N_1P_1K_1\\) 47,0 49,9 50,9 49,1 71,7 39,6 308,2 Total 308,3 351,4 324,5 338,8 298,2 325,7 1946,9 Dados originais:DOWNLOAD O ensaio foi montado de acordo com o delineamento em blocos casualizados e, portanto, a análise de variância preliminar, obtida da maneira usual, foi a seguinte: Causas de Variação GL SQ QM F Tratamentos 7 2949,18 421,31 6,38** Blocos 5 235,45 47,09 0,71 Resíduo 35 2310,92 66,03 Total 47 5495,55 Conclusão: O teste F para tratmentos foi significativo ao nível de \\(1\\%\\) de probabilidade, logo, rejeitamos a hipótese da nulidade (\\(H_0\\)), e concluímos que os efeitos dos tratamentos diferentes entre si em relação à produção da cultura do cafeeiro, com um grau de confiança superior a \\(99\\%\\) de probabilidade. Devemos agora, desdobrar a soma de quadrado e os graus de liberdade de tratamentos para estudar os efeitos principais e os efeitos das interações entre os fatores. Para estudo do fatorial \\(2^3\\), podemos utilizar o método dos contrastes de totais de tratamentos ou o método dos totais de tratamentos (sem utilizar contraste) para a obtenção das somas de quadrados. 20.2 Cálculo das somas de quadrados pelo método dos contrastes de totais de tratamentos. Para a obtenção dos contrastes, organizamos uma tabela de dupla entrada onde as linhas correspondem aos efeitos e as colunas aos tratamentos. Então, no nosso exemplo, temos: Efeitos \\(N_0P_0K_0\\) \\(N_1P_0K_0\\) \\(N_0P_1K_0\\) \\(N_0P_0K_1\\) \\(N_1P_1K_0\\) \\(N_1P_0K_1\\) \\(N_0P_1K_1\\) \\(N_1P_1K_1\\) \\(\\hat{Y}\\) N - + - - + + - + 232,70 P - - + - + - + + 32,1 K - - - + - + + + 182,3 NxP + - - + + - - + -54,1 NxK + - + - - + - + 214,9 PxK + + - - - - + + -50,1 NxPxK - + + + - - - + -39,1 Totais 206,2 214,4 230,5 200,8 231,2 336 219,6 308.2 Neste quadro, os contrastes são obtidos, tomando-se para cada efeito, os totais dos tratamentos, com os quais os sinais correspondentes, ou seja: Contraste para N: \\(\\hat{Y}_{N}= -T_{N_0P_0K_0} +T_{N_1P_0K_0} -T_{N_0P_1K_0} -T_{N_0P_0K_1} +T_{N_1P_1K_0} +T_{N_1P_0K_1} -T_{N_0P_1K_1} +T_{N_1P_1K_1}\\) \\(\\hat{Y}_{N}= -206.2 +214.4 -230.5 -200.8 +231.2 +336 -219.6 +308.2 \\\\ \\hat{Y}_{N} = 232.7\\) Contraste para P: \\(\\hat{Y}_{P}= -T_{N_0P_0K_0} -T_{N_1P_0K_0} +T_{N_0P_1K_0} -T_{N_0P_0K_1} +T_{N_1P_1K_0} -T_{N_1P_0K_1} +T_{N_0P_1K_1} +T_{N_1P_1K_1}\\) \\(\\hat{Y}_{P}= -206.2 -214.4 +230.5 -200.8 +231.2 -336 +219.6 +308.2 \\\\ \\hat{Y}_{P} = 32.1\\) Contraste para K: \\(\\hat{Y}_{K}= -T_{N_0P_0K_0} -T_{N_1P_0K_0} -T_{N_0P_1K_0} +T_{N_0P_0K_1} -T_{N_1P_1K_0} +T_{N_1P_0K_1} +T_{N_0P_1K_1} +T_{N_1P_1K_1}\\) \\(\\hat{Y}_{K}= -206.2 -214.4 -230.5 +200.8 -231.2 +336 +219.6 +308.2 \\\\ \\hat{Y}_{K}= 182.3\\) Contraste para NP: \\(\\hat{Y}_{NP}= +T_{N_0P_0K_0} -T_{N_1P_0K_0} -T_{N_0P_1K_0} +T_{N_0P_0K_1} +T_{N_1P_1K_0} -T_{N_1P_0K_1} -T_{N_0P_1K_1} +T_{N_1P_1K_1}\\) \\(\\hat{Y}_{NP}= +206.2 -214.4 -230.5 +200.8 +231.2 -336 -219.6 +308.2 \\\\ \\hat{Y}_{NP}= -54.1\\) Contraste para NK: \\(\\hat{Y}_{NK}= +T_{N_0P_0K_0} -T_{N_1P_0K_0} +T_{N_0P_1K_0} -T_{N_0P_0K_1} -T_{N_1P_1K_0} +T_{N_1P_0K_1} -T_{N_0P_1K_1} +T_{N_1P_1K_1}\\) \\(\\hat{Y}_{NK}= +206.2 -214.4 +230.5 -200.8 -231.2 +336 -219.6 +308.2 \\\\ \\hat{Y}_{NK}= 214.9\\) Contraste para PK: \\(\\hat{Y}_{PK}= +T_{N_0P_0K_0} +T_{N_1P_0K_0} -T_{N_0P_1K_0} -T_{N_0P_0K_1} -T_{N_1P_1K_0} -T_{N_1P_0K_1} +T_{N_0P_1K_1} +T_{N_1P_1K_1}\\) \\(\\hat{Y}_{PK}= +206.2 +214.4 -230.5 -200.8 -231.2 -336 +219.6 +308.2 \\\\ \\hat{Y}_{PK}= -50.1\\) Contraste para NPK: \\(\\hat{Y}_{NPK}= -T_{N_0P_0K_0} +T_{N_1P_0K_0} +T_{N_0P_1K_0} +T_{N_0P_0K_1} -T_{N_1P_1K_0} -T_{N_1P_0K_1} -T_{N_0P_1K_1} +T_{N_1P_1K_1}\\) \\(\\hat{Y}_{NPK}= -206.2 +214.4 +230.5 +200.8 -231.2 -336 -219.6 +308.2 \\\\ \\hat{Y}_{NPK}= -39.1\\) E a soma de quadrados correspondente a um dado contraste \\(Y_i\\) é dadi por: \\[ SQ_{Y_{i}} = \\frac{\\hat{Y}_i^2}{r\\sum_{i=1}^Ic_i^2} \\] onde, \\(\\hat{Y}_i\\) é a estimativa do contraste. \\(r\\) é o número de repetições com que foram obtidos os totais de tratamentos. \\(\\sum_{i=1}^Ic_i^2\\) é a soma dos quadrados dos coeficientes dos totais de tratamentos no contraste. Então, no nosso exemplo, temos: \\[ SQ_{N} = \\frac{\\hat{Y}_N^2}{8\\cdot r}=\\frac{(232,70)^2}{8\\cdot 6}=1128,11 \\\\ SQ_{P} = \\frac{\\hat{Y}_P^2}{8\\cdot r}=\\frac{(31,10)^2}{8\\cdot 6}=21,47 \\\\ SQ_{K} = \\frac{\\hat{Y}_K^2}{8\\cdot r}=\\frac{(182,30)^2}{8\\cdot 6}=692,36 \\\\ SQ_{NP} = \\frac{\\hat{Y}_{NP}^2}{8\\cdot r}=\\frac{(-54,10)^2}{8\\cdot 6}=60,98 \\\\ SQ_{NK} = \\frac{\\hat{Y}_{NK}^2}{8\\cdot r}=\\frac{(214,90)^2}{8\\cdot 6}=962,13 \\\\ SQ_{PK} = \\frac{\\hat{Y}_{PK}^2}{8\\cdot r}=\\frac{(-50,10)^2}{8\\cdot 6}=52,29 \\\\ SQ_{NPK} = \\frac{\\hat{Y}_{NPK}^2}{8\\cdot r}=\\frac{(-39,10)^2}{8\\cdot 6}=31,85 \\] Uma vez obtidas as somas de quadrados, podemos montar o seguine quadro de análise de variância Causas de Variação GL SQ QM F Efeito de N 1 11238,11 11238,11 17,08** Efeito de P 1 21,47 21,47 0,33 Efeito de K 1 692,36 692,36 10,49** Efeito de NP 1 60,98 60,98 0,92 Efeito NK 1 962,13 962,13 14,57** Efeito de PK 1 52,29 52,29 0,79 Efeito de NPK 1 31,84 31,84 0,48 (Tratamentos) 7 2949,18   Blocos 5 235,45 47,09 0,71 Resíduo 35 2310,92 66,03  Total 47 5495,55   Valores de F da tabela (\\(1 \\times 35 GL\\)): \\(\\begin{cases}5\\%=4,12 \\\\ 1\\%=7,43 \\end{cases}\\) 20.2.1 Conclusões a) Para efeito de N: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\), e concluir que os níveis de \\(N_0\\) e \\(N_1\\) possuem efeitos diferentes sobre a produção do cafeeiro. b) Para efeito de P: O teste F foi não significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\), e concluir que os níveis de \\(P_0\\) e \\(P_1\\) não diferem entre si em relação à produção do cafeeiro. c) Para efeito de K: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\), e concluir que os níveis de \\(K_0\\) e \\(K_1\\) possuem efeitos diferentes sobres a produção do cafeeiro. d) Para efeito da interação NP: O teste F foi não significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\), e concluir que os os fatores \\(N\\) e \\(P\\) agem independentemente sobre a produção do cafeeiro. e) Para efeito da interação NK: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\), e concluir que os os fatores \\(N\\) e \\(K\\) não agem independentemente sobre a produção do cafeeiro. f) Para efeito da interação PK: O teste F foi não significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\), e concluir que os os fatores \\(P\\) e \\(K\\) agem independentemente sobre a produção do cafeeiro. g) Para efeito da interação NPK: O teste F foi não significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\), e concluir que os os fatores \\(N\\), \\(P\\) e \\(K\\) agem independentemente sobre a produção do cafeeiro. Como o teste F para a interação \\(N \\times K\\) foi significativa, isto indica que o efeito de N depende de qual o nível de K que está sendo utilizado, e o efeito de K depende de qual nível de N que está sendo utilizado. Assim sendo, devemos desdobrar os graus de liberdade da interação \\(N \\times K\\), para estudar os efeitos de N dentro de cada nível de K, e os efeitos de K dentro de cada nível de N. 20.3 Desdobramento da Interação \\(N \\times K\\) Para obter as somas de quadrados para o desdobramento da interação \\(N \\times K\\), vamos utilizar o quadro auxilliar. (12) \\(K_0\\) \\(K_1\\) TOTAL \\(N_0\\) 436,70 420,40 857,10 \\(N_1\\) 445,60 644,20 1089,80 TOTAL 882,30 1064,60 1946,90 20.3.1 1) Desdobramento da Interação NK para estudar os efeitos de N dentro de K (N d. K). \\[ \\begin{align} SQ_{N\\;d.\\;K_0}&amp;=\\frac{1}{2 \\cdot r}(T_{N_0K_0}^2+T_{N_1K_0}^2) - \\frac{T_{K_0}^2}{4 \\cdot r} \\\\ &amp;= \\frac{1}{12}(436,70^2+445,60^2) - \\frac{882,30^2}{24} \\\\ &amp;=3,30 \\end{align} \\] \\[ \\begin{align} SQ_{N\\;d.\\;K_1}&amp;=\\frac{1}{2 \\cdot r}(T_{N_0K_1}^2+T_{N_1K_1}^2) - \\frac{T_{K_1}^2}{4 \\cdot r} \\\\ &amp;= \\frac{1}{12}(420.40^2+644.20^2) - \\frac{1064.60^2}{24} \\\\ &amp;=2086.93 \\end{align} \\] Então, o quadro de análise de variância com o desdobramento da interação NK para estudar os efeitos de N d. K, será o seguinte: Causas de Variação GL SQ QM F Efeito de N d K0 1 3,30 3,30 0,05 Efeito de N d K1 1 2086,93 2086,93 31,61 Efeito de P 1 21,47 21,47 0,33 Efeito de K 1 692,36 692,36 10,49** Efeito de NP 1 60,98 60,98 0,92 Efeito de PK 1 52,29 52,29 0,79 Efeito de NPK 1 31,84 31,84 0,48 (Tratamentos) 7 2949,18   Blocos 5 235,45 47,09 0,71 Resíduo 35 2310,92 66,03  Total 47 5495,55   Valores de F da tabela (\\(1 \\times 35 GL\\)): \\(\\begin{cases}5\\%=4,12 \\\\ 1\\%=7,43 \\end{cases}\\) 20.3.2 Conclusões Para Efeito de N d. K0: O teste F foi não significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\), e concluir que o nível de \\(N_0\\) e \\(N_1\\) não diferem entre si, na ausência de K, em relação à produção do cafeeiro. Para Efeito de N d. K1: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\), e concluir que o nível de \\(N_0\\) e \\(N_1\\) diferem entre si, na presença de K, em relação à produção do cafeeiro. 20.3.3 2) Desdobramento da Interação NK para estudar os efeitos de K dentro de N (K d. N). \\[ \\begin{align} SQ_{K\\;d.\\;N_0}&amp;=\\frac{1}{2 \\cdot r}(T_{N_0K_0}^2+T_{N_0K_1}^2) - \\frac{T_{N_0}^2}{4 \\cdot r} \\\\ &amp;= \\frac{1}{12}(436,70^2+420,40^2) - \\frac{857,10^2}{24} \\\\ &amp;=11,07 \\end{align} \\] \\[ \\begin{align} SQ_{K\\;d.\\;N_1}&amp;=\\frac{1}{2 \\cdot r}(T_{N_1K_0}^2+T_{N_1K_1}^2) - \\frac{T_{N_1}^2}{4 \\cdot r} \\\\ &amp;= \\frac{1}{12}(445,60^2+644,20^2) - \\frac{1089,80^2}{24} \\\\ &amp;=1643,41 \\end{align} \\] Então, o quadro de análise de variância com o desdobramento da interação NK para estudar os efeitos de K d. N, será o seguinte: Causas de Variação GL SQ QM F Efeito de N 1 1128,11 1128,11 17,08** Efeito de P 1 21,47 21,47 0,33 Efeito de K d N0 1 11,07 11,07 0,17 Efeito de K d N1 1 1643,41 1643,41 24,89** Efeito de NP 1 60,98 60,98 0,92 Efeito de PK 1 52,29 52,29 0,79 Efeito de NPK 1 31,84 31,84 0,48 (Tratamentos) 7 2949,18   Blocos 5 235,45 47,09 0,71 Resíduo 35 2310,92 66,03  Total 47 5495,55   Valores de F da tabela (\\(1 \\times 35 GL\\)): \\(\\begin{cases}5\\%=4,12 \\\\ 1\\%=7,43 \\end{cases}\\) 20.3.4 Conclusões Para Efeito de K d. N0: O teste F foi não significativo ao nível de \\(5\\%\\) de probabilidade, indicando que não devemos rejeitar \\(H_0\\), e concluir que o nível de \\(K_0\\) e \\(K_1\\) não diferem entre si, na ausência de N, em relação à produção do cafeeiro. Para Efeito de K d. N1: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar \\(H_0\\), e concluir que o nível de \\(K_0\\) e \\(K_1\\) diferem entre si, na presença de N, em relação à produção do cafeeiro. 20.3.5 Cálculo das médias e erros padrões das médias As médias dos efeitos principais de N, P e K serão obtidas por: \\[ \\hat{m}_{N_0} = \\frac{T_{N_0}}{4\\cdot r} = \\frac{857,10}{4\\cdot 6} = 35,71\\;kg\\;parcela^{-1} \\\\ \\hat{m}_{N_1} = \\frac{T_{N_1}}{4\\cdot r} = \\frac{1089,80}{4\\cdot 6} = 45,41\\;kg\\;parcela^{-1} \\\\ \\hat{m}_{P_0} = \\frac{T_{P_0}}{4\\cdot r} = \\frac{957,4}{4\\cdot 6} = 39,89\\;kg\\;parcela^{-1} \\\\ \\hat{m}_{P_1} = \\frac{T_{P_1}}{4\\cdot r} = \\frac{989,50}{4\\cdot 6} = 41,23\\;kg\\;parcela^{-1} \\\\ \\hat{m}_{K_0} = \\frac{T_{K_0}}{4\\cdot r} = \\frac{882,30}{4\\cdot 6} = 36,76\\;kg\\;parcela^{-1} \\\\ \\hat{m}_{K_1} = \\frac{T_{K_1}}{4\\cdot r} = \\frac{1064,60}{4\\cdot 6} = 44,36\\;kg\\;parcela^{-1} \\] Erro padrão das médias \\[ s(\\hat{m})=\\frac{s}{\\sqrt{4\\cdot r}} = \\sqrt{\\frac{QM_{Res}}{4\\cdot6}}=\\sqrt{\\frac{66,03}{24}}=1,66\\;kg\\;parcela^{-1} \\] Como a interação NK foi significativa, é interessante obter também, as médias dos níveis de N d. K ou dos níveis de K d. N, ou seja: \\[ \\hat{m}_{N_0\\;d.\\;K_0} = \\frac{T_{N_0K_0}}{2\\cdot r} = \\frac{436,70}{2\\cdot 6} = 36,39\\;kg\\;parcela^{-1} \\\\ \\hat{m}_{N_1\\;d.\\;K_0} = \\frac{T_{N_1K_0}}{2\\cdot r} = \\frac{445,60}{2\\cdot 6} = 37,13\\;kg\\;parcela^{-1} \\\\ \\hat{m}_{N_0\\;d.\\;K_1} = \\frac{T_{N_0K_1}}{2\\cdot r} = \\frac{420,40}{2\\cdot 6} = 35,03\\;kg\\;parcela^{-1} \\\\ \\hat{m}_{N_1\\;d.\\;K_1} = \\frac{T_{N_1K_1}}{2\\cdot r} = \\frac{644,20}{2\\cdot 6} = 53,68\\;kg\\;parcela^{-1} \\] Erro padrão das médias \\[ s(\\hat{m})=\\frac{s}{\\sqrt{2\\cdot r}} = \\sqrt{\\frac{QM_{Res}}{2\\cdot6}}=\\sqrt{\\frac{66,03}{12}}=2,35\\;kg\\;parcela^{-1} \\] 20.3.6 Cálculo do coeficiente de variação \\[ \\hat{m} = \\frac{G}{IJ} = \\frac{1946,90}{8\\cdot6} = 40,56\\;kg\\;parcela^{-1} \\\\ s=\\sqrt{QM_{Res}}=\\sqrt{66,03}=8,13\\;kg\\;parcela^{-1} \\\\ CV=100\\cdot \\frac{8,13}{40,56}=20,04\\% \\] # Análise prelinimar require(ExpDes.pt) caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/cafeeiro3fatores.txt&quot; dados&lt;-read.table(caminho, header = TRUE) dados$Trat&lt;-paste(dados$N,dados$P,dados$K,sep=&quot;&quot;) dbc(dados$Trat,dados$Bloco,dados$Y) ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Tratamento 7 2949.2 421.31 6.3810 0.00007 ## Bloco 5 235.5 47.09 0.7132 0.61765 ## Residuo 35 2310.9 66.03 ## Total 47 5495.6 ## ------------------------------------------------------------------------ ## CV = 20.03 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos ## valor-p: 0.1204836 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## Teste de homogeneidade de variancia ## valor-p: 0.4223185 ## De acordo com o teste de oneillmathews a 5% de significancia, as variancias podem ser consideradas homogeneas. ## ------------------------------------------------------------------------ ## ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 101 56 ## ab 111 51.36667 ## bc 110 38.53333 ## bc 010 38.41667 ## bc 011 36.6 ## c 100 35.73333 ## c 000 34.36667 ## c 001 33.46667 ## ------------------------------------------------------------------------ # Análise com desdobramento para estudo dos fatores e interações fat3.dbc(dados$N,dados$P,dados$K,dados$Bloco,dados$Y,fac.names = c(&quot;N&quot;,&quot;P&quot;,&quot;K&quot;)) ## ------------------------------------------------------------------------ ## Legenda: ## FATOR 1: N ## FATOR 2: P ## FATOR 3: K ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Bloco 5 235.45854 47.09171 0.7132 0.6177 ## N 1 1128.11021 1128.11021 17.0858 2e-04 ## P 1 21.46688 21.46688 0.3251 0.5722 ## K 1 692.36021 692.36021 10.4861 0.0026 ## N*P 1 60.97521 60.97521 0.9235 0.3431 ## N*K 1 962.12521 962.12521 14.5719 5e-04 ## P*K 1 52.29188 52.29188 0.792 0.3796 ## N*P*K 1 31.85021 31.85021 0.4824 0.4919 ## Residuo 35 2310.91646 66.02618 ## Total 42 5495.55479 ## ------------------------------------------------------------------------ ## CV = 20.03 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos (Shapiro-Wilk) ## valor-p: 0.1204836 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ## ## Interacao N*K significativa: desdobrando a interacao ## ------------------------------------------------------------------------ ## ## Desdobrando N dentro de cada nivel de K ## ------------------------------------------------------------------------ ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## N:K 0 1 3.30042 3.30042 0.05 0.8244 ## N:K 1 1 2086.93500 2086.93500 31.6077 0 ## Residuo 35 2310.91646 66.02618 ## ------------------------------------------------------------------------ ## ## ## ## N dentro do nivel 0 de K ## ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 0 36.39167 ## 2 1 37.13333 ## ------------------------------------------------------------------------ ## ## ## N dentro do nivel 1 de K ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 1 53.68333 ## b 0 35.03333 ## ------------------------------------------------------------------------ ## ## ## ## Desdobrando K dentro de cada nivel de N ## ------------------------------------------------------------------------ ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## K:N 0 1 11.07042 11.07042 0.1677 0.6847 ## K:N 1 1 1643.41500 1643.41500 24.8904 0 ## Residuo 35 2310.91646 66.02618 ## ------------------------------------------------------------------------ ## ## ## ## K dentro do nivel 0 de N ## ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 0 36.39167 ## 2 1 35.03333 ## ------------------------------------------------------------------------ ## ## ## K dentro do nivel 1 de N ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 1 53.68333 ## b 0 37.13333 ## ------------------------------------------------------------------------ ## ## Analisando os efeitos simples do fator P ## ------------------------------------------------------------------------ ## P ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 0 39.89167 ## 2 1 41.22917 ## ------------------------------------------------------------------------ "],["experimento-em-parcelas-subdivididas.html", "21 Experimento em Parcelas Subdivididas", " 21 Experimento em Parcelas Subdivididas Os experimentos em parcelas subdivididas, também conhecidos como Split-plot, são utilizados quando, em um mesmo ensaio, queremos testar os efeitos de \\(2\\) ou mais fatores, mas em condições experimentais um pouco diferentes daquelas utilizadas nos experimentos fatoriais. Por exemplo: \\(4\\) Variedades e \\(3\\) Níveis de Adubação \\(3\\) Níveis de Irrigação e \\(4\\) Níveis de Adubação \\(3\\) Espaçamentos e \\(4\\) densidades de Semeadura etc. As unidades experimentais ou parcelas, são divididas em partes menores e iguais, chamadas de subparcelas. As parcelas podem ser ditribuídas de acordo com um delineamento qualquer, ou seja, inteiramente ao acaso ou blocos casualizados. A principal característica deste delineamento é a casualização dos tratamentos, que é feita em \\(2\\) estágios. No primeiro estágio, é feita a casualização dos níveis do fator testado, nas parcelas, de acordo com o delineamento adotado. No segundo estágio, em cada parcela, é feita a casualização dos níveis do fator que será testados na subparcelas. Denominamos de tratamentos principais ou tratamento primários, aqueles que são colocados nas parcelas, e de tratamento secundários ou subtratamentos aqueles que são colocados nas subparcelas. Nesses experimento, temos \\(2\\) resíduos: O resíduo a, que serve como base de comparação para os tratamentos principais, e o resíduo b, que serve como base de comparação para os tratamentos secundários e para a interação \\(P \\times S\\). Em consequência do tipo de casualização feita, o erro experimental devido aos tratamentos secundários ( \\(QM\\) Resíduo b), geralmente é menor que o erro experimental devido aos tratamentos principais ( \\(QM\\) Resíduo a). Dessa maneira, os efeitos dos tratamentos principais são determinados com menor precisão que os efeitos dos tratamentos secundários. Assim, por exemplo, num experimento em parcelas subdivididas, com os fatores: Adubação (tratamento principal-\\(I\\) ) e Variedades (tratamento secundário-\\(K\\) ), sendo utilizados \\(2\\) níveis de Adubação (\\(A_0\\; e\\; A_1\\)) e \\(3\\) Variedades (\\(V_1\\;V_2\\;e\\; V_3\\)), o esquema de casualização dos tratamento, se o experimento fosse montado de acordo com o DBC, com \\(5\\) blocos (J), seria o seguinte. O esquema de análise de variância deste ensaio seria o seguinte: Causas de Variação GL Blocos 4 (J-1) Adubação (A) 1 (I-1) Resíduo a 4 (J-1)(I-1) (Parcelas) ( 9 ) (IJ-1) Variedades (V) 2 (K-1) Interação (A x V) 2 (I-1)(K-1) Resíduo b 16 I(K-1)(J-1) Total 29 (IJK-1) Observação: Caso este mesmo ensaio fosse montado de acordo com o esquema fatorial \\(2 \\times 3\\) em \\(5\\) blocos, a casualização seria feita de modo diferente e, como exemplo, apresentamos o sorteio do delineamento seguinte: e o esquema da análise de variância do experimento de acordo com o esquema fatorial \\(2 \\times 3\\), seria o seguinte: Causas de Variação GL Adubação (A) 1 Variedades (V) 2 Interação (A x V) 2 (Tratamentos) ( 5 ) Blocos 4 Resíduo 20 Total 29 A análise de variância do experimento em parcelas subdivididas reflete a característica principal do delineamento: A análise dos tratamentos principais é em blocos ao acaso, com os 2 níveis de adubação repetidos em 5 blocos (parte superior do quadro de análise de variância), sendo a análise dos tratamentos secundários, a análise de \\(3\\) variedades distribuídos ao acaso nas \\(3\\) subparcelas de cada uma das \\(10\\) parcelas. "],["obtenção-da-análise-de-variância-8.html", "22 Obtenção da análise de variância 22.1 1. Desdobramento da interação \\(E \\times V\\) para estudar os efeitos de Variedades dentro de cada Época de plantio (V d. E): 22.2 2. Desdobramento da interação \\(E \\times V\\) para estudar os efeitos de épocas dentro de cada Variedade (E d. V): 22.3 Cálculo das médias 22.4 Cálculo dos erros padrões: 22.5 Teste de Tukey para comparar médias de tratamentos principais (épocas) dentro de tratamentos secundérios (variedades) 22.6 Cálculo dos coeficientes de variação do experimento", " 22 Obtenção da análise de variância Para a obtenção da análise de variância de um experimento em parcelas subdivididas, vamos utilizar os dados obtidos do trabalho intitulado Efeito de épocas de plantio, sobre várias características agronômicas na cultura da soja (Glycine max. (L.) Merril), variedades Santa Rosa e Viçoja, e Jaboticabal, SP, realizado por K. YUYAMA (1976). Foram utilizadas \\(8\\) épocas de plantio (20/10/74, 30/10/74, 10/11/74, 20/11/74, 30/11/74, 10/12/47, 20/12/74 e 30/12/74) e duas variedade de soja (\\(V_1\\) = Viçoja e \\(V_2\\) = Santa Rosa). O ensaio foi montado de acordo com o delineamento em parcelas subdivididas, com as épocas de plantio nas parcelas, e as variedades nas subparcelas. Os resultados obtidos para produção de grãos (\\(t\\;ha^{-1}\\)), foram os seguintes: Tratamentos Bloco 1 Bloco 2 Bloco 3 Total \\(E_1V_1\\) 2,9166 2,8833 2,4750 8,2749 \\(E_1V_2\\) 2,6416 3,6666 3,6166 9,9248 \\(E_2V_1\\) 3,4889 3,5833 3,3333 10,4055 \\(E_2V_2\\) 4,0583 4,3000 2,9083 11,2666 \\(E_3V_1\\) 2,3166 2,8666 2,4916 7,6748 \\(E_3V_2\\) 3,4500 3,7666 3,5333 10,7499 \\(E_4V_1\\) 2,7916 2,7583 3,1916 8,7415 \\(E_4V_2\\) 3,4166 2,7416 3,5083 9,6665 \\(E_5V_1\\) 3,5583 3,1583 2,7916 9,5082 \\(E_5V_2\\) 3,5000 3,1166 3,0916 9,7082 \\(E_6V_1\\) 2,7833 2,5166 2,1250 7,4249 \\(E_6V_2\\) 2,5583 2,5666 2,0416 7,1665 \\(E_7V_1\\) 2,3000 2,2083 2,0666 6,5749 \\(E_7V_2\\) 1,4250 1,9166 1,8750 5,2166 \\(E_8V_1\\) 1,1666 1,6916 1,4666 4,3248 \\(E_8V_2\\) 2,0083 1,7833 1,7416 5,5332 Total 44,3800 45,5242 42,2576 132,1618 Dados originais:DOWNLOAD Pelo quadro de dados do experimento, podemos obter as somas de quadrados Total e de Blocos, utilizando: \\(I\\) (número de níveis de Tratamentos Principais). \\(J\\) (número de Blocos ou repetições). \\(K\\) (número de níveis de Tratamentos Secundários). Assim, temos: \\[ \\begin{align} SQ_{Total} &amp;= \\sum_{i=1}^I \\sum_{j=1}^J \\sum_{k=1}^K x_{ijk}^2 - \\frac{G^2}{IJK} \\\\ &amp;=(2,9166^2+2,8833^2+\\cdots +1,7416^2)-\\frac{132,1618^2}{8\\cdot 3\\cdot 2} \\\\ &amp;= 25,6734 \\end{align} \\] e, \\[ \\begin{align} SQ_{Blocos} &amp;= \\frac{1}{IK} \\sum_{j = 1}^JB_j^2-C \\\\ &amp;=\\frac{1}{8\\cdot2} (44,3800^2+45,5242^2+42,2576^2) -\\frac{132,1618^2}{48} \\\\ &amp;=0,3434 \\end{align} \\] Para a obtenção das demais somas de quadrados, é conveniente montar os seguintes quadros auxiliares: Quadro auxiliar para obtenção dos totais de parcelas. (2) Bloco 1 Bloco 2 Bloco 3 Total \\(E_1\\) 5,5582 6,5499 6,0916 18,1997 \\(E_2\\) 7,5472 7,8833 6,2416 21,6721 \\(E_3\\) 5,7666 6,6332 6,0249 18,4247 \\(E_4\\) 6,2082 5,4999 6,6999 18,408 \\(E_5\\) 7,0583 6,2749 5,8832 19,2164 \\(E_6\\) 5,3416 5,0832 4,1666 14,5914 \\(E_7\\) 3,7250 4,1249 3,9416 11,7915 \\(E_8\\) 3,1749 3,4749 3,2082 9,8580 Total 44,38 45,5242 42,2576 132,1618 Pelo quado de totais de parcelas, obtemos: \\[ \\begin{align} SQ_{Parcelas} &amp;= \\frac{1}{K}[T_{E_1B_1}^2+T_{E_1B_2}^2+\\cdots+T_{E_iB_j}^2]-C \\\\ &amp;=\\frac{1}{2}[5,5582^2+6,5499^2+\\cdots+3,2082^2]-363,8904 \\\\ &amp;= 21,4149 \\\\ \\\\ SQ_{Épocas} &amp;= \\frac{1}{JK}(T_{E_1}^2+T_{E_2}^2+\\cdots +T_{E_I}^2) - C\\\\ &amp;= \\frac{1}{3\\cdot2}(18,1997^2+21,6721^2+\\cdots +9,8580^2) - 363,8904\\\\ &amp;=19,0482 \\\\ \\\\ SQ_{Res(a)}&amp; = SQ_{Parcelas} - SQ_{Épocas}- SQ_{Blocos} \\\\ &amp; = 21,4149-19,0482-0,3434 \\\\ &amp; = 2,0233 \\end{align} \\] Quadro auxiliar para obtenção dos totais de tratamentos principais x tratamento secundários. (3) \\(V_1\\) \\(V_2\\) Total \\(E_1\\) 8.2749 9.9248 18.1997 \\(E_2\\) 10.4055 11.2666 21.6721 \\(E_3\\) 7.6748 10.7499 18.4247 \\(E_4\\) 8.7415 9.6665 18.408 \\(E_5\\) 9.5082 9.7082 19.2164 \\(E_6\\) 7.4249 7.1665 14.5914 \\(E_7\\) 6.5749 5.2166 11.7915 \\(E_8\\) 4.3248 5.5332 9.858 Total 62.9295 69.2323 132.1618 Por este quadro, calcularemos a Soma de quadrados devido ao tratamento secundário e devido à interação Primário x Secundário. \\[ \\begin{align} SQ_{Variedades} &amp;= \\frac{1}{IJ}[T_{V_1}^2+T_{V_2}^2+\\cdots+T_{V_K}^2]-C \\\\ &amp;=\\frac{1}{8\\cdot3}[62,9295^2+69,2323^2]-363,8904 \\\\ &amp;= 0,8276 \\\\ \\\\ SQ_{E,V} &amp;= \\frac{1}{J}[T_{E_1V_1}^2+T_{E_1V_2}^2+\\cdots+T_{E_IV_K}^2]-C \\\\ &amp;=\\frac{1}{3}[8,2749^2+9,9248^2+\\cdots+5,5332^2]-363,8904 \\\\ &amp;= 21,9127 \\\\ \\\\ SQ_{Interação\\;E \\times V} &amp;= SQ_{E,V}-SQ_{E} -SQ_{V}\\\\ &amp;=21,9127-19,0482-0,8276\\\\ &amp;=2,0370 \\end{align} \\] Obtidas as somas de quadrados, podemos montar o seguinte quadro de análise de variância: Causas de Variação GL SQ QM F Blocos 2 0,3434 0,1717 1,19 Épocas de Plantio (E) 7 19,0482 2,7212 18,83** Resíduo(a) 14 2,0233 0,1445  (Parcelas) ( 23 ) ( 21,4149 )   Variedades (V) 1 0,8276 0,8276 9,50** Interação (E x V) 7 2,0370 0,2910 3,34* Resíduo(b) 16 1,3939 0,0871  Total 47 25,6734   Valores de F da tabela para Épocas (\\(7\\times14GL\\)): \\(\\begin{cases} 5\\%=2,76 \\\\ 1\\%=4,28 \\end{cases}\\) Valores de F da tabela para Variedades (\\(1\\times16GL\\)): \\(\\begin{cases} 5\\%=4,49 \\\\ 1\\%=8,53 \\end{cases}\\) Valores de F da tabela para Interação E \\(\\times\\) V (\\(7\\times16GL\\)): \\(\\begin{cases} 5\\%=2,66 \\\\ 1\\%=4,03 \\end{cases}\\) 22.0.1 Conclusões Para efeito de Épocas: O teste foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar a hipótese da nulidade e concluir que as épocas de plantio diferem entre si em relação à produção da cultura da soja. Para efeito de Variedades: O teste foi significativo ao nível de \\(1\\%\\) de probabilidade, indicando que devemos rejeitar a hipótese da nulidade e concluir que as variedades testadas diferem entre si em relação à produção da cultura da soja. Para efeito da Interação E \\(\\times\\) V: O teste foi significativo ao nível de \\(5\\%\\) de probabilidade, indicando que devemos rejeitar a hipótese \\(H_0\\) e concluir que os fatores Épocas de plantio e variedades agem conjuntamente sobre a produção da cultura da soja, ou seja, Épocas de plantio e variedades não agem de maneira independente. Como a interação E\\(\\times\\)V foi significativa, devemos desdobrar os graus de liberdade da interação para estudar os efeitos de um fator em cada um dos níveis do outro fator. 22.1 1. Desdobramento da interação \\(E \\times V\\) para estudar os efeitos de Variedades dentro de cada Época de plantio (V d. E): Para obtenção das somas de quadrados, utilizamos o quadro auxiliar: (3) \\(V_1\\) \\(V_2\\) Total \\(E_1\\) 8.2749 9.9248 18.1997 \\(E_2\\) 10.4055 11.2666 21.6721 \\(E_3\\) 7.6748 10.7499 18.4247 \\(E_4\\) 8.7415 9.6665 18.408 \\(E_5\\) 9.5082 9.7082 19.2164 \\(E_6\\) 7.4249 7.1665 14.5914 \\(E_7\\) 6.5749 5.2166 11.7915 \\(E_8\\) 4.3248 5.5332 9.858 Total 62.9295 69.2323 132.1618 Então, temos: \\[ \\begin{align} SQ_{V\\;d.\\;E_1} &amp;= \\frac{1}{3}[8,2749^2+9,9248^2]-\\frac{18,1997^2}{2\\cdot3}=0,4537 \\\\ SQ_{V\\;d.\\;E_2} &amp;= \\frac{1}{3}[10,4055^2+11,26668^2]-\\frac{21,6721^2}{2\\cdot3}=0,1236 \\\\ SQ_{V\\;d.\\;E_3} &amp;= \\frac{1}{3}[7,6748^2+10,7499^2]-\\frac{18,4247^2}{2\\cdot3}=1,5760 \\\\ SQ_{V\\;d.\\;E_4} &amp;= \\frac{1}{3}[8,7415^2+9,6665^2]-\\frac{18,4080^2}{2\\cdot3}=0,1426 \\\\ SQ_{V\\;d.\\;E_5} &amp;= \\frac{1}{3}[9,5082^2+9,7082^2]-\\frac{19,2164^2}{2\\cdot3}=0,0067 \\\\ SQ_{V\\;d.\\;E_6} &amp;= \\frac{1}{3}[7,4249^2+7,1665^2]-\\frac{14,5914^2}{2\\cdot3}=0,0111 \\\\ SQ_{V\\;d.\\;E_7} &amp;= \\frac{1}{3}[6,5749^2+5,2166^2]-\\frac{11,7915^2}{2\\cdot3}=0,3075 \\\\ SQ_{V\\;d.\\;E_8} &amp;= \\frac{1}{3}[4,3248^2+5,5332^2]-\\frac{9,8580^2}{2\\cdot3}=0,2434 \\end{align} \\] Verificação: \\[ SQ_{Variedade}+SQ_{Interalção\\;E \\times V} = SQ_{V\\;d.E_1}+SQ_{V\\;d.E_2}+SQ_{V\\;d.E_3}+SQ_{V\\;d.E_4}+SQ_{V\\;d.E_5}+SQ_{V\\;d.E_6}+SQ_{V\\;d.E_7}+SQ_{V\\;d.E_8} \\] Causas de Variação GL SQ QM F Variedades d. E1 1 0,4537 0,4537 5,21* Variedades d. E2 1 0,1236 0,1236 1,42 Variedades d. E3 1 1,5760 1,5760 18,09** Variedades d. E4 1 0,1426 0,1426 1,64 Variedades d. E5 1 0,0067 0,0067 0,08 Variedades d. E6 1 0,0111 0,0111 0,13 Variedades d. E7 1 0,3075 0,3075 3,53 Variedades d. E8 1 0,2434 0,2434 2,79 Resíduo(b) 16 1,3939 0,0871  Valores de \\(F\\) da Variedades d. E (\\(1\\times16GL\\)): \\(\\begin{cases} 5\\%=4,49 \\\\ 1\\%=8,53 \\end{cases}\\) Conclusão: Os valores de F foram significativos para V d. E1 e V d. E3, indicando que existe diferença entre as variedades apenas quando plantadas nestas duas épocas em relação à produção da cultura da soja. 22.2 2. Desdobramento da interação \\(E \\times V\\) para estudar os efeitos de épocas dentro de cada Variedade (E d. V): \\[ \\begin{align} SQ_{E\\;d.\\;V_1} &amp;= \\frac{1}{3}[8,2749^2+10,4055^2+\\cdots+4,3248^2]-\\frac{62,9295^2}{8\\cdot3}=8,1726 \\\\ SQ_{E\\;d.\\;V_2} &amp;= \\frac{1}{3}[9,9248^2+11,2666^2+\\cdots+5,5332^2]-\\frac{69,2323^2}{8\\cdot3}=12,9126 \\end{align} \\] Como neste caso, estamos comparando tratamento principais dentro de tratamento secundários, estão envolvidos na comparação o Resíduo (a) e o Resíduo (b). Então, para aplicarmos o teste F, devemos obter um residuo médio (variância complexa), dado por: \\[ QM_{Res.Médio}=\\frac{QM_{Res(a)}+(b-1)QM_{Res(b)}}{b}, \\] onde \\(b\\) é o número de tratamentos secundários. Neste caso, o número de graus de liberdade (\\(n&#39;\\)), associado a este Resíduo Médio, pode ser obtido pla fórmula de SATTERTHWAITE: \\[ n&#39;=\\frac{[QM_{Res(a)}+(b-1)QM_{Res(b)}]^2} {\\frac{[QM_{Res(a)}]^2}{GL_{Res(a)}}+\\frac{[(b-1)QM_{Res(b)}]^2}{GL_{Res(b)}}} \\] Então, no nosso exemplo, temos: \\[ QM_{Res.Médio}=\\frac{0,1445+(2-1)\\cdot0,0871}{2}=0,1158 \\] e o número de gruas de liberdade associado a este resíduo será: \\[ n&#39;=\\frac{[0,1445+(2-1) \\cdot 0,0871]^2} {\\frac{[0,1455]^2}{14}+\\frac{[(2-1) \\cdot 0,0871]^2}{16}}=27.29 \\] Então, podemos montar o seguinte quadro de análise de variância. Causas de Variação GL SQ QM F Épocas d. V1 7 8,1726 1,1675 10,08** Épocas d. V2 7 12,9126 1,8447 15,93** Resíduo Médio 27  0,1158  Valores de F da Épocas d. V (\\(7\\times27GL\\)): \\(\\begin{cases} 5\\%=2,73 \\\\ 1\\%=3,39 \\end{cases}\\) Conclusão: Os valores de F foram significativos para Épocas d. V1 e Épocas d. V2, indicando que existe diferença entre as épocas de plantio para ambas as variedades em relação à produção da cultura da soja. Portanto, para verificar qual é a melhor época de plantio para da variedade de soja, devemos aplicar testes de comparações de médias. 22.3 Cálculo das médias A partir do quadro de totais, vamos construir a tabela de médias Quadro de Totais (3) \\(V_1\\) \\(V_2\\) Total \\(E_1\\) \\(8,2749\\div3\\) \\(9,9248\\div3\\) \\(18,1997\\div6\\) \\(E_2\\) \\(10,4055\\div3\\) \\(11,2666\\div3\\) \\(21,6721\\div6\\) \\(E_3\\) \\(7,6748\\div3\\) \\(10,7499\\div3\\) \\(18,4247\\div6\\) \\(E_4\\) \\(8,7415\\div3\\) \\(9,6665\\div3\\) \\(18,408\\div6\\) \\(E_5\\) \\(9,5082\\div3\\) \\(9,7082\\div3\\) \\(19,2164\\div6\\) \\(E_6\\) \\(7,4249\\div3\\) \\(7,1665\\div3\\) \\(14,5914\\div6\\) \\(E_7\\) \\(6,5749\\div3\\) \\(5,2166\\div3\\) \\(11,7915\\div6\\) \\(E_8\\) \\(4,3248\\div3\\) \\(5,5332\\div3\\) \\(9,858\\div6\\) Total \\(62,9295\\div24\\) \\(69,2323\\div24\\) \\(132,1618\\div48\\) Quadro de médias (3) \\(V_1\\) \\(V_2\\) Médias (E) \\(E_1\\) 2,7583 3,3083 3,0333 \\(E_2\\) 3,4685 3,7555 3,6120 \\(E_3\\) 2,5583 3,5833 3,0708 \\(E_4\\) 2,9138 3,2222 3,0680 \\(E_5\\) 3,1694 3,2361 3,2027 \\(E_6\\) 2,4750 2,3888 2,4319 \\(E_7\\) 2,1916 1,7389 1,9653 \\(E_8\\) 1,4416 1,8444 1,9653 Médias (V) 2,6221 2,8847 2,7534 22.4 Cálculo dos erros padrões: 22.4.1 a) Tratamentos principais (épocas) \\[ s(\\hat{m}_E)=\\frac{s_a}{\\sqrt{r_E}}=\\frac{\\sqrt{QM_{Res(a)}}}{\\sqrt{r_E}}=\\sqrt{\\frac{0.1445}{6}}= 0,1552\\;t\\;ha^{-1} \\] 22.4.2 b) Tratamentos secundários (Variedades) \\[ s(\\hat{m}_V)=\\frac{s_b}{\\sqrt{r_V}}=\\frac{\\sqrt{QM_{Res(b)}}}{\\sqrt{r_V}}=\\sqrt{\\frac{0,087}{24}}= 0,0602\\;t\\;ha^{-1} \\] 22.4.3 c) Tratamentos secundários (Variedades) dentro de tratamentos principais (Épocas) \\[ s(\\hat{m}_{V\\;d.E})=\\frac{s_b}{\\sqrt{r_{V\\;d.E}}}=\\frac{\\sqrt{QM_{Res(b)}}}{\\sqrt{r_{V\\;d.E}}}=\\sqrt{\\frac{0,0871}{3}}= 0,1704\\;t\\;ha^{-1} \\] 22.4.4 d) Tratamentos principais (Épocas) dentro de tratamentos secundários (Variedades) Note, apesar de serem as mesmas médias, do item anterior, a comparação é diferente, então no erro padrão delas, devemos utilizar o \\(QM_{Resíduo\\;Médio}\\): \\[ s(\\hat{m}_{E\\;d.V})=\\frac{s_{médio}}{\\sqrt{r_{E\\;d.V}}}=\\frac{\\sqrt{QM_{Res.Médio}}}{\\sqrt{r_{E\\;d.V}}}=\\sqrt{\\frac{0,1158}{3}}= 0,1965\\;t\\;ha^{-1} \\] 22.5 Teste de Tukey para comparar médias de tratamentos principais (épocas) dentro de tratamentos secundérios (variedades) O valor da diferença mínima significativa pelo teste de Tukey (\\(5\\%\\)) será: \\[ dms=q\\cdot\\sqrt{\\frac{QM_{Res.Médio}}{r_{E\\;d.V}}},\\\\ \\text{considerando:}\\;q(8\\;Épocas \\times 27\\;GL\\;Res.Médio,5\\%)=4,64 \\\\ r=3\\\\ \\text{Então, temos:}\\\\ dms=4,64\\cdot0,1965=0,9116\\;t\\;ha^{-1} \\] Resumo do teste de Tukey: \\(V_1\\) \\(V_2\\) \\(E_1\\) 2,7583 abc 3,3083 a \\(E_2\\) 3,4685 a 3,7555 a \\(E_3\\) 2,5583 abc 3,5833 a \\(E_4\\) 2,9138 abc 3,2222 ab \\(E_5\\) 3,1694 ab 3,2361 ab \\(E_6\\) 2,4750 bc 2,3888 bc \\(E_7\\) 2,1916 cd 1,7389 c \\(E_8\\) 1,4416 d 1,8444 c 22.6 Cálculo dos coeficientes de variação do experimento Como temos 2 resíduos, teremos também, dois coeficientes de variação: Coeficiente de variação para parcelas: \\[ CV_a = 100\\frac{s_a}{\\hat{m}}=100\\frac{\\sqrt{0,1445}}{2,7534} = 13,81\\% \\] Coeficiente de variação para subparcelas: \\[ CV_b = 100\\frac{s_b}{\\hat{m}}=100\\frac{\\sqrt{0,0871}}{2,7534} = 10,72\\% \\] require(ExpDes.pt) caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/sojapsub.txt&quot; d&lt;-read.table(caminho,h=T) psub2.dbc(d$E,d$V,d$Bloco,d$Y,fac.names = c(&quot;Épocas&quot;,&quot;Variedades&quot;)) ## ------------------------------------------------------------------------ ## Legenda: ## FATOR 1 (parcela): Épocas ## FATOR 2 (subparcela): Variedades ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## $`Quadro da analise de variancia\\n------------------------------------------------------------------------\\n` ## GL SQ QM Fc Pr(&gt;Fc) ## Épocas 7 19.0482 2.72117 18.8291 4e-06 *** ## Bloco 2 0.3434 0.17171 1.1882 0.333718 ## Erro a 14 2.0233 0.14452 ## Variedades 1 0.8276 0.82761 9.4997 0.007142 ** ## Épocas*Variedades 7 2.0370 0.29100 3.3402 0.021671 * ## Erro b 16 1.3939 0.08712 ## Total 47 25.6734 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ------------------------------------------------------------------------ ## CV 1 = 13.80697 % ## CV 2 = 10.71998 % ## ## ## ## Interacao significativa: desdobrando a interacao ## ------------------------------------------------------------------------ ## ## Desdobrando Épocas dentro de cada nivel de Variedades ## ------------------------------------------------------------------------ ## GL SQ QM Fc valor.p ## Épocas : Variedades 1 7.00000 8.172581 1.167512 10.080441 4e-06 ## Épocas : Variedades 2 7.00000 12.912557 1.844651 15.926946 0 ## Erro combinado 27.28941 3.160659 0.115820 ## ------------------------------------------------------------------------ ## ## ## Épocas dentro de Variedades 1 ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 2 3.4685 ## ab 5 3.1694 ## abc 4 2.913833 ## abc 1 2.7583 ## abc 3 2.558267 ## bc 6 2.474967 ## cd 7 2.191633 ## d 8 1.4416 ## ------------------------------------------------------------------------ ## ## Épocas dentro de Variedades 2 ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 2 3.755533 ## a 3 3.5833 ## a 1 3.308267 ## ab 5 3.236067 ## ab 4 3.222167 ## bc 6 2.388833 ## c 8 1.8444 ## c 7 1.738867 ## ------------------------------------------------------------------------ ## ## ## Desdobrando Variedades dentro de cada nivel de Épocas ## ------------------------------------------------------------------------ ## GL SQ QM Fc valor.p ## Variedades : Épocas 1 1 0.453695 0.453695 5.207702 0.036511 ## Variedades : Épocas 2 1 0.123582 0.123582 1.418528 0.251016 ## Variedades : Épocas 3 1 1.576040 1.576040 18.09045 0.000607 ## Variedades : Épocas 4 1 0.142604 0.142604 1.636871 0.218998 ## Variedades : Épocas 5 1 0.006667 0.006667 0.076523 0.785608 ## Variedades : Épocas 6 1 0.011128 0.011128 0.127737 0.725461 ## Variedades : Épocas 7 1 0.307496 0.307496 3.529574 0.078628 ## Variedades : Épocas 8 1 0.243372 0.243372 2.793523 0.114086 ## Erro b 16 1.393919 0.087120 ## ------------------------------------------------------------------------ ## ## ## Variedades dentro de Épocas 1 ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 2 3.308267 ## b 1 2.7583 ## ------------------------------------------------------------------------ ## ------------------------------------------------------------------------ ## ## ## Variedades dentro de Épocas 2 ## ------------------------------------------------------------------------ ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 3.468500 ## 2 2 3.755533 ## ------------------------------------------------------------------------ ## ## Variedades dentro de Épocas 3 ## ------------------------------------------------------------------------ ## Teste de Tukey ## ------------------------------------------------------------------------ ## Grupos Tratamentos Medias ## a 2 3.5833 ## b 1 2.558267 ## ------------------------------------------------------------------------ ## ------------------------------------------------------------------------ ## ## ## Variedades dentro de Épocas 4 ## ------------------------------------------------------------------------ ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 2.913833 ## 2 2 3.222167 ## ------------------------------------------------------------------------ ## ## Variedades dentro de Épocas 5 ## ------------------------------------------------------------------------ ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 3.169400 ## 2 2 3.236067 ## ------------------------------------------------------------------------ ## ## Variedades dentro de Épocas 6 ## ------------------------------------------------------------------------ ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 2.474967 ## 2 2 2.388833 ## ------------------------------------------------------------------------ ## ## Variedades dentro de Épocas 7 ## ------------------------------------------------------------------------ ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 2.191633 ## 2 2 1.738867 ## ------------------------------------------------------------------------ ## ## Variedades dentro de Épocas 8 ## ------------------------------------------------------------------------ ## De acordo com o teste F, as medias desse fator sao estatisticamente iguais. ## ------------------------------------------------------------------------ ## Niveis Medias ## 1 1 1.4416 ## 2 2 1.8444 ## ------------------------------------------------------------------------ "],["análise-de-regressão-por-polinômios-ortogonais.html", "23 Análise de Regressão por Polinômios Ortogonais 23.1 Obtenção da análise de variância, estudando-se os efeitos da regressão 23.2 Coeficientes de interpolação de polinômios ortogonais 23.3 Obtendo a Equação de Regressão 23.4 Cálculo do Coeficiente de Determinação (R²) 23.5 Gráfico da Regressão Ajustada", " 23 Análise de Regressão por Polinômios Ortogonais Nos experimentos em que os tratamentos são quantitativos, como por exemplo, níveis crescentes de um adubo, doses crescentes de um inseticida, etc, muitas vezes existe uma correspondência funcional, denominada equação de regressão, que relaciona os valores dos tratamentos (X) com os dados analisados (Y). Por exemplo, essa dependência pode ser notada no caso seguinte,onde X representa oas doses de um adubo (\\(kg/ha\\)) e y a produção de milho (\\(kg/ha\\)). X 0 25 50 75 100 Y 2100 2600 3000 3550 4150 Vamos criar um gráfico para essa visualização no R. X&lt;-seq(0,100,25) Y&lt;-c(2100,2600,3000,3550,4150) plot(X,Y,pch=21,bg=&quot;gray&quot;,cex=1.5,las=1, xlab= &quot;Dose (kg/ha)&quot;, ylab= &quot;Produção do Milho (kg/ha)&quot;, cex.lab=1.2 ) Verificamos, portanto, que há uma tendência de aumento na produção à medida que aumentamos a quantidade de adubo aplicada. Vejamos então, como fazer a análise de variância para o estudo da regressão. O método utilizado é denominado de método dos polinômios ortogonais, e é de fácil aplicação quanto os níveis de X são equidistantes, pois permitem a utilização de coeficientes obtidos em tabelas. 23.1 Obtenção da análise de variância, estudando-se os efeitos da regressão Para estudo da regressão, vamos utilizar os dados do trabalho: Efeito de doses de gesso na cultura do feijoeiro (Phaseolu vulgaris L.), realizado por RAGAZZI (1979). Neste trabalho foram utilizadas 7 doses de gesso 0, 50, 100, 150, 200, 250, e 300 \\(kg/ha\\). Os resultados obtidos para peso de 1000 sementes, em gramas, são apresentados a seguir: Tratamentos Rep. 1 Rep. 2 Rep. 3 Rep. 4 Total 0 134,8 139,7 147,6 132,3 554,4 50 161,7 157,7 150,3 144,7 614,4 100 160,7 172,7 163,4 161,3 658,1 150 169,8 168,2 160,7 161,0 659,7 200 165,7 160,0 158,2 151,0 634,9 250 171,8 157,3 150,4 160,4 639,9 300 154,5 160,4 148,8 154,0 617,7 Total 4379,1 Dados originais:DOWNLOAD A análise de variância preliminar será realizada de acordo com o delineamento experimental utilizado. O Ensaio foi montado de acordo com o delineamento inteiramente casualizado e, portanto, a análise de variância preliminar, obtido de maneira usual, foi a seguinte: \\[ SQ_{Total} = (134,8^2+139,7^2+\\cdots+154,0^2)-\\frac{4379,1^2}{7\\cdot4}=2828,17 \\\\ \\\\ SQ_{Trat}=\\frac{1}{4}[554,4^2+614,4^2+\\cdots+617,7^2]-\\frac{4379,1^2}{7\\cdot4} = 1941,83 \\\\ \\\\ SQ_{Res} = SQ_{Total}-SQ_{Trat}=2828,17-1941,83=886,34 \\] Quadro de análise de variância preliminar: Causas de Variação GL SQ QM F Tratamentos 6 1941,83 323,64 7,67** Resíduo 21 886,34 42,21  Total 27 2828,17   Conclusão: O teste F foi significativo ao nível de \\(1\\%\\) de probabilidade, logo, rejeitamos a hipótese da nulidade (\\(H_0\\)), e concluímos que as doses de gesso aplicadas possuem efeitos diferentes sobre o peso de 1000 sementes. No entanto, um caso como este, em que os tratamentos são quantitativos, e em mais de 2 níveis, uma análise mais detalhada deve levar em conta a regressão, desdobrando-se os 6 graus de liberdade de tratamentos em: Regressão Linear..1 GL Regressão Quadrática1 GL Regressão Cúbica..1 GL Regressão de 4º grau1 GL Regressão de 5º grau1 GL Regressão de 6º grau1 GL  (Tratamentos)(6) GL Porém, as regressões de grau maior que 3º não tem interesse prático, de modo que, na análise de variância, podemos considerar as regressões de graus maior que o 3º como uma única causa de variação, que denominamos de Desvios da Regressão. Assim, no nosso exemplo, temos: Causas de Variação GL Regressão Linear 1 Regressão Quadrática 1 Regressão Cúbica 1 Desvios da Regressão 3 (Tratamentos) (6) Resíduo 21 Total 27 Esta decomposição pode ser feita pelo método dos polinômios ortogonais, e é de fácil aplicação quando as quantidades que determinam os tratamentos são igualmente espaçadas (equidistantes), o que ocorre no caso em estudo (0, 50, 100, 150, 20, 250, 300). Neste caso, os coeficientes dos polinômios ortogonais são obtidos em tabelas, como a seguintes: 23.2 Coeficientes de interpolação de polinômios ortogonais \\[ \\begin{align} &amp; P_1 = x\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;P_2=x^2-\\frac{n^2-1}{12}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;P_3=x^3-\\frac{3n^2-7}{20}x \\\\ \\\\ &amp; onde, \\\\ \\\\ &amp; x=\\frac{X-\\bar{X}}{q} \\end{align} \\] Para facilitar os cálculos, montamos o quadro seguinte, onde aparecem os totais de tratamentos (\\(T_i\\)) e os coeficientes dos polinômios ortogonais (\\(C_{ji}\\)) para cada componente: No nosso exemplo, utilizaremos os coeficientes para \\(n=7\\) níveis: Com estes coeficientes (\\(C_{ji}\\)) e Totais (\\(T_i\\)), estabelecemos contrastes ortogonais, sendo um contraste para o efeito da Regressão Linear (1º grau), outro para a Regressão Quadrática (2º grau) e outro para a Regressão Cúbica (3º grau). As tabelas de coeficientes dos polinômios ortogonais nos fornecem ainda, a soma dos quadrados dos coeficientes (K), e uma constante (M) que deverá ser utilizada na determinação da equação de regressão. Então, o contraste correspondente à regressão de grau \\(j\\) será: \\[ \\hat{Y}_{Grau\\;j} = \\sum_{i=1}^IC_{ji}T_{i} \\] E a soma de quadrados correspondente, será: \\[ SQ_{Reg.Grau\\;j} = \\frac{\\hat{Y}_{Grau\\;j}^2}{r\\cdot K_j} \\] onde, \\(\\hat{Y}_{Grau\\;j}\\) = Estimativa do contraste para a regressão de grau \\(j\\). \\(r=\\)número de repetições com que foram calculados os totais \\(T_i\\). \\(K_j=\\sum_{i=1}^IC_{ji}^2\\) = soma dos quadrado dos coeficientes do contraste para a regressão de grau \\(j\\). Portanto, no nosso exemplo, temos: Regressão Linear ou de Grau 1: \\[ \\begin{align} &amp; Y_{\\text{Reg. Linear}} = \\sum_{i=1}^IC_{1i}T_i=-3T_1-2T_2-1T_3+0T_4+1T_5+2T_6+3T_7 \\\\ &amp; \\hat{Y}_{\\text{Reg.Linear}} = -3(554,4)-2(614,4)-1(658,1)+0(659,7)+1(634,9)+2(639,9)+3(617,7) \\\\ &amp; \\hat{Y}_{\\text{Reg.Linear}} = 217,7\\;g \\\\ &amp; SQ_{Reg.Linear}=\\frac{\\hat{Y}_{\\text{Reg.Linear}}^2}{r\\cdot K_1}=\\frac{217,7^2}{4\\cdot28}=423,15 \\end{align} \\] Regressão Quadrádica ou de Grau 2: \\[ \\begin{align} &amp; Y_{\\text{Reg.Quadrática}} = \\sum_{i=1}^IC_{2i}T_i=5T_1+0T_2-3T_3-4T_4-3T_5+0T_6+5T_7 \\\\ &amp; \\hat{Y}_{\\text{Reg.Quadrática}} = 5(554,4)+0(614,4)-3(658,1)-4(659,7)-3(634,9)+0(639,9)+5(617,7) \\\\ &amp; \\hat{Y}_{\\text{Reg.Quadrática}} = -657,3\\;g \\\\ &amp; SQ_{Reg.Quadrática}=\\frac{\\hat{Y}_{\\text{Grau.2}}^2}{r\\cdot K_2}=\\frac{(-657,3)^2}{4\\cdot84}=1285,84 \\end{align} \\] Regressão Cúbica ou de Grau 3: \\[ \\begin{align} &amp; Y_{\\text{Reg.Cúbica}} = \\sum_{i=1}^IC_{3i}T_i=-1T_1+1T_2+1T_3+0T_4-1T_5-1T_6+1T_7 \\\\ &amp; \\hat{Y}_{\\text{Reg.Cúbica}} = -1(554,4)+1(614,4)+1(658,1)+0(659,7)-1(634,9)-1(639,9)+1(617,7) \\\\ &amp; \\hat{Y}_{\\text{Reg.Cúbica}} = 61,0\\;g \\\\ &amp; SQ_{Reg.Cúbica}=\\frac{\\hat{Y}_{\\text{Grau.3}}^2}{r\\cdot K_3}=\\frac{155,04^2}{4\\cdot6}=155,04 \\end{align} \\] d) Desvios da Regressão: \\[ SQ_{Desvios\\;da\\;Regressão}= SQ_{Trat.}-SQ_{Reg.Linear}-SQ_{Reg.Quadrática}-SQ_{Reg.Cúbica} \\\\ SQ_{Desvios\\;da\\;Regressão}= 1941,83-423,15-1285,84-155,04 \\\\ SQ_{Desvios\\;da\\;Regressão}= 77,80 \\] Com estas somas de quadrado, podemos montar Causas de Variação GL SQ QM F Regressão Linear 1 423,15 423,15 10,02** Regressão Quadrática 1 1285,84 1285,84 30,46** Regressão Cúbica 1 155,04 155,04 3,67 Desvios da Regressão 3 77,80 25,93 0,61 (Tratamentos) 6 (1941,83)   Resíduo 21 886,34 42,21  Total 27 2828,17   Valores de F da tabela (\\(1\\times21GL\\)): \\(\\begin{cases} 5\\%=4,32 \\\\ 1\\%=8,02 \\end{cases}\\) Valores de F da tabela (\\(3\\times21GL\\)): \\(\\begin{cases} 5\\%=3,07 \\\\ 1\\%=4,87 \\end{cases}\\) CONCLUSÃO: Os testes F para a regressão linear e regressão quadrática foram significativos ao nível de \\(1\\%\\) de probabilidade, indicando que é possível estabelecer uma relação funcional entre a dose de gesso (\\(X\\)) e o peso de 1000 sementes do feijoeiro (\\(Y\\)). A equação de regressão que melhor se ajusta aos dados é a correspondente à REGRESSÃO DE MAIS ALTO GRAU cujo teste F foi significativo. Assim, sendo, no nosso exemplo, a equação que melhor se ajusta é a de 2º grau (Quadrática). Quando o teste F para Desvios da Regressão for significativo, isto indica que existe alguma regressão significativa de grau maior que o 3º e, se tivermos interesse em estudá-la, devemos desdobrar os graus de liberdade de Desvio da Regressão. 23.3 Obtendo a Equação de Regressão A equação de regressão, até um certo grau \\(p\\), tem a seguinte expressão: \\[ \\hat{Y}=\\bar{Y}+\\hat{b}_1M_1P_1+\\hat{b}_2M_2P_2+\\cdots+\\hat{b}_pM_pP_p \\] onde: \\[ \\bar{Y}=\\frac{\\sum_{i=1}^IT_i}{I\\cdot r}=\\frac{G}{I \\cdot r} \\\\ \\\\ \\hat{b}_1=\\frac{\\sum_{i=1}^IC_{1i}T_i}{r \\cdot K_1} = \\frac{\\hat{Y}_{Reg.Linear}}{r \\cdot K_1} \\\\ \\\\ \\hat{b}_2=\\frac{\\sum_{i=1}^IC_{2i}T_i}{r \\cdot K_2} = \\frac{\\hat{Y}_{Reg.Quadrática}}{r \\cdot K_2} \\\\ \\\\ \\cdots \\\\ \\hat{b}_p=\\frac{\\sum_{i=1}^IC_{pi}T_i}{r \\cdot K_p} = \\frac{\\hat{Y}_{Reg.Grau\\;p}}{r \\cdot K_p} \\] e os \\(P_j\\) são os polinômios ortogonais expressos literalmente em relação à \\(x\\) (obtidos em tabelas): \\[ P_1 = x\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;P_2=x^2-\\frac{n^2-1}{12}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;P_3=x^3-\\frac{3n^2-7}{20}x \\\\ \\] No nosso exemplo o teste F indicou que a equação que melhor se ajusta aos dados é a do 2º grau, e portanto, temos: \\[ \\bar{Y}=\\frac{\\sum_{i=1}^IT_i}{I\\cdot r}=\\frac{4379.1}{7 \\cdot 4}=156,3964 \\\\ \\hat{b}_1= \\frac{\\hat{Y}_{Reg.Linear}}{r \\cdot K_1}=\\frac{217,7}{4\\cdot28}=1,9438 \\\\ M_1 = 1 \\\\ P_1=x\\\\ \\hat{b}_2= \\frac{\\hat{Y}_{Reg.Quadrática}}{r \\cdot K_2}=\\frac{-657,3}{4\\cdot84}=-1,9563 \\\\ M_2 = 1 \\\\ P_2=x^2-\\frac{n^2-1}{12} \\text{, onde }n\\text{ = nº de níveis de X} \\\\ \\] No exemplo, temos \\(7\\) níveis de X, e portanto, o valor de \\(P_2\\) será: \\[ P_2=x^2-\\frac{7^2-1}{12}=x^2-4 \\] Então, a equação de regressão será: \\[ \\hat{Y}=\\bar{Y}+\\hat{b}_1M_1P_1+\\hat{b}_2M_2P_2 \\\\ \\hat{Y}=156,3964+1,9438x-1,9563(x^2-4) \\\\ \\hat{Y}= 164,2216+1,9438x-1,9563x^2 \\] Esta equação foi obtida utilizando-se, para facilitar os cálculos, uma variável auxiliar: \\(x=\\frac{X-\\bar{X}}{q}\\), onde \\(\\bar{X}\\) é a média dos valores de \\(X\\) e \\(q\\) é a diferença entre dois níveis consecutivos de X. Assim sendo, devemos expressar a equação de regressão em função de X. No nosso exemplo, temos: \\[ \\bar{X}=\\frac{0+50+100+150+200+250+300}{7}=150 \\\\ q=50 \\] Então, \\(x=\\frac{X-150}{50}\\) Substituindo-se a expressão de \\(x\\) na equação de regressão, teremos \\[ \\hat{Y}= 164,2216+1,9438 \\left( \\frac{X-150}{50} \\right) -1,9563 \\left(\\frac{X-150}{50}\\right)^2 \\\\ \\hat{Y}= 164,2216+1,9438 \\left( \\frac{X-150}{50} \\right) -1,9563 \\left(\\frac{X^2-300X+150^2}{50^2}\\right) \\\\ \\hat{Y}= 164,2216+0,0389X-5,8314-0,000783X^2+0,2348X-17,6067 \\\\ \\hat{Y}= 140,7835+0,2737X-0,000783X^2, \\;para\\;0 \\leq X \\leq 300 \\] onde, \\(\\hat{Y}\\) é o Peso de 1000 sementes (\\(g\\)). \\(X\\) é a dose de gesso aplicada (\\(kg/ha\\)). 23.4 Cálculo do Coeficiente de Determinação (R²) O coeficiente de determinação fornece uma medida do grau de ajuste da equação de regressão estimada. Ele varia entre 0 e 1 e quanto mais próximo de 1 maior o grau de ajuste da equação de regressão estimada. O valor do coeficiente de determinação para uma regressão polinomial de grau \\(p\\), pode ser obitido por: \\[ R^2= \\frac{SQ_{Reg.Grau\\;1}+SQ_{Reg.Grau\\;2}+\\cdots+SQ_{reg.Grau\\;p}}{SQ_{Trat}} \\] então, como nosso exemplo, a equação ajustada foi do segundo grau, portanto, o coeficiente de determinação será: \\[ R^2= \\frac{SQ_{Reg.Grau\\;1}+SQ_{Reg.Grau\\;2}}{SQ_{Trat}}=\\frac{423,15+1285,84}{1941,83} =0,8801 \\] Portanto, \\(88,01\\%\\) da variação do peso de 1000 sementes é explicada pela regressão do peso de 1000 sementes em função da dose de gesso aplicada. 23.5 Gráfico da Regressão Ajustada Podemos fazer uma verificação do ajuste da equação de regressão, calculando os valores esperados de (\\(\\hat{Y}_i\\)) por meio da equação de regressão ajustada, e os valores observados (\\(\\bar{Y}_{i\\;Obs}\\)) por meio das médias dos tratamentos. Devemos verificar que \\(\\sum_{i=1}^I\\bar{Y}_{i~Obs}=\\sum_{i=1}^I\\hat{Y}_{i}\\). Estes valores podem também ser utilizados para a construção do gráfico de regressão ajustada. No nosso exemplo, temos: Dose de Gesso (X) \\(\\bar{Y}_{i~Obs}\\) \\(\\hat{Y}_i\\) 0 138,60 140,78 50 153,60 152,51 100 164,53 160,32 150 164,93 164,22 200 158,73 164,20 250 159,98 160,27 300 154,43 152,42 Total 1094,80 1094,72 Gráfico 1. Relação entre o Peso de \\(1000\\) sementes e as doses de gesso aplicadas. require(ExpDes.pt) caminho&lt;-&quot;https://raw.githubusercontent.com/arpanosso/ExpAgr_2020/master/dados/feijaoREG.txt&quot; d&lt;-read.table(caminho,h=T) # Análise de Variância dic(d$trat,d$y,quali = FALSE) ## ------------------------------------------------------------------------ ## Quadro da analise de variancia ## ------------------------------------------------------------------------ ## GL SQ QM Fc Pr&gt;Fc ## Tratamento 6 1941.83 323.64 7.668 0.00018763 ## Residuo 21 886.34 42.21 ## Total 27 2828.17 ## ------------------------------------------------------------------------ ## CV = 4.15 % ## ## ------------------------------------------------------------------------ ## Teste de normalidade dos residuos ( Shapiro-Wilk ) ## Valor-p: 0.5471519 ## De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais. ## ------------------------------------------------------------------------ ## ## ------------------------------------------------------------------------ ## Teste de homogeneidade de variancia ## valor-p: 0.3337639 ## De acordo com o teste de bartlett a 5% de significancia, as variancias podem ser consideradas homogeneas. ## ------------------------------------------------------------------------ ## ## Ajuste de modelos polinomiais de regressao ## ------------------------------------------------------------------------ ## ## Modelo Linear ## ========================================= ## Estimativa Erro.padrao tc valor.p ## ----------------------------------------- ## b0 150.5652 2.2134 68.0255 0 ## b1 0.0389 0.0123 3.1664 0.0046 ## ----------------------------------------- ## ## R2 do modelo linear ## -------- ## 0.217915 ## -------- ## ## Analise de variancia do modelo linear ## ========================================================= ## GL SQ QM Fc valor.p ## --------------------------------------------------------- ## Efeito linear 1 423.1544 423.1544 10.03 0.00465 ## Desvios de Regressao 5 1,518.6780 303.7356 7.2 0.00046 ## Residuos 21 886.3375 42.2066 ## --------------------------------------------------------- ## ------------------------------------------------------------------------ ## ## Modelo quadratico ## ========================================= ## Estimativa Erro.padrao tc valor.p ## ----------------------------------------- ## b0 140.7839 2.8354 49.6527 0 ## b1 0.2736 0.0443 6.1812 0 ## b2 -0.0008 0.0001 -5.5196 0.00002 ## ----------------------------------------- ## ## R2 do modelo quadratico ## -------- ## 0.880095 ## -------- ## ## Analise de variancia do modelo quadratico ## =========================================================== ## GL SQ QM Fc valor.p ## ----------------------------------------------------------- ## Efeito linear 1 423.1544 423.1544 10.03 0.00465 ## Efeito quadratico 1 1,285.8430 1,285.8430 30.47 2e-05 ## Desvios de Regressao 4 232.8346 58.2087 1.38 0.27505 ## Residuos 21 886.3375 42.2066 ## ----------------------------------------------------------- ## ------------------------------------------------------------------------ ## ## Modelo cubico ## ========================================= ## Estimativa Erro.padrao tc valor.p ## ----------------------------------------- ## b0 138.2423 3.1302 44.1645 0 ## b1 0.4431 0.0989 4.4812 0.0002 ## b2 -0.0023 0.0008 -2.8551 0.0095 ## b3 0.000003 0 1.9166 0.0690 ## ----------------------------------------- ## ## R2 do modelo cubico ## -------- ## 0.959938 ## -------- ## ## Analise de variancia do modelo cubico ## =========================================================== ## GL SQ QM Fc valor.p ## ----------------------------------------------------------- ## Efeito linear 1 423.1544 423.1544 10.03 0.00465 ## Efeito quadratico 1 1,285.8430 1,285.8430 30.47 2e-05 ## Efeito cubico 1 155.0417 155.0417 3.67 0.069 ## Desvios de Regressao 3 77.7930 25.9310 0.61 0.61327 ## Residuos 21 886.3375 42.2066 ## ----------------------------------------------------------- ## ------------------------------------------------------------------------ # Construção do gráfico X&lt;-seq(0,300,50) Y&lt;-tapply(d$y, d$trat, mean) plot(Y~X, las=1, pch=21,col=&quot;black&quot;,bg=&quot;gray&quot;,cex=1.3, xlab=&quot;Dose de Gesso (kg/ha)&quot;, ylab=&quot;Peso de 1000 sementes (g)&quot; );curve(140.7836+0.2737*x-0.000783*x^2,add=TRUE,col=&quot;red&quot;,lwd=2) text(155,145,&quot;140,7836+0,2737X-0,000783X²\\nR²=0,8801&quot;) "],["referências.html", "Referências", " Referências Andrade, P. F. &amp; Ogliari, P. (2010) Estatística para Ciências Agrárias e Biológicas e Noções de Experimentação. Florianópolis, Ed.: UFSC. Ascencio, A. F. G. &amp; de Campos, E. A. V. (2007) Fundamentos da programação de computadores. 2 ed. ed. São Paulo: Pearson Prentice Hall, 434 p. Banzatto, D. A. &amp; Kronka, S. N. (2013) Experimentação Agrícola. 4 ed. Jaboticabal: Funep, p. Bussab, W. O. &amp; Morettin, P. A. (2002) Estatística Básica. 5 ed. São Paulo: Saraiva, p. Magalhães, M. N. &amp; Lima, A. C. P. (2005) Noções de Probabilidade e Estatística. São Paulo: Editora da Universidade de São Paulo, 392 p. Crawley, M. J. (2007) The R Book. UK, Ed. Willey &amp; Sons Ltda, 950 p. (pdf). Forbellone, A. L. V. &amp; Eberspächer, H. F. (2005) Lógica de programação: a construção de algoritmos e estruturas de dados. 3 ed ed. São Paulo: Pearson Prentice Hall, 218 p. Long, J. D. &amp; Teetor, P. (2019) R Cookbook. USA: OReilly Media, Inc, https://rc2e.com/. Peternelli, L. A. &amp; Mello, M. P. (2011) Conhecendo o R: uma visão estatística. 2ª ed. Viçosa, MG, Ed. UFV, 185 p. Pimentel-Gomes, F. (2000) Curso de Estatística Experimental. 10ª ed. Piracicaba, SP, Brasil: F. Pimentel-Gomes, 477 p. Vieira, S. (1999) Estatística experimental. 2ª ed. São Paulo, Ed.: Editora Atlas S.A, 185 p.  Wickham, H. &amp; Grolemund G. (2019) R for Data Science. https://r4ds.had.co.nz/. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
